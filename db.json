{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"source/images/pasted-13.png","path":"images/pasted-13.png","modified":0,"renderable":0},{"_id":"source/images/pasted-2.png","path":"images/pasted-2.png","modified":0,"renderable":0},{"_id":"source/images/pasted-4.png","path":"images/pasted-4.png","modified":0,"renderable":0},{"_id":"source/images/pasted-6.png","path":"images/pasted-6.png","modified":0,"renderable":0},{"_id":"source/images/pasted-5.png","path":"images/pasted-5.png","modified":0,"renderable":0},{"_id":"source/images/pasted-9.png","path":"images/pasted-9.png","modified":0,"renderable":0},{"_id":"source/images/pasted-3.png","path":"images/pasted-3.png","modified":0,"renderable":0},{"_id":"source/images/pasted-7.png","path":"images/pasted-7.png","modified":0,"renderable":0},{"_id":"source/images/pasted-8.png","path":"images/pasted-8.png","modified":0,"renderable":0},{"_id":"source/image/1.png","path":"image/1.png","modified":0,"renderable":0},{"_id":"source/images/pasted-11.png","path":"images/pasted-11.png","modified":0,"renderable":0},{"_id":"source/images/pasted-10.png","path":"images/pasted-10.png","modified":0,"renderable":0},{"_id":"source/images/pasted-12.png","path":"images/pasted-12.png","modified":0,"renderable":0},{"_id":"themes/next/source/css/main.styl","path":"css/main.styl","modified":0,"renderable":1},{"_id":"themes/next/source/images/algolia_logo.svg","path":"images/algolia_logo.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/apple-touch-icon-next.png","path":"images/apple-touch-icon-next.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/avatar.gif","path":"images/avatar.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","path":"images/cc-by-nc-nd.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","path":"images/cc-by-nc-sa.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nd.svg","path":"images/cc-by-nd.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc.svg","path":"images/cc-by-nc.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by.svg","path":"images/cc-by.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/favicon-16x16-next.png","path":"images/favicon-16x16-next.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-sa.svg","path":"images/cc-by-sa.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-zero.svg","path":"images/cc-zero.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/favicon-32x32-next.png","path":"images/favicon-32x32-next.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/loading.gif","path":"images/loading.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/logo.svg","path":"images/logo.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/placeholder.gif","path":"images/placeholder.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/quote-l.svg","path":"images/quote-l.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/quote-r.svg","path":"images/quote-r.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/searchicon.png","path":"images/searchicon.png","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/affix.js","path":"js/src/affix.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/bootstrap.js","path":"js/src/bootstrap.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/algolia-search.js","path":"js/src/algolia-search.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/exturl.js","path":"js/src/exturl.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/js.cookie.js","path":"js/src/js.cookie.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/hook-duoshuo.js","path":"js/src/hook-duoshuo.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/motion.js","path":"js/src/motion.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/post-details.js","path":"js/src/post-details.js","modified":0,"renderable":1},{"_id":"themes/next/source/images/touxiang.jpeg","path":"images/touxiang.jpeg","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/scroll-cookie.js","path":"js/src/scroll-cookie.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/scrollspy.js","path":"js/src/scrollspy.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","path":"lib/algolia-instant-search/instantsearch.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/utils.js","path":"js/src/utils.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/canvas-ribbon/canvas-ribbon.js","path":"lib/canvas-ribbon/canvas-ribbon.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","path":"lib/canvas-nest/canvas-nest.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/LICENSE","path":"lib/fastclick/LICENSE","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/README.md","path":"lib/fastclick/README.md","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/bower.json","path":"lib/fastclick/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/bower.json","path":"lib/font-awesome/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","path":"lib/font-awesome/HELP-US-OUT.txt","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","path":"lib/jquery_lazyload/README.md","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","path":"lib/jquery_lazyload/CONTRIBUTING.md","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","path":"lib/jquery_lazyload/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","path":"lib/jquery_lazyload/jquery.scrollstop.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","path":"lib/jquery_lazyload/jquery.lazyload.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-barber-shop.min.css","path":"lib/pace/pace-theme-barber-shop.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/needsharebutton/font-embedded.css","path":"lib/needsharebutton/font-embedded.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.css","path":"lib/needsharebutton/needsharebutton.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-big-counter.min.css","path":"lib/pace/pace-theme-big-counter.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.js","path":"lib/needsharebutton/needsharebutton.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-atom.min.css","path":"lib/pace/pace-theme-center-atom.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-bounce.min.css","path":"lib/pace/pace-theme-bounce.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-circle.min.css","path":"lib/pace/pace-theme-center-circle.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-radar.min.css","path":"lib/pace/pace-theme-center-radar.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-simple.min.css","path":"lib/pace/pace-theme-center-simple.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-corner-indicator.min.css","path":"lib/pace/pace-theme-corner-indicator.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-fill-left.min.css","path":"lib/pace/pace-theme-fill-left.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-flash.min.css","path":"lib/pace/pace-theme-flash.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-loading-bar.min.css","path":"lib/pace/pace-theme-loading-bar.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-mac-osx.min.css","path":"lib/pace/pace-theme-mac-osx.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-minimal.min.css","path":"lib/pace/pace-theme-minimal.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace.min.js","path":"lib/pace/pace.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/three/canvas_lines.min.js","path":"lib/three/canvas_lines.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/three/canvas_sphere.min.js","path":"lib/three/canvas_sphere.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/bower.json","path":"lib/velocity/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/three/three-waves.min.js","path":"lib/three/three-waves.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.min.js","path":"lib/velocity/velocity.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","path":"lib/velocity/velocity.ui.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","path":"lib/velocity/velocity.ui.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery/index.js","path":"lib/jquery/index.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/schemes/pisces.js","path":"js/src/schemes/pisces.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.css","path":"lib/Han/dist/han.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.min.css","path":"lib/Han/dist/han.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.min.js","path":"lib/Han/dist/han.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","path":"lib/fancybox/source/blank.gif","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","path":"lib/fancybox/source/fancybox_loading.gif","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","path":"lib/fancybox/source/fancybox_loading@2x.gif","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","path":"lib/fancybox/source/fancybox_overlay.png","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","path":"lib/fancybox/source/fancybox_sprite.png","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","path":"lib/fancybox/source/fancybox_sprite@2x.png","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","path":"lib/fancybox/source/jquery.fancybox.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","path":"lib/fancybox/source/jquery.fancybox.pack.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","path":"lib/fancybox/source/jquery.fancybox.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","path":"lib/fastclick/lib/fastclick.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","path":"lib/fastclick/lib/fastclick.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","path":"lib/font-awesome/css/font-awesome.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","path":"lib/font-awesome/css/font-awesome.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","path":"lib/font-awesome/css/font-awesome.css.map","modified":0,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","path":"lib/ua-parser-js/dist/ua-parser.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","path":"lib/ua-parser-js/dist/ua-parser.pack.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.js","path":"lib/Han/dist/han.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","path":"lib/font-awesome/fonts/fontawesome-webfont.woff2","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","path":"lib/font-awesome/fonts/fontawesome-webfont.woff","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.js","path":"lib/velocity/velocity.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han-space.otf","path":"lib/Han/dist/font/han-space.otf","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han-space.woff","path":"lib/Han/dist/font/han-space.woff","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.otf","path":"lib/Han/dist/font/han.otf","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.woff","path":"lib/Han/dist/font/han.woff","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.woff2","path":"lib/Han/dist/font/han.woff2","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","path":"lib/fancybox/source/helpers/fancybox_buttons.png","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","path":"lib/fancybox/source/helpers/jquery.fancybox-buttons.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","path":"lib/fancybox/source/helpers/jquery.fancybox-buttons.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","path":"lib/fancybox/source/helpers/jquery.fancybox-media.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","path":"lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","path":"lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","path":"lib/font-awesome/fonts/FontAwesome.otf","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","path":"lib/font-awesome/fonts/fontawesome-webfont.eot","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","path":"lib/font-awesome/fonts/fontawesome-webfont.ttf","modified":0,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","path":"lib/algolia-instant-search/instantsearch.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/three/three.min.js","path":"lib/three/three.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","path":"lib/font-awesome/fonts/fontawesome-webfont.svg","modified":0,"renderable":1},{"_id":"source/images/pasted-0.png","path":"images/pasted-0.png","modified":0,"renderable":0},{"_id":"source/images/pasted-1.png","path":"images/pasted-1.png","modified":0,"renderable":0},{"_id":"source/images/pasted-14.png","path":"images/pasted-14.png","modified":0,"renderable":0},{"_id":"source/images/pasted-15.png","path":"images/pasted-15.png","modified":0,"renderable":0},{"_id":"source/images/pasted-16.png","path":"images/pasted-16.png","modified":0,"renderable":0},{"_id":"source/images/pasted-17.png","path":"images/pasted-17.png","modified":0,"renderable":0},{"_id":"source/images/pasted-18.png","path":"images/pasted-18.png","modified":0,"renderable":0},{"_id":"source/images/pasted-19.png","path":"images/pasted-19.png","modified":0,"renderable":0},{"_id":"source/images/pasted-20.png","path":"images/pasted-20.png","modified":0,"renderable":0},{"_id":"source/images/pasted-21.png","path":"images/pasted-21.png","modified":0,"renderable":0},{"_id":"source/images/pasted-22.png","path":"images/pasted-22.png","modified":0,"renderable":0},{"_id":"source/images/pasted-23.png","path":"images/pasted-23.png","modified":0,"renderable":0},{"_id":"source/images/pasted-24.png","path":"images/pasted-24.png","modified":0,"renderable":0},{"_id":"source/images/pasted-25.png","path":"images/pasted-25.png","modified":0,"renderable":0},{"_id":"source/images/pasted-26.png","path":"images/pasted-26.png","modified":0,"renderable":0},{"_id":"source/images/pasted-27.png","path":"images/pasted-27.png","modified":0,"renderable":0},{"_id":"source/images/pasted-28.png","path":"images/pasted-28.png","modified":0,"renderable":0},{"_id":"source/images/pasted-29.png","path":"images/pasted-29.png","modified":0,"renderable":0}],"Cache":[{"_id":"themes/next/.bowerrc","hash":"3228a58ed0ece9f85e1e3136352094080b8dece1","modified":1523731104000},{"_id":"themes/next/.editorconfig","hash":"792fd2bd8174ece1a75d5fd24ab16594886f3a7f","modified":1523731104000},{"_id":"themes/next/.gitattributes","hash":"44bd4729c74ccb88110804f41746fec07bf487d4","modified":1523731104000},{"_id":"themes/next/.gitignore","hash":"0b5c2ffd41f66eb1849d6426ba8cf9649eeed329","modified":1523731104000},{"_id":"themes/next/.hound.yml","hash":"b76daa84c9ca3ad292c78412603370a367cc2bc3","modified":1523731104000},{"_id":"themes/next/.javascript_ignore","hash":"8a224b381155f10e6eb132a4d815c5b52962a9d1","modified":1523731104000},{"_id":"themes/next/.jshintrc","hash":"9928f81bd822f6a8d67fdbc909b517178533bca9","modified":1523731104000},{"_id":"themes/next/.stylintrc","hash":"b28e24704a5d8de08346c45286574c8e76cc109f","modified":1523731104000},{"_id":"themes/next/.travis.yml","hash":"d60d4a5375fea23d53b2156b764a99b2e56fa660","modified":1523731104000},{"_id":"themes/next/LICENSE","hash":"f293bcfcdc06c0b77ba13570bb8af55eb5c059fd","modified":1523731104000},{"_id":"themes/next/README.cn.md","hash":"2c766b3369ed477bce134a5450dab45bef161504","modified":1523731104000},{"_id":"themes/next/README.md","hash":"8ce60ce578963eb4e1eb5e33e1efc2fc4779af9c","modified":1523731104000},{"_id":"themes/next/bower.json","hash":"0674f11d3d514e087a176da0e1d85c2286aa5fba","modified":1523731104000},{"_id":"themes/next/gulpfile.coffee","hash":"031bffc483e417b20e90eceb6cf358e7596d2e69","modified":1523731104000},{"_id":"themes/next/_config.yml","hash":"5400551981e601951a6048f5721ef284ede7f89d","modified":1549769664914},{"_id":"themes/next/package.json","hash":"036d3a1346203d2f1a3958024df7f74e7ac07bfe","modified":1523731104000},{"_id":"source/_discarded/Untitled.md","hash":"08fb74bac4f4d4fc4341f6fd94321b0ecf3aac0c","modified":1551877499296},{"_id":"source/_posts/Pytorch中的squeeze-和unsqueeze-函数.md","hash":"e0880f88004922cc42a27a037c053dc2c1b1b95f","modified":1550235377614},{"_id":"source/_posts/Pytorch中的torch-cat-函数.md","hash":"debe5ce9bbab0c90ceb3637d4191e2a7e931b619","modified":1550156518523},{"_id":"source/_posts/交叉熵的数学原理及应用——Pytorch中的CrossEntropyLoss()函数.md","hash":"b76bde1ad8cbf59b463b0b99eb48b0e97685e2ba","modified":1552291052216},{"_id":"source/_posts/Ubuntu18-04深度优化美化记录.md","hash":"549b7a6fbd29befb41e2ba19eac3d43836a13787","modified":1552553738091},{"_id":"source/_posts/今天我发布了新的博客.md","hash":"6fda79ea3a5a556ff11468c655e495b4d1c43a9d","modified":1549361032172},{"_id":"source/about/index.md","hash":"21631d9bb8f533f4492911c92b6a3620d7898ab3","modified":1550378697135},{"_id":"source/images/pasted-13.png","hash":"5c99afce1aed13198b172f34e757c0b24118ee37","modified":1550592270875},{"_id":"source/images/pasted-2.png","hash":"88c123350e4b027c31a14989b4141650a25cccec","modified":1549341914793},{"_id":"source/images/pasted-4.png","hash":"c9ec19380e5a600ca3f1ffede82e8eb59087334a","modified":1549353952749},{"_id":"source/images/pasted-6.png","hash":"64f2e2da84500318338933c842f2e83c3c878998","modified":1549355901118},{"_id":"source/images/pasted-5.png","hash":"a535a3a14789128efab296423fd7d59a7a441f86","modified":1549355756749},{"_id":"source/images/pasted-9.png","hash":"0561293439f052b5b347c4a79144e11a93f3e94e","modified":1550235418283},{"_id":"source/images/pasted-3.png","hash":"8eb012dbd7188f5750e167aaa117ba2cc5b116a8","modified":1549345798391},{"_id":"source/tags/index.md","hash":"400f01ac7bdb949766f8e487b1ed0cd59ca436b5","modified":1549361076336},{"_id":"source/images/pasted-7.png","hash":"499fab7c600cbd6ddbcfa7beab63ab5a5b752f37","modified":1549357100401},{"_id":"source/images/pasted-8.png","hash":"af2ec7cd323b99da610b4d7c4039314571d2d2be","modified":1549359203460},{"_id":"themes/next/.github/CONTRIBUTING.md","hash":"3b5eafd32abb718e56ccf8d1cee0607ad8ce611d","modified":1523731104000},{"_id":"themes/next/.github/ISSUE_TEMPLATE.md","hash":"50d48c47162817a3810a9d9ad51104e83947419a","modified":1523731104000},{"_id":"themes/next/.github/PULL_REQUEST_TEMPLATE.md","hash":"902f627155a65099e0a37842ff396a58d0dc306f","modified":1523731104000},{"_id":"themes/next/.github/browserstack_logo.png","hash":"a6c43887f64a7f48a2814e3714eaa1215e542037","modified":1523731104000},{"_id":"source/categories/index.md","hash":"6e55cce97c665ca3f5cb7ebd47707fc594246cb8","modified":1549360533026},{"_id":"themes/next/languages/de.yml","hash":"057e7df11ddeb1c8c15a5d7c5ff29430d725ec6b","modified":1523731104000},{"_id":"themes/next/languages/default.yml","hash":"44ef3f26917f467459326c2c8be2f73e4d947f35","modified":1523731104000},{"_id":"themes/next/languages/en.yml","hash":"7e680d9bb8f3a3a9d1ba1c9d312b3d257183dded","modified":1523731104000},{"_id":"themes/next/languages/fr-FR.yml","hash":"7e4eb7011b8feee641cfb11c6e73180b0ded1c0f","modified":1523731104000},{"_id":"themes/next/languages/id.yml","hash":"b5de1ea66dd9ef54cac9a1440eaa4e3f5fc011f5","modified":1523731104000},{"_id":"themes/next/languages/it.yml","hash":"aa595f2bda029f73ef7bfa104b4c55c3f4e9fb4c","modified":1523731104000},{"_id":"themes/next/languages/ja.yml","hash":"3c76e16fd19b262864475faa6854b718bc08c4d8","modified":1523731104000},{"_id":"themes/next/languages/ko.yml","hash":"ea5b46056e73ebcee121d5551627af35cbffc900","modified":1523731104000},{"_id":"themes/next/languages/nl-NL.yml","hash":"edca4f3598857dbc3cbf19ed412213329b6edd47","modified":1523731104000},{"_id":"themes/next/languages/pt-BR.yml","hash":"b1694ae766ed90277bcc4daca4b1cfa19cdcb72b","modified":1523731104000},{"_id":"themes/next/languages/pt.yml","hash":"44b61f2d085b827b507909a0b8f8ce31c6ef5d04","modified":1523731104000},{"_id":"themes/next/languages/ru.yml","hash":"98ec6f0b7183282e11cffc7ff586ceb82400dd75","modified":1523731104000},{"_id":"themes/next/languages/vi.yml","hash":"fd08d3c8d2c62965a98ac420fdaf95e54c25d97c","modified":1523731104000},{"_id":"themes/next/languages/zh-Hans.yml","hash":"16ef56d0dea94638de7d200984c90ae56f26b4fe","modified":1523731104000},{"_id":"themes/next/languages/zh-hk.yml","hash":"9396f41ae76e4fef99b257c93c7354e661f6e0fa","modified":1523731104000},{"_id":"themes/next/languages/zh-tw.yml","hash":"50b71abb3ecc0686f9739e179e2f829cd074ecd9","modified":1523731104000},{"_id":"themes/next/layout/_layout.swig","hash":"da0929166674ea637e0ad454f85ad0d7bac4aff2","modified":1523731104000},{"_id":"themes/next/layout/archive.swig","hash":"f0a8225feafd971419837cdb4bcfec98a4a59b2f","modified":1523731104000},{"_id":"themes/next/layout/category.swig","hash":"4472255f4a3e3dd6d79201523a9526dcabdfbf18","modified":1523731104000},{"_id":"themes/next/layout/index.swig","hash":"783611349c941848a0e26ee2f1dc44dd14879bd1","modified":1523731104000},{"_id":"themes/next/layout/page.swig","hash":"969caaee05bdea725e99016eb63d810893a73e99","modified":1523731104000},{"_id":"themes/next/layout/post.swig","hash":"b3589a8e46288a10d20e41c7a5985d2493725aec","modified":1523731104000},{"_id":"themes/next/layout/schedule.swig","hash":"d86f8de4e118f8c4d778b285c140474084a271db","modified":1523731104000},{"_id":"themes/next/scripts/merge-configs.js","hash":"81e86717ecfb775986b945d17f0a4ba27532ef07","modified":1523731104000},{"_id":"themes/next/layout/tag.swig","hash":"7e0a7d7d832883eddb1297483ad22c184e4368de","modified":1523731104000},{"_id":"themes/next/scripts/merge.js","hash":"9130dabe6a674c54b535f322b17d75fe6081472f","modified":1523731104000},{"_id":"themes/next/test/.jshintrc","hash":"19f93d13d1689fe033c82eb2d5f3ce30b6543cc0","modified":1523731104000},{"_id":"themes/next/test/helpers.js","hash":"a1f5de25154c3724ffc24a91ddc576cdbd60864f","modified":1523731104000},{"_id":"themes/next/test/intern.js","hash":"11fa8a4f5c3b4119a179ae0a2584c8187f907a73","modified":1523731104000},{"_id":"source/image/1.png","hash":"eb6b555561f0668f8a0e10a235b3ab2d220ad8e1","modified":1549165466395},{"_id":"source/images/pasted-11.png","hash":"aede79ed04ecba9d9eb18bcd5f7327331e989364","modified":1550373798902},{"_id":"source/images/pasted-10.png","hash":"f851b82756d75d65c1c0394d9443655cbaaea2a5","modified":1550371486921},{"_id":"source/images/pasted-12.png","hash":"fee455e46fcff51658d5a0064e24025347e87cb3","modified":1550378115284},{"_id":"themes/next/source/fonts/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1523731104000},{"_id":"themes/next/layout/_custom/header.swig","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1523731104000},{"_id":"themes/next/layout/_custom/sidebar.swig","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1523731104000},{"_id":"themes/next/layout/_partials/comments.swig","hash":"4a6f5b1792b2e5262b7fdab9a716b3108e2f09c7","modified":1523731104000},{"_id":"themes/next/layout/_partials/footer.swig","hash":"c4d6181f5d3db5365e622f78714af8cc58d7a45e","modified":1523731104000},{"_id":"themes/next/layout/_partials/head.swig","hash":"6b94fe8f3279daea5623c49ef4bb35917ba57510","modified":1523731104000},{"_id":"themes/next/layout/_partials/header.swig","hash":"ed042be6252848058c90109236ec988e392d91d4","modified":1523731104000},{"_id":"themes/next/layout/_partials/page-header.swig","hash":"1efd925d34a5d4ba2dc0838d9c86ba911e705fc9","modified":1523731104000},{"_id":"themes/next/layout/_partials/pagination.swig","hash":"9e8e21d194ef44d271b1cca0bc1448c14d7edf4f","modified":1523731104000},{"_id":"themes/next/layout/_partials/search.swig","hash":"9dbd378e94abfcb3f864a5b8dbbf18d212ca2ee0","modified":1523731104000},{"_id":"themes/next/layout/_macro/post-collapse.swig","hash":"31322a7f57936cf2dc62e824af5490da5354cf02","modified":1523731104000},{"_id":"themes/next/layout/_macro/post.swig","hash":"446a35a2cd389f8cfc3aa38973a9b44ad0740134","modified":1523731104000},{"_id":"themes/next/layout/_macro/reward.swig","hash":"56e8d8556cf474c56ae1bef9cb7bbd26554adb07","modified":1523731104000},{"_id":"themes/next/layout/_macro/post-copyright.swig","hash":"665a928604f99d2ba7dc4a4a9150178229568cc6","modified":1523731104000},{"_id":"themes/next/layout/_macro/sidebar.swig","hash":"6a54c3c85ff6b19d275827a327abbf4bd99b2ebf","modified":1523731104000},{"_id":"themes/next/layout/_macro/wechat-subscriber.swig","hash":"39852700e4084ecccffa6d4669168e5cc0514c9e","modified":1523731104000},{"_id":"themes/next/layout/_scripts/boostrap.swig","hash":"03aaebe9d50f6acb007ec38cc04acd1cfceb404d","modified":1523731104000},{"_id":"themes/next/layout/_scripts/commons.swig","hash":"766b2bdda29523ed6cd8d7aa197f996022f8fd94","modified":1523731104000},{"_id":"themes/next/layout/_scripts/vendors.swig","hash":"a266f96ad06ee87bdeae6e105a4b53cd587bbd04","modified":1523731104000},{"_id":"themes/next/layout/_third-party/duoshuo-hot-articles.swig","hash":"5d4638c46aef65bf32a01681495b62416ccc98db","modified":1523731104000},{"_id":"themes/next/layout/_third-party/exturl.swig","hash":"7c04a42319d728be356746363aff8ea247791d24","modified":1523731104000},{"_id":"themes/next/layout/_third-party/mathjax.swig","hash":"6d25596d6a7c57700d37b607f8d9a62d89708683","modified":1523731104000},{"_id":"themes/next/layout/_third-party/needsharebutton.swig","hash":"5fe0447cc88a5a63b530cf0426f93c4634811876","modified":1523731104000},{"_id":"themes/next/layout/_third-party/rating.swig","hash":"fc93b1a7e6aed0dddb1f3910142b48d8ab61174e","modified":1523731104000},{"_id":"themes/next/layout/_third-party/schedule.swig","hash":"22369026c87fc23893c35a7f250b42f3bb1b60f1","modified":1523731104000},{"_id":"themes/next/layout/_third-party/scroll-cookie.swig","hash":"1ddb2336a1a19b47af3017047012c01ec5f54529","modified":1523731104000},{"_id":"themes/next/scripts/tags/button.js","hash":"d023f10a00077f47082b0517e2ad666e6e994f60","modified":1523731104000},{"_id":"themes/next/scripts/tags/center-quote.js","hash":"535fc542781021c4326dec24d8495cbb1387634a","modified":1523731104000},{"_id":"themes/next/scripts/tags/exturl.js","hash":"8d7e60f60779bde050d20fd76f6fdc36fc85e06d","modified":1523731104000},{"_id":"themes/next/scripts/tags/full-image.js","hash":"8eeb3fb89540299bdbb799edfdfdac3743b50596","modified":1523731104000},{"_id":"themes/next/scripts/tags/group-pictures.js","hash":"49252824cd53184dc9b97b2f2d87ff28e1b3ef27","modified":1523731104000},{"_id":"themes/next/scripts/tags/label.js","hash":"2f8f41a7316372f0d1ed6b51190dc4acd3e16fff","modified":1523731104000},{"_id":"themes/next/scripts/tags/lazy-image.js","hash":"eeeabede68cf263de9e6593ecf682f620da16f0a","modified":1523731104000},{"_id":"themes/next/scripts/tags/note.js","hash":"64de4e9d01cf3b491ffc7d53afdf148ee5ad9779","modified":1523731104000},{"_id":"themes/next/scripts/tags/tabs.js","hash":"5786545d51c38e8ca38d1bfc7dd9e946fc70a316","modified":1523731104000},{"_id":"themes/next/source/css/main.styl","hash":"20702c48d6053c92c5bcdbc68e8d0ef1369848a0","modified":1523731104000},{"_id":"themes/next/source/images/algolia_logo.svg","hash":"ec119560b382b2624e00144ae01c137186e91621","modified":1523731104000},{"_id":"themes/next/source/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1523731104000},{"_id":"themes/next/source/images/avatar.gif","hash":"264082bb3a1af70d5499c7d22b0902cb454b6d12","modified":1523731104000},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1523731104000},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1523731104000},{"_id":"themes/next/source/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1523731104000},{"_id":"themes/next/source/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1523731104000},{"_id":"themes/next/source/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1523731104000},{"_id":"themes/next/source/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1523731104000},{"_id":"themes/next/source/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1523731104000},{"_id":"themes/next/source/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1523731104000},{"_id":"themes/next/source/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1523731104000},{"_id":"themes/next/source/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1523731104000},{"_id":"themes/next/source/images/logo.svg","hash":"d29cacbae1bdc4bbccb542107ee0524fe55ad6de","modified":1523731104000},{"_id":"themes/next/source/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1523731104000},{"_id":"themes/next/source/images/quote-l.svg","hash":"94e870b4c8c48da61d09522196d4dd40e277a98f","modified":1523731104000},{"_id":"themes/next/source/images/quote-r.svg","hash":"e60ae504f9d99b712c793c3740c6b100d057d4ec","modified":1523731104000},{"_id":"themes/next/source/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1523731104000},{"_id":"themes/next/layout/_scripts/schemes/mist.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1523731104000},{"_id":"themes/next/layout/_scripts/schemes/muse.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1523731104000},{"_id":"themes/next/source/css/_mixins/Mist.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1523731104000},{"_id":"themes/next/source/css/_mixins/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1523731104000},{"_id":"themes/next/source/css/_mixins/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1523731104000},{"_id":"themes/next/source/css/_variables/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1523731104000},{"_id":"themes/next/source/css/_variables/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1523731104000},{"_id":"themes/next/layout/_partials/head/custom-head.swig","hash":"9e1b9666efa77f4cf8d8261bcfa445a9ac608e53","modified":1523731104000},{"_id":"themes/next/layout/_partials/head/external-fonts.swig","hash":"7ce76358411184482bb0934e70037949dd0da8ca","modified":1523731104000},{"_id":"themes/next/layout/_partials/search/localsearch.swig","hash":"957701729b85fb0c5bfcf2fb99c19d54582f91ed","modified":1523731104000},{"_id":"themes/next/layout/_partials/search/swiftype.swig","hash":"959b7e04a96a5596056e4009b73b6489c117597e","modified":1523731104000},{"_id":"themes/next/layout/_partials/search/tinysou.swig","hash":"eefe2388ff3d424694045eda21346989b123977c","modified":1523731104000},{"_id":"themes/next/layout/_partials/share/add-this.swig","hash":"23e23dc0f76ef3c631f24c65277adf7ea517b383","modified":1523731104000},{"_id":"themes/next/layout/_partials/share/baidushare.swig","hash":"1f1107468aaf03f7d0dcd7eb2b653e2813a675b4","modified":1523731104000},{"_id":"themes/next/layout/_partials/share/jiathis.swig","hash":"048fd5e98149469f8c28c21ba3561a7a67952c9b","modified":1523731104000},{"_id":"themes/next/layout/_partials/share/duoshuo_share.swig","hash":"89c5a5240ecb223acfe1d12377df5562a943fd5d","modified":1523731104000},{"_id":"themes/next/layout/_scripts/pages/post-details.swig","hash":"069d1357c717572256e5cdee09574ebce529cbae","modified":1523731104000},{"_id":"themes/next/layout/_scripts/schemes/gemini.swig","hash":"a44acf9b0d0f44ef3dfc767376a95c984cc127de","modified":1523731104000},{"_id":"themes/next/layout/_scripts/schemes/pisces.swig","hash":"a44acf9b0d0f44ef3dfc767376a95c984cc127de","modified":1523731104000},{"_id":"themes/next/layout/_third-party/comments/changyan.swig","hash":"0e3378f7c39b2b0f69638290873ede6b6b6825c0","modified":1523731104000},{"_id":"themes/next/layout/_third-party/comments/disqus.swig","hash":"c316758546dc9ba6c60cb4d852c17ca6bb6d6724","modified":1523731104000},{"_id":"themes/next/layout/_third-party/comments/gitment.swig","hash":"10160daceaa6f1ecf632323d422ebe2caae49ddf","modified":1523731104000},{"_id":"themes/next/layout/_third-party/comments/duoshuo.swig","hash":"a356b2185d40914447fde817eb3d358ab6b3e4c3","modified":1523731104000},{"_id":"themes/next/layout/_third-party/comments/hypercomments.swig","hash":"3e8dc5c6c912628a37e3b5f886bec7b2e5ed14ea","modified":1523731104000},{"_id":"themes/next/layout/_third-party/comments/index.swig","hash":"aa0629277d751c55c6d973e7691bf84af9b17a60","modified":1523731104000},{"_id":"themes/next/layout/_third-party/comments/livere.swig","hash":"8a2e393d2e49f7bf560766d8a07cd461bf3fce4f","modified":1523731104000},{"_id":"themes/next/layout/_third-party/comments/valine.swig","hash":"fcabbb241f894c9a6309c44e126cf3e8fea81fd4","modified":1523731104000},{"_id":"themes/next/layout/_third-party/comments/youyan.swig","hash":"8b6650f77fe0a824c8075b2659e0403e0c78a705","modified":1523731104000},{"_id":"themes/next/layout/_third-party/analytics/analytics-with-widget.swig","hash":"98df9d72e37dd071e882f2d5623c9d817815b139","modified":1523731104000},{"_id":"themes/next/layout/_third-party/analytics/application-insights.swig","hash":"60426bf73f8a89ba61fb1be2df3ad5398e32c4ef","modified":1523731104000},{"_id":"themes/next/layout/_third-party/analytics/baidu-analytics.swig","hash":"deda6a814ed48debc694c4e0c466f06c127163d0","modified":1523731104000},{"_id":"themes/next/layout/_third-party/analytics/busuanzi-counter.swig","hash":"18e7bef8923d83ea42df6c97405e515a876cede4","modified":1523731104000},{"_id":"themes/next/layout/_third-party/analytics/cnzz-analytics.swig","hash":"8160b27bee0aa372c7dc7c8476c05bae57f58d0f","modified":1523731104000},{"_id":"themes/next/layout/_third-party/analytics/facebook-sdk.swig","hash":"a234c5cd1f75ca5731e814d0dbb92fdcf9240d1b","modified":1523731104000},{"_id":"themes/next/layout/_third-party/analytics/firestore.swig","hash":"1cd01c6e92ab1913d48e556a92bb4f28b6dc4996","modified":1523731104000},{"_id":"themes/next/layout/_third-party/analytics/google-analytics.swig","hash":"5d9943d74cc2e0a91badcf4f755c6de77eab193a","modified":1523731104000},{"_id":"themes/next/layout/_third-party/analytics/index.swig","hash":"5e9bb24c750b49513d9a65799e832f07410002ac","modified":1523731104000},{"_id":"themes/next/layout/_third-party/analytics/tencent-analytics.swig","hash":"3658414379e0e8a34c45c40feadc3edc8dc55f88","modified":1523731104000},{"_id":"themes/next/layout/_third-party/analytics/lean-analytics.swig","hash":"fc65b9c98a0a8ab43a5e7aabff6c5f03838e09c8","modified":1523731104000},{"_id":"themes/next/layout/_third-party/analytics/tencent-mta.swig","hash":"0ddc94ed4ba0c19627765fdf1abc4d8efbe53d5a","modified":1523731104000},{"_id":"themes/next/layout/_third-party/analytics/vkontakte-api.swig","hash":"c3971fd154d781088e1cc665035f8561a4098f4c","modified":1523731104000},{"_id":"themes/next/layout/_third-party/search/index.swig","hash":"c747fb5c6b1f500e8f0c583e44195878b66e4e29","modified":1523731104000},{"_id":"themes/next/layout/_third-party/search/localsearch.swig","hash":"385c066af96bee30be2459dbec8aae1f15d382f5","modified":1523731104000},{"_id":"themes/next/layout/_third-party/search/tinysou.swig","hash":"cb3a5d36dbe1630bab84e03a52733a46df7c219b","modified":1523731104000},{"_id":"themes/next/layout/_third-party/seo/baidu-push.swig","hash":"c057b17f79e8261680fbae8dc4e81317a127c799","modified":1523731104000},{"_id":"themes/next/source/css/_custom/custom.styl","hash":"0f93cebf566442b4cce6943cf711a4919f54b1c1","modified":1549772789793},{"_id":"themes/next/source/css/_mixins/Gemini.styl","hash":"2aa5b7166a85a8aa34b17792ae4f58a5a96df6cc","modified":1523731104000},{"_id":"themes/next/source/css/_mixins/Pisces.styl","hash":"9ab65361ba0a12a986edd103e56492644c2db0b8","modified":1523731104000},{"_id":"themes/next/source/css/_mixins/base.styl","hash":"82f9055955920ed88a2ab6a20ab02169abb2c634","modified":1523731104000},{"_id":"themes/next/source/css/_variables/Gemini.styl","hash":"99fbb4686ea9a3e03a4726ed7cf4d8f529034452","modified":1523731104000},{"_id":"themes/next/source/css/_variables/Mist.styl","hash":"be087dcc060e8179f7e7f60ab4feb65817bd3d9f","modified":1523731104000},{"_id":"themes/next/source/css/_variables/Pisces.styl","hash":"f29165e36489a87ba32d17dddfd2720d84e3f3ec","modified":1523731104000},{"_id":"themes/next/source/css/_variables/base.styl","hash":"29c261fa6b4046322559074d75239c6b272fb8a3","modified":1523731104000},{"_id":"themes/next/source/js/src/affix.js","hash":"978e0422b5bf1b560236d8d10ebc1adcf66392e3","modified":1523731104000},{"_id":"themes/next/source/js/src/bootstrap.js","hash":"034bc8113e0966fe2096ba5b56061bbf10ef0512","modified":1523731104000},{"_id":"themes/next/source/js/src/algolia-search.js","hash":"b172f697ed339a24b1e80261075232978d164c35","modified":1523731104000},{"_id":"themes/next/source/js/src/exturl.js","hash":"e42e2aaab7bf4c19a0c8e779140e079c6aa5c0b1","modified":1523731104000},{"_id":"themes/next/source/js/src/js.cookie.js","hash":"9b37973a90fd50e71ea91682265715e45ae82c75","modified":1523731104000},{"_id":"themes/next/source/js/src/hook-duoshuo.js","hash":"a6119070c0119f33e08b29da7d2cce2635eb40a0","modified":1523731104000},{"_id":"themes/next/source/js/src/motion.js","hash":"754b294394f102c8fd9423a1789ddb1201677898","modified":1523731104000},{"_id":"themes/next/source/js/src/post-details.js","hash":"a13f45f7aa8291cf7244ec5ba93907d119c5dbdd","modified":1523731104000},{"_id":"themes/next/source/images/touxiang.jpeg","hash":"100d80a96823e9ecfdcfe1a0231cf5eb4a380465","modified":1548904092229},{"_id":"themes/next/source/js/src/scroll-cookie.js","hash":"09dc828cbf5f31158ff6250d2bf7c3cde6365c67","modified":1523731104000},{"_id":"themes/next/source/js/src/scrollspy.js","hash":"fe4da1b9fe73518226446f5f27d2831e4426fc35","modified":1523731104000},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","hash":"90ef19edc982645b118b095615838d9c5eaba0de","modified":1523731104000},{"_id":"themes/next/source/js/src/utils.js","hash":"9b1325801d27213083d1487a12b1a62b539ab6f8","modified":1523731104000},{"_id":"themes/next/source/lib/fancybox/.bower.json","hash":"cc40a9b11e52348e554c84e4a5c058056f6b7aeb","modified":1523731104000},{"_id":"themes/next/source/lib/canvas-ribbon/canvas-ribbon.js","hash":"ff5915eb2596e890a2fc6697c864f861a1995ec0","modified":1523731104000},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","hash":"0387e75e23b1db108a755073fe52a0d03eb391a7","modified":1523731104000},{"_id":"themes/next/source/lib/fancybox/.gitattributes","hash":"2db21acfbd457452462f71cc4048a943ee61b8e0","modified":1523731104000},{"_id":"themes/next/source/lib/fastclick/.bower.json","hash":"93ebd5b35e632f714dcf1753e1f6db77ec74449b","modified":1523731104000},{"_id":"themes/next/source/lib/fastclick/LICENSE","hash":"dcd5b6b43095d9e90353a28b09cb269de8d4838e","modified":1523731104000},{"_id":"themes/next/source/lib/fastclick/README.md","hash":"1decd8e1adad2cd6db0ab50cf56de6035156f4ea","modified":1523731104000},{"_id":"themes/next/source/lib/fastclick/bower.json","hash":"13379463c7463b4b96d13556b46faa4cc38d81e6","modified":1523731104000},{"_id":"themes/next/source/lib/font-awesome/.bower.json","hash":"a2aaaf12378db56bd10596ba3daae30950eac051","modified":1523731104000},{"_id":"themes/next/source/lib/font-awesome/.gitignore","hash":"69d152fa46b517141ec3b1114dd6134724494d83","modified":1523731104000},{"_id":"themes/next/source/lib/font-awesome/.npmignore","hash":"dcf470ab3a358103bb896a539cc03caeda10fa8b","modified":1523731104000},{"_id":"themes/next/source/lib/font-awesome/bower.json","hash":"279a8a718ab6c930a67c41237f0aac166c1b9440","modified":1523731104000},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","hash":"4f7bf961f1bed448f6ba99aeb9219fabf930ba96","modified":1523731104000},{"_id":"themes/next/source/lib/jquery/.bower.json","hash":"91745c2cc6c946c7275f952b2b0760b880cea69e","modified":1523731104000},{"_id":"themes/next/source/lib/jquery_lazyload/.bower.json","hash":"b7638afc93e9cd350d0783565ee9a7da6805ad8e","modified":1523731104000},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","hash":"895d50fa29759af7835256522e9dd7dac597765c","modified":1523731104000},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","hash":"4891864c24c28efecd81a6a8d3f261145190f901","modified":1523731104000},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","hash":"65bc85d12197e71c40a55c0cd7f6823995a05222","modified":1523731104000},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","hash":"0e9a81785a011c98be5ea821a8ed7d411818cfd1","modified":1523731104000},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","hash":"481fd478650e12b67c201a0ea41e92743f8b45a3","modified":1523731104000},{"_id":"themes/next/source/lib/pace/pace-theme-barber-shop.min.css","hash":"ee0d51446cb4ffe1bb96bd7bc8c8e046dddfcf46","modified":1523731104000},{"_id":"themes/next/source/lib/needsharebutton/font-embedded.css","hash":"c39d37278c1e178838732af21bd26cd0baeddfe0","modified":1523731104000},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.css","hash":"3ef0020a1815ca6151ea4886cd0d37421ae3695c","modified":1523731104000},{"_id":"themes/next/source/lib/pace/pace-theme-big-counter.min.css","hash":"5b561dc328af4c4d512e20a76fe964d113a32ba8","modified":1523731104000},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.js","hash":"9885fd9bea5e7ebafc5b1de9d17be5e106248d96","modified":1523731104000},{"_id":"themes/next/source/lib/pace/pace-theme-center-atom.min.css","hash":"dcf79c24fe5350fb73d8038573a104e73639e9d3","modified":1523731104000},{"_id":"themes/next/source/lib/pace/pace-theme-bounce.min.css","hash":"f6bdb9a785b7979dd8ec5c60e278af955ef1e585","modified":1523731104000},{"_id":"themes/next/source/lib/pace/pace-theme-center-circle.min.css","hash":"a4066769c78affbfbc5e30a600e2c7862cd532e0","modified":1523731104000},{"_id":"themes/next/source/lib/pace/pace-theme-center-radar.min.css","hash":"ab7cba998bf4c03b13df342bf43647fa4f419783","modified":1523731104000},{"_id":"themes/next/source/lib/pace/pace-theme-center-simple.min.css","hash":"67f44c947548bd4d77e7590d3f59e236cbf9e98a","modified":1523731104000},{"_id":"themes/next/source/lib/pace/pace-theme-corner-indicator.min.css","hash":"b3c64c973f31884e3d8145989476707333406b9a","modified":1523731104000},{"_id":"themes/next/source/lib/pace/pace-theme-fill-left.min.css","hash":"0bec1e235a4a2cccda3f993b205424e1441a44ae","modified":1523731104000},{"_id":"themes/next/source/lib/pace/pace-theme-flash.min.css","hash":"13ace22c40312d7bbd8d9c1e50eff897a7a497d8","modified":1523731104000},{"_id":"themes/next/source/lib/pace/pace-theme-loading-bar.min.css","hash":"7ee28875dfc1230d76c537f6605766e8d4011e9f","modified":1523731104000},{"_id":"themes/next/source/lib/pace/pace-theme-mac-osx.min.css","hash":"9f2e7b51b084da407863826b25265b31150b3821","modified":1523731104000},{"_id":"themes/next/source/lib/pace/pace-theme-minimal.min.css","hash":"9cd783cceb8a191f3c8b5d81f7a430ecc3e489d3","modified":1523731104000},{"_id":"themes/next/source/lib/pace/pace.min.js","hash":"9944dfb7814b911090e96446cea4d36e2b487234","modified":1523731104000},{"_id":"themes/next/source/lib/three/canvas_lines.min.js","hash":"dce4a3b65f8bf958f973690caa7ec4952f353b0c","modified":1523731104000},{"_id":"themes/next/source/lib/three/canvas_sphere.min.js","hash":"d8ea241a53c135a650f7335d2b6982b899fd58a9","modified":1523731104000},{"_id":"themes/next/source/lib/velocity/.bower.json","hash":"05f960846f1c7a93dab1d3f9a1121e86812e8c88","modified":1523731104000},{"_id":"themes/next/source/lib/velocity/bower.json","hash":"2ec99573e84c7117368beccb9e94b6bf35d2db03","modified":1523731104000},{"_id":"themes/next/source/lib/three/three-waves.min.js","hash":"d968cba6b3a50b3626a02d67b544f349d83b147c","modified":1523731104000},{"_id":"themes/next/source/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1523731104000},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1523731104000},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1523731104000},{"_id":"themes/next/source/lib/jquery/index.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1523731104000},{"_id":"themes/next/layout/_third-party/search/algolia-search/assets.swig","hash":"28ff4ed6714c59124569ffcbd10f1173d53ca923","modified":1523731104000},{"_id":"themes/next/layout/_third-party/search/algolia-search/dom.swig","hash":"ba698f49dd3a868c95b240d802f5b1b24ff287e4","modified":1523731104000},{"_id":"themes/next/source/css/_common/components/back-to-top-sidebar.styl","hash":"4719ce717962663c5c33ef97b1119a0b3a4ecdc3","modified":1523731104000},{"_id":"themes/next/source/css/_common/outline/outline.styl","hash":"2186be20e317505cd31886f1291429cc21f76703","modified":1523731104000},{"_id":"themes/next/source/css/_common/components/back-to-top.styl","hash":"31050fc7a25784805b4843550151c93bfa55c9c8","modified":1523731104000},{"_id":"themes/next/source/css/_common/components/buttons.styl","hash":"7e509c7c28c59f905b847304dd3d14d94b6f3b8e","modified":1523731104000},{"_id":"themes/next/source/css/_common/components/components.styl","hash":"a6bb5256be6195e76addbda12f4ed7c662d65e7a","modified":1523731104000},{"_id":"themes/next/source/css/_common/components/comments.styl","hash":"471f1627891aca5c0e1973e09fbcb01e1510d193","modified":1523731104000},{"_id":"themes/next/source/css/_common/components/pagination.styl","hash":"c5d48863f332ff8ce7c88dec2c893f709d7331d3","modified":1523731104000},{"_id":"themes/next/source/css/_common/components/tag-cloud.styl","hash":"dd8a3b22fc2f222ac6e6c05bd8a773fb039169c0","modified":1523731104000},{"_id":"themes/next/source/css/_common/scaffolding/mobile.styl","hash":"47a46583a1f3731157a3f53f80ed1ed5e2753e8e","modified":1523731104000},{"_id":"themes/next/source/css/_common/scaffolding/base.styl","hash":"f7c44b0ee46cf2cf82a4c9455ba8d8b55299976f","modified":1523731104000},{"_id":"themes/next/source/css/_common/scaffolding/scaffolding.styl","hash":"a280a583b7615e939aaddbf778f5c108ef8a2a6c","modified":1523731104000},{"_id":"themes/next/source/css/_common/scaffolding/tables.styl","hash":"64f5d56c08d74a338813df1265580ca0cbf0190b","modified":1523731104000},{"_id":"themes/next/source/css/_schemes/Mist/_base.styl","hash":"c2d079788d6fc2e9a191ccdae94e50d55bf849dc","modified":1523731104000},{"_id":"themes/next/source/css/_common/scaffolding/helpers.styl","hash":"9c25c75311e1bd4d68df031d3f2ae6d141a90766","modified":1523731104000},{"_id":"themes/next/source/css/_schemes/Mist/_header.styl","hash":"5ae7906dc7c1d9468c7f4b4a6feddddc555797a1","modified":1523731104000},{"_id":"themes/next/source/css/_schemes/Mist/_logo.styl","hash":"38e5df90c8689a71c978fd83ba74af3d4e4e5386","modified":1523731104000},{"_id":"themes/next/source/css/_common/scaffolding/normalize.styl","hash":"ece571f38180febaf02ace8187ead8318a300ea7","modified":1523731104000},{"_id":"themes/next/source/css/_schemes/Gemini/index.styl","hash":"18c3336ee3d09bd2da6a876e1336539f03d5a973","modified":1523731104000},{"_id":"themes/next/source/css/_schemes/Mist/_menu.styl","hash":"b0dcca862cd0cc6e732e33d975b476d744911742","modified":1523731104000},{"_id":"themes/next/source/css/_schemes/Mist/_search.styl","hash":"1452cbe674cc1d008e1e9640eb4283841058fc64","modified":1523731104000},{"_id":"themes/next/source/css/_schemes/Mist/_posts-expanded.styl","hash":"3b25edfa187d1bbbd0d38b50dd013cef54758abf","modified":1523731104000},{"_id":"themes/next/source/css/_schemes/Mist/index.styl","hash":"9a5581a770af8964064fef7afd3e16963e45547f","modified":1523731104000},{"_id":"themes/next/source/css/_schemes/Muse/_layout.styl","hash":"0efa036a15c18f5abb058b7c0fad1dd9ac5eed4c","modified":1523731104000},{"_id":"themes/next/source/css/_schemes/Muse/_logo.styl","hash":"8829bc556ca38bfec4add4f15a2f028092ac6d46","modified":1523731104000},{"_id":"themes/next/source/css/_schemes/Muse/_menu.styl","hash":"4aac01962520d60b03b23022ab601ad4bd19c08c","modified":1523731104000},{"_id":"themes/next/source/css/_schemes/Muse/index.styl","hash":"a0e2030a606c934fb2c5c7373aaae04a1caac4c5","modified":1523731104000},{"_id":"themes/next/source/css/_schemes/Pisces/_brand.styl","hash":"c4ed249798296f60bda02351fe6404fb3ef2126f","modified":1523731104000},{"_id":"themes/next/source/css/_schemes/Muse/_search.styl","hash":"1452cbe674cc1d008e1e9640eb4283841058fc64","modified":1523731104000},{"_id":"themes/next/source/css/_schemes/Pisces/_menu.styl","hash":"215de948be49bcf14f06d500cef9f7035e406a43","modified":1523731104000},{"_id":"themes/next/source/css/_schemes/Pisces/_layout.styl","hash":"5b93958239d3d2bf9aeaede44eced2434d784462","modified":1523731104000},{"_id":"themes/next/source/css/_schemes/Pisces/_posts.styl","hash":"2f878213cb24c5ddc18877f6d15ec5c5f57745ac","modified":1523731104000},{"_id":"themes/next/source/css/_schemes/Pisces/_sidebar.styl","hash":"9d16fa3c14ed76b71229f022b63a02fd0f580958","modified":1523731104000},{"_id":"themes/next/source/js/src/schemes/pisces.js","hash":"8050a5b2683d1d77238c5762b6bd89c543daed6e","modified":1523731104000},{"_id":"themes/next/source/css/_schemes/Pisces/index.styl","hash":"69ecd6c97e7cdfd822ac8102b45ad0ede85050db","modified":1523731104000},{"_id":"themes/next/source/lib/Han/dist/han.css","hash":"bd40da3fba8735df5850956814e312bd7b3193d7","modified":1523731104000},{"_id":"themes/next/source/lib/Han/dist/han.min.css","hash":"a0c9e32549a8b8cf327ab9227b037f323cdb60ee","modified":1523731104000},{"_id":"themes/next/source/lib/Han/dist/han.min.js","hash":"f559c68a25065a14f47da954a7617d87263e409d","modified":1523731104000},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1523731104000},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1523731104000},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1523731104000},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1523731104000},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1523731104000},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1523731104000},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","hash":"5f163444617b6cf267342f06ac166a237bb62df9","modified":1523731104000},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","hash":"53360764b429c212f424399384417ccc233bb3be","modified":1523731104000},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","hash":"1cf3d47b5ccb7cb6e9019c64f2a88d03a64853e4","modified":1523731104000},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","hash":"06cef196733a710e77ad7e386ced6963f092dc55","modified":1523731104000},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","hash":"2cae0f5a6c5d6f3cb993015e6863f9483fc4de18","modified":1523731104000},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","hash":"0140952c64e3f2b74ef64e050f2fe86eab6624c8","modified":1523731104000},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1523731104000},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","hash":"0189d278706509412bac4745f96c83984e1d59f4","modified":1523731104000},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","hash":"38628e75e4412cc6f11074e03e1c6d257aae495b","modified":1523731104000},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"214dad442a92d36af77ed0ca1d9092b16687f02f","modified":1523731104000},{"_id":"themes/next/source/lib/Han/dist/han.js","hash":"e345397e0585c9fed1449e614ec13e0224acf2ab","modified":1523731104000},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1523731104000},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1523731104000},{"_id":"themes/next/source/lib/velocity/velocity.js","hash":"9f08181baea0cc0e906703b7e5df9111b9ef3373","modified":1523731104000},{"_id":"themes/next/source/css/_common/components/footer/footer.styl","hash":"7905a7f625702b45645d8be1268cb8af3f698c70","modified":1523731104000},{"_id":"themes/next/source/css/_common/components/header/headerband.styl","hash":"d27448f199fc2f9980b601bc22b87f08b5d64dd1","modified":1523731104000},{"_id":"themes/next/source/css/_common/components/header/header.styl","hash":"ae1ca14e51de67b07dba8f61ec79ee0e2e344574","modified":1523731104000},{"_id":"themes/next/source/css/_common/components/header/site-meta.styl","hash":"6c00f6e0978f4d8f9a846a15579963728aaa6a17","modified":1523731104000},{"_id":"themes/next/source/css/_common/components/header/menu.styl","hash":"8a2421cb9005352905fae9d41a847ae56957247e","modified":1523731104000},{"_id":"themes/next/source/css/_common/components/header/site-nav.styl","hash":"49c2b2c14a1e7fcc810c6be4b632975d0204c281","modified":1523731104000},{"_id":"themes/next/source/css/_common/components/highlight/highlight.styl","hash":"25dc25f61a232f03ca72472b7852f882448ec185","modified":1523731104000},{"_id":"themes/next/source/css/_common/components/highlight/diff.styl","hash":"96f32ea6c3265a3889e6abe57587f6e2a2a40dfb","modified":1523731104000},{"_id":"themes/next/source/css/_common/components/highlight/theme.styl","hash":"b76387934fb6bb75212b23c1a194486892cc495e","modified":1523731104000},{"_id":"themes/next/source/css/_common/components/pages/archive.styl","hash":"f5aa2ba3bfffc15475e7e72a55b5c9d18609fdf5","modified":1523731104000},{"_id":"themes/next/source/css/_common/components/pages/categories.styl","hash":"4eff5b252d7b614e500fc7d52c97ce325e57d3ab","modified":1523731104000},{"_id":"themes/next/source/css/_common/components/pages/pages.styl","hash":"2039590632bba3943c39319d80ef630af7928185","modified":1523731104000},{"_id":"themes/next/source/css/_common/components/pages/post-detail.styl","hash":"9bf4362a4d0ae151ada84b219d39fbe5bb8c790e","modified":1523731104000},{"_id":"themes/next/source/css/_common/components/pages/schedule.styl","hash":"a82afbb72d83ee394aedc7b37ac0008a9823b4f4","modified":1523731104000},{"_id":"themes/next/source/css/_common/components/post/post-button.styl","hash":"e72a89e0f421444453e149ba32c77a64bd8e44e8","modified":1523731104000},{"_id":"themes/next/source/css/_common/components/post/post-copyright.styl","hash":"f54367c0feda6986c030cc4d15a0ca6ceea14bcb","modified":1523731104000},{"_id":"themes/next/source/css/_common/components/post/post-collapse.styl","hash":"0f7f522cc6bfb3401d5afd62b0fcdf48bb2d604b","modified":1523731104000},{"_id":"themes/next/source/css/_common/components/post/post-expand.styl","hash":"535b3b4f8cb1eec2558e094320e7dfb01f94c0e7","modified":1523731104000},{"_id":"themes/next/source/css/_common/components/post/post-eof.styl","hash":"2cdc094ecf907a02fce25ad4a607cd5c40da0f2b","modified":1523731104000},{"_id":"themes/next/source/css/_common/components/post/post-meta.styl","hash":"aea21141015ca8c409d8b33e3e34ec505f464e93","modified":1523731104000},{"_id":"themes/next/source/css/_common/components/post/post-nav.styl","hash":"a5d8617a24d7cb6c5ad91ea621183ca2c0917331","modified":1523731104000},{"_id":"themes/next/source/css/_common/components/post/post-gallery.styl","hash":"387ce23bba52b22a586b2dfb4ec618fe1ffd3926","modified":1523731104000},{"_id":"themes/next/source/css/_common/components/post/post-reward.styl","hash":"36332c8a91f089f545f3c3e8ea90d08aa4d6e60c","modified":1523731104000},{"_id":"themes/next/source/css/_common/components/post/post-rtl.styl","hash":"017074ef58166e2d69c53bb7590a0e7a8947a1ed","modified":1523731104000},{"_id":"themes/next/source/css/_common/components/post/post-title.styl","hash":"d5a4e4fc17f1f7e7c3a61b52d8e2e9677e139de7","modified":1523731104000},{"_id":"themes/next/source/css/_common/components/post/post-type.styl","hash":"10251257aceecb117233c9554dcf8ecfef8e2104","modified":1523731104000},{"_id":"themes/next/source/css/_common/components/post/post-tags.styl","hash":"a352ae5b1f8857393bf770d2e638bf15f0c9585d","modified":1523731104000},{"_id":"themes/next/source/css/_common/components/post/post-widgets.styl","hash":"e4055a0d2cd2b0ad9dc55928e2f3e7bd4e499da3","modified":1523731104000},{"_id":"themes/next/source/css/_common/components/post/post.styl","hash":"262debfd4442fa03d9919ceb88b948339df03fb0","modified":1523731104000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author-links.styl","hash":"0a6c0efffdf18bddbc1d1238feaed282b09cd0fe","modified":1523731104000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-blogroll.styl","hash":"89dd4f8b1f1cce3ad46cf2256038472712387d02","modified":1523731104000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-feed-link.styl","hash":"9486ddd2cb255227db102d09a7df4cae0fabad72","modified":1523731104000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author.styl","hash":"920343e41c124221a17f050bbb989494d44f7a24","modified":1523731104000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-dimmer.styl","hash":"efa5e5022e205b52786ce495d4879f5e7b8f84b2","modified":1523731104000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toc.styl","hash":"12937cae17c96c74d5c58db6cb29de3b2dfa14a2","modified":1523731104000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-nav.styl","hash":"45fa7193435a8eae9960267438750b4c9fa9587f","modified":1523731104000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toggle.styl","hash":"f7784aba0c1cd20d824c918c120012d57a5eaa2a","modified":1523731104000},{"_id":"themes/next/source/css/_common/components/tags/blockquote-center.styl","hash":"c2abe4d87148e23e15d49ee225bc650de60baf46","modified":1523731104000},{"_id":"themes/next/source/css/_common/components/sidebar/site-state.styl","hash":"3623e7fa4324ec1307370f33d8f287a9e20a5578","modified":1523731104000},{"_id":"themes/next/source/css/_common/components/tags/full-image.styl","hash":"37e951e734a252fe8a81f452b963df2ba90bfe90","modified":1523731104000},{"_id":"themes/next/source/css/_common/components/tags/exturl.styl","hash":"1b3cc9f4e5a7f6e05b4100e9990b37b20d4a2005","modified":1523731104000},{"_id":"themes/next/source/css/_common/components/tags/label.styl","hash":"4a457d265d62f287c63d48764ce45d9bcfc9ec5a","modified":1523731104000},{"_id":"themes/next/source/css/_common/components/tags/group-pictures.styl","hash":"4851b981020c5cbc354a1af9b831a2dcb3cf9d39","modified":1523731104000},{"_id":"themes/next/source/css/_common/components/tags/note-modern.styl","hash":"ee7528900578ef4753effe05b346381c40de5499","modified":1523731104000},{"_id":"themes/next/source/css/_common/components/tags/tabs.styl","hash":"4ab5deed8c3b0c338212380f678f8382672e1bcb","modified":1523731104000},{"_id":"themes/next/source/css/_common/components/tags/tags.styl","hash":"ead0d0f2321dc71505788c7f689f92257cf14947","modified":1523731104000},{"_id":"themes/next/source/css/_common/components/tags/note.styl","hash":"32c9156bea5bac9e9ad0b4c08ffbca8b3d9aac4b","modified":1523731104000},{"_id":"themes/next/source/css/_common/components/third-party/algolia-search.styl","hash":"fd42777b9125fd8969dc39d4f15473e2b91b4142","modified":1523731104000},{"_id":"themes/next/source/css/_common/components/third-party/baidushare.styl","hash":"93b08815c4d17e2b96fef8530ec1f1064dede6ef","modified":1523731104000},{"_id":"themes/next/source/css/_common/components/third-party/busuanzi-counter.styl","hash":"d4e6d8d7b34dc69994593c208f875ae8f7e8a3ae","modified":1523731104000},{"_id":"themes/next/source/css/_common/components/third-party/duoshuo.styl","hash":"2340dd9b3202c61d73cc708b790fac5adddbfc7f","modified":1523731104000},{"_id":"themes/next/source/css/_common/components/third-party/han.styl","hash":"cce6772e2cdb4db85d35486ae4c6c59367fbdd40","modified":1523731104000},{"_id":"themes/next/source/css/_common/components/third-party/gitment.styl","hash":"34935b40237c074be5f5e8818c14ccfd802b7439","modified":1523731104000},{"_id":"themes/next/source/css/_common/components/third-party/localsearch.styl","hash":"d89c4b562b528e4746696b2ad8935764d133bdae","modified":1523731104000},{"_id":"themes/next/source/css/_common/components/third-party/jiathis.styl","hash":"327b5f63d55ec26f7663185c1a778440588d9803","modified":1523731104000},{"_id":"themes/next/source/css/_common/components/third-party/third-party.styl","hash":"1ccfbd4d0f5754b2dc2719a91245c95f547a7652","modified":1523731104000},{"_id":"themes/next/source/css/_common/components/third-party/needsharebutton.styl","hash":"a5e3e6b4b4b814a9fe40b34d784fed67d6d977fa","modified":1523731104000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar.styl","hash":"50305b6ad7d09d2ffa4854e39f41ec1f4fe984fd","modified":1523731104000},{"_id":"themes/next/source/css/_schemes/Mist/outline/outline.styl","hash":"5dc4859c66305f871e56cba78f64bfe3bf1b5f01","modified":1523731104000},{"_id":"themes/next/source/css/_schemes/Mist/sidebar/sidebar-blogroll.styl","hash":"817587e46df49e819858c8ecbafa08b53d5ff040","modified":1523731104000},{"_id":"themes/next/source/css/_schemes/Muse/sidebar/sidebar-blogroll.styl","hash":"817587e46df49e819858c8ecbafa08b53d5ff040","modified":1523731104000},{"_id":"themes/next/source/lib/Han/dist/font/han-space.otf","hash":"07436f011b44051f61b8329c99de4bec64e86f4b","modified":1523731104000},{"_id":"themes/next/source/lib/Han/dist/font/han-space.woff","hash":"7a635062b10bf5662ae1d218ba0980171005d060","modified":1523731104000},{"_id":"themes/next/source/lib/Han/dist/font/han.otf","hash":"f1f6bb8f461f5672e000380195d3d2358a28494c","modified":1523731104000},{"_id":"themes/next/source/lib/Han/dist/font/han.woff","hash":"f38ff9b2eecaa17b50b66aa2dae87e9e7436d195","modified":1523731104000},{"_id":"themes/next/source/lib/Han/dist/font/han.woff2","hash":"623af3ed5423371ac136a4fe0e8cc7bb7396037a","modified":1523731104000},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1523731104000},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1523731104000},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","hash":"91e41741c2e93f732c82aaacec4cfc6e3f3ec876","modified":1523731104000},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","hash":"3bdf69ed2469e4fb57f5a95f17300eef891ff90d","modified":1523731104000},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1523731104000},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","hash":"53e194f4a72e649c04fb586dd57762b8c022800b","modified":1523731104000},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","hash":"048707bc52ac4b6563aaa383bfe8660a0ddc908c","modified":1523731104000},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1523731104000},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","hash":"13b1eab65a983c7a73bc7997c479d66943f7c6cb","modified":1523731104000},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","hash":"9ccc6f8144f54e86df9a3fd33a18368d81cf3a4f","modified":1523731104000},{"_id":"themes/next/source/lib/three/three.min.js","hash":"73f4cdc17e51a72b9bf5b9291f65386d615c483b","modified":1523731104000},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","hash":"98a8aa5cf7d62c2eff5f07ede8d844b874ef06ed","modified":1523731104000},{"_id":"source/images/pasted-0.png","hash":"04d27ca6e02bf3493d91a9c8cb8bac0e92568ce4","modified":1549173345055},{"_id":"source/images/pasted-1.png","hash":"04d27ca6e02bf3493d91a9c8cb8bac0e92568ce4","modified":1549173517771},{"_id":"source/images/pasted-14.png","hash":"80ef4c276e984f0ff35343ed4c9f16a15d1feda1","modified":1552553354150},{"_id":"source/images/pasted-15.png","hash":"80ef4c276e984f0ff35343ed4c9f16a15d1feda1","modified":1552553354406},{"_id":"source/images/pasted-16.png","hash":"6ff1fced42672e2125fb6e84380e836ba0043d16","modified":1552553419977},{"_id":"source/images/pasted-17.png","hash":"c8697dfed3cd71c164f1246b6262efc95f33c6f1","modified":1552553468286},{"_id":"source/images/pasted-18.png","hash":"04e2ccd687ffc8342840efa1b183f501fba91e6f","modified":1552553499065},{"_id":"source/images/pasted-19.png","hash":"43c4ed1abe4a87c8fb65760979e8dc3eea214228","modified":1552553529015},{"_id":"source/images/pasted-20.png","hash":"5bb4fac391d64a65087ef0326ffe4207335abd44","modified":1552553548125},{"_id":"source/images/pasted-21.png","hash":"362f4b1112200e09a6bda1aea315a63681dc9250","modified":1552553566799},{"_id":"source/images/pasted-22.png","hash":"710adadcf6f5b9903b8e64b65bcdf89f1a1e20cb","modified":1552553591236},{"_id":"source/images/pasted-23.png","hash":"6724a67ed6805521eef231c7ad9bfc7ec9095fae","modified":1552553607814},{"_id":"source/images/pasted-24.png","hash":"4bb6452708e497486f77a74b61976c82ed5ab78e","modified":1552553619763},{"_id":"source/images/pasted-25.png","hash":"ce6a95a4a9dea09acc3aa896c44a645b8cb0f20e","modified":1552553644028},{"_id":"source/images/pasted-26.png","hash":"7bd20c53a57e73008ae73cda9fa752fd4ea9dc5b","modified":1552553667958},{"_id":"source/images/pasted-27.png","hash":"4c7751e2fd78d6e3fbb1e519ea2f05715201716f","modified":1552553697032},{"_id":"source/images/pasted-28.png","hash":"2b779a8f65655a94c6983e8b68254bed514b6120","modified":1552553709005},{"_id":"source/images/pasted-29.png","hash":"b4c406fdba19d17a20240e9941dc02e5711a08f5","modified":1552553736415}],"Category":[{"name":"深度学习","_id":"cjt8bm3u400049gojap42qmg7"},{"name":"Ubuntu","_id":"cjt8bm3v3000d9gojdy453yz7"}],"Data":[],"Page":[{"title":"","date":"2019-02-01T07:51:00.000Z","_content":"### \t挣扎在视觉领域的柯基爱好者，博客主要用来记录自己的学习心得,并激励自己持续吸收知识，毕竟不学点什么是更不出来博客的（才不是字写的难看又懒得记笔记  （Q v Q)... ）\n\n---\n<p align=\"right\">————种一棵树最好的时间是十年前，其次就是现在</p>","source":"about/index.md","raw":"title: ''\ndate: 2019-02-01 15:51:00\n---\n### \t挣扎在视觉领域的柯基爱好者，博客主要用来记录自己的学习心得,并激励自己持续吸收知识，毕竟不学点什么是更不出来博客的（才不是字写的难看又懒得记笔记  （Q v Q)... ）\n\n---\n<p align=\"right\">————种一棵树最好的时间是十年前，其次就是现在</p>","updated":"2019-02-17T04:44:57.135Z","path":"about/index.html","comments":1,"layout":"page","_id":"cjt8bm3ts00019gojpsqhoorr","content":"<h3 id=\"挣扎在视觉领域的柯基爱好者，博客主要用来记录自己的学习心得-并激励自己持续吸收知识，毕竟不学点什么是更不出来博客的（才不是字写的难看又懒得记笔记-（Q-v-Q-…-）\"><a href=\"#挣扎在视觉领域的柯基爱好者，博客主要用来记录自己的学习心得-并激励自己持续吸收知识，毕竟不学点什么是更不出来博客的（才不是字写的难看又懒得记笔记-（Q-v-Q-…-）\" class=\"headerlink\" title=\"挣扎在视觉领域的柯基爱好者，博客主要用来记录自己的学习心得,并激励自己持续吸收知识，毕竟不学点什么是更不出来博客的（才不是字写的难看又懒得记笔记  （Q v Q)… ）\"></a>挣扎在视觉领域的柯基爱好者，博客主要用来记录自己的学习心得,并激励自己持续吸收知识，毕竟不学点什么是更不出来博客的（才不是字写的难看又懒得记笔记  （Q v Q)… ）</h3><hr>\n<p align=\"right\">————种一棵树最好的时间是十年前，其次就是现在</p>","site":{"data":{}},"excerpt":"","more":"<h3 id=\"挣扎在视觉领域的柯基爱好者，博客主要用来记录自己的学习心得-并激励自己持续吸收知识，毕竟不学点什么是更不出来博客的（才不是字写的难看又懒得记笔记-（Q-v-Q-…-）\"><a href=\"#挣扎在视觉领域的柯基爱好者，博客主要用来记录自己的学习心得-并激励自己持续吸收知识，毕竟不学点什么是更不出来博客的（才不是字写的难看又懒得记笔记-（Q-v-Q-…-）\" class=\"headerlink\" title=\"挣扎在视觉领域的柯基爱好者，博客主要用来记录自己的学习心得,并激励自己持续吸收知识，毕竟不学点什么是更不出来博客的（才不是字写的难看又懒得记笔记  （Q v Q)… ）\"></a>挣扎在视觉领域的柯基爱好者，博客主要用来记录自己的学习心得,并激励自己持续吸收知识，毕竟不学点什么是更不出来博客的（才不是字写的难看又懒得记笔记  （Q v Q)… ）</h3><hr>\n<p align=\"right\">————种一棵树最好的时间是十年前，其次就是现在</p>"},{"title":"tags","date":"2019-02-01T07:51:11.000Z","type":"tags","_content":"","source":"tags/index.md","raw":"---\ntitle: tags\ndate: 2019-02-01 15:51:11\ntype: \"tags\"\n---\n","updated":"2019-02-05T10:04:36.336Z","path":"tags/index.html","comments":1,"layout":"page","_id":"cjt8bm3tz00039goj92hwes48","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"categories","date":"2019-02-01T11:44:19.000Z","type":"categories","_content":"","source":"categories/index.md","raw":"---\ntitle: categories\ndate: 2019-02-01 19:44:19\ntype: \"categories\"\n---\n","updated":"2019-02-05T09:55:33.026Z","path":"categories/index.html","comments":1,"layout":"page","_id":"cjt8bm3ub00079goj3yk7oo07","content":"","site":{"data":{}},"excerpt":"","more":""}],"Post":[{"title":"Pytorch中的squeeze()和unsqueeze()函数","author":"wangjiansen","date":"2019-02-14T14:37:00.000Z","_content":"在numpy库中，经常会出现“秩为1的一维数组”（come from 吴恩达的深度学习，目前还没有搞清楚numpy中如此设计的意图）。比如：\n\n~~~python\n>>> a = torch.rand(3)\n>>> a.shape\ntorch.Size([3])\n~~~\n注意这里的a的shape是[3] ，既不是 [1,3] 也不是 [3,1]。这就说明它既不是行向量也不是列向量，只是一个数组。\n\n但是我们可以用squeeze（）和unsqueeze（）对其进行操作，比如：\n~~~python\n>>> a = torch.rand(3)\n>>> a.shape\ntorch.Size([3])\n>>> b = a.unsqueeze(1)\n>>> b.shape\ntorch.Size([3,1])\n>>> c = a.unsqueeze(0)\n>>> c.shape\ntorch.Size([1,3])\n~~~\n在对这两个函数讲解之前，我们先统一一下定义:\n~~~python\n>>> x.shape\ntorch.Size([a,b,c])\n~~~\n这里我们说a是第一个维度（表示第一个维度的数量是a），b是第二个维度（表示第一个维度的数量是b），c是第三个维度（表示第三个维度的数量是c）。\n\npython中很多函数都是这样表示**“维度”**这个概念的,比如numpy.stack()。\n\n简而言之，unsqueeze（arg）是增添第arg个维度为1，以插入的形式填充。比如：\n~~~python\n>>> a = torch.rand(1,3)\n>>> a.shape\ntorch.Size([1,3])\n>>> b = a.unsqueeze(1)\n>>> b.shape\ntorch.Size([1,1,3])\n>>> b\ntensor([[[0.6289,0.0814,0.9177]]])\n>>> a\ntensor([[0.6289,0.0814,0.9177]])\n>>> c = a.unsqueeze(0)\n>>> c.shape\ntorch.Size([1,1,3])\n>>> d = a.unsqueeze(2)\n>>> d.shape\ntorch.Size([1,3,1])\n~~~\n相反，squeeze（arg）是删除第arg个维度(如果当前维度不为1，则不会进行删除)，比如：\n~~~python\n>>> d.shape\ntorch.Size([1,3,1])\n>>> e = d.squeeze(0)\n>>> e.shape\ntorch.Size([3,1])\n>>> e\ntensor([[0,6289],\n\t\t[0.0814],\n        [0.9177]])\n>>> f = d.squeeze(1)\n>>> f.shape\ntorch.Size([1,3,1])\n~~~\n\n最后，奥哈哈，情人节快乐～～！！！**(\\*^__^\\*) ……**","source":"_posts/Pytorch中的squeeze-和unsqueeze-函数.md","raw":"title: Pytorch中的squeeze()和unsqueeze()函数\nauthor: wangjiansen\ntags:\n  - 深度学习\n  - Pytorch\n  - ''\ncategories:\n  - 深度学习\ndate: 2019-02-14 22:37:00\n---\n在numpy库中，经常会出现“秩为1的一维数组”（come from 吴恩达的深度学习，目前还没有搞清楚numpy中如此设计的意图）。比如：\n\n~~~python\n>>> a = torch.rand(3)\n>>> a.shape\ntorch.Size([3])\n~~~\n注意这里的a的shape是[3] ，既不是 [1,3] 也不是 [3,1]。这就说明它既不是行向量也不是列向量，只是一个数组。\n\n但是我们可以用squeeze（）和unsqueeze（）对其进行操作，比如：\n~~~python\n>>> a = torch.rand(3)\n>>> a.shape\ntorch.Size([3])\n>>> b = a.unsqueeze(1)\n>>> b.shape\ntorch.Size([3,1])\n>>> c = a.unsqueeze(0)\n>>> c.shape\ntorch.Size([1,3])\n~~~\n在对这两个函数讲解之前，我们先统一一下定义:\n~~~python\n>>> x.shape\ntorch.Size([a,b,c])\n~~~\n这里我们说a是第一个维度（表示第一个维度的数量是a），b是第二个维度（表示第一个维度的数量是b），c是第三个维度（表示第三个维度的数量是c）。\n\npython中很多函数都是这样表示**“维度”**这个概念的,比如numpy.stack()。\n\n简而言之，unsqueeze（arg）是增添第arg个维度为1，以插入的形式填充。比如：\n~~~python\n>>> a = torch.rand(1,3)\n>>> a.shape\ntorch.Size([1,3])\n>>> b = a.unsqueeze(1)\n>>> b.shape\ntorch.Size([1,1,3])\n>>> b\ntensor([[[0.6289,0.0814,0.9177]]])\n>>> a\ntensor([[0.6289,0.0814,0.9177]])\n>>> c = a.unsqueeze(0)\n>>> c.shape\ntorch.Size([1,1,3])\n>>> d = a.unsqueeze(2)\n>>> d.shape\ntorch.Size([1,3,1])\n~~~\n相反，squeeze（arg）是删除第arg个维度(如果当前维度不为1，则不会进行删除)，比如：\n~~~python\n>>> d.shape\ntorch.Size([1,3,1])\n>>> e = d.squeeze(0)\n>>> e.shape\ntorch.Size([3,1])\n>>> e\ntensor([[0,6289],\n\t\t[0.0814],\n        [0.9177]])\n>>> f = d.squeeze(1)\n>>> f.shape\ntorch.Size([1,3,1])\n~~~\n\n最后，奥哈哈，情人节快乐～～！！！**(\\*^__^\\*) ……**","slug":"Pytorch中的squeeze-和unsqueeze-函数","published":1,"updated":"2019-02-15T12:56:17.614Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjt8bm3tg00009gojlykpkzru","content":"<p>在numpy库中，经常会出现“秩为1的一维数组”（come from 吴恩达的深度学习，目前还没有搞清楚numpy中如此设计的意图）。比如：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>a = torch.rand(<span class=\"number\">3</span>)</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>a.shape</span><br><span class=\"line\">torch.Size([<span class=\"number\">3</span>])</span><br></pre></td></tr></table></figure>\n<p>注意这里的a的shape是[3] ，既不是 [1,3] 也不是 [3,1]。这就说明它既不是行向量也不是列向量，只是一个数组。</p>\n<p>但是我们可以用squeeze（）和unsqueeze（）对其进行操作，比如：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>a = torch.rand(<span class=\"number\">3</span>)</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>a.shape</span><br><span class=\"line\">torch.Size([<span class=\"number\">3</span>])</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>b = a.unsqueeze(<span class=\"number\">1</span>)</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>b.shape</span><br><span class=\"line\">torch.Size([<span class=\"number\">3</span>,<span class=\"number\">1</span>])</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>c = a.unsqueeze(<span class=\"number\">0</span>)</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>c.shape</span><br><span class=\"line\">torch.Size([<span class=\"number\">1</span>,<span class=\"number\">3</span>])</span><br></pre></td></tr></table></figure></p>\n<p>在对这两个函数讲解之前，我们先统一一下定义:<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>x.shape</span><br><span class=\"line\">torch.Size([a,b,c])</span><br></pre></td></tr></table></figure></p>\n<p>这里我们说a是第一个维度（表示第一个维度的数量是a），b是第二个维度（表示第一个维度的数量是b），c是第三个维度（表示第三个维度的数量是c）。</p>\n<p>python中很多函数都是这样表示<strong>“维度”</strong>这个概念的,比如numpy.stack()。</p>\n<p>简而言之，unsqueeze（arg）是增添第arg个维度为1，以插入的形式填充。比如：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>a = torch.rand(<span class=\"number\">1</span>,<span class=\"number\">3</span>)</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>a.shape</span><br><span class=\"line\">torch.Size([<span class=\"number\">1</span>,<span class=\"number\">3</span>])</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>b = a.unsqueeze(<span class=\"number\">1</span>)</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>b.shape</span><br><span class=\"line\">torch.Size([<span class=\"number\">1</span>,<span class=\"number\">1</span>,<span class=\"number\">3</span>])</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>b</span><br><span class=\"line\">tensor([[[<span class=\"number\">0.6289</span>,<span class=\"number\">0.0814</span>,<span class=\"number\">0.9177</span>]]])</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>a</span><br><span class=\"line\">tensor([[<span class=\"number\">0.6289</span>,<span class=\"number\">0.0814</span>,<span class=\"number\">0.9177</span>]])</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>c = a.unsqueeze(<span class=\"number\">0</span>)</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>c.shape</span><br><span class=\"line\">torch.Size([<span class=\"number\">1</span>,<span class=\"number\">1</span>,<span class=\"number\">3</span>])</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>d = a.unsqueeze(<span class=\"number\">2</span>)</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>d.shape</span><br><span class=\"line\">torch.Size([<span class=\"number\">1</span>,<span class=\"number\">3</span>,<span class=\"number\">1</span>])</span><br></pre></td></tr></table></figure></p>\n<p>相反，squeeze（arg）是删除第arg个维度(如果当前维度不为1，则不会进行删除)，比如：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>d.shape</span><br><span class=\"line\">torch.Size([<span class=\"number\">1</span>,<span class=\"number\">3</span>,<span class=\"number\">1</span>])</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>e = d.squeeze(<span class=\"number\">0</span>)</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>e.shape</span><br><span class=\"line\">torch.Size([<span class=\"number\">3</span>,<span class=\"number\">1</span>])</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>e</span><br><span class=\"line\">tensor([[<span class=\"number\">0</span>,<span class=\"number\">6289</span>],</span><br><span class=\"line\">\t\t[<span class=\"number\">0.0814</span>],</span><br><span class=\"line\">        [<span class=\"number\">0.9177</span>]])</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>f = d.squeeze(<span class=\"number\">1</span>)</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>f.shape</span><br><span class=\"line\">torch.Size([<span class=\"number\">1</span>,<span class=\"number\">3</span>,<span class=\"number\">1</span>])</span><br></pre></td></tr></table></figure></p>\n<p>最后，奥哈哈，情人节快乐～～！！！<strong>(*^__^*) ……</strong></p>\n","site":{"data":{}},"excerpt":"","more":"<p>在numpy库中，经常会出现“秩为1的一维数组”（come from 吴恩达的深度学习，目前还没有搞清楚numpy中如此设计的意图）。比如：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>a = torch.rand(<span class=\"number\">3</span>)</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>a.shape</span><br><span class=\"line\">torch.Size([<span class=\"number\">3</span>])</span><br></pre></td></tr></table></figure>\n<p>注意这里的a的shape是[3] ，既不是 [1,3] 也不是 [3,1]。这就说明它既不是行向量也不是列向量，只是一个数组。</p>\n<p>但是我们可以用squeeze（）和unsqueeze（）对其进行操作，比如：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>a = torch.rand(<span class=\"number\">3</span>)</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>a.shape</span><br><span class=\"line\">torch.Size([<span class=\"number\">3</span>])</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>b = a.unsqueeze(<span class=\"number\">1</span>)</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>b.shape</span><br><span class=\"line\">torch.Size([<span class=\"number\">3</span>,<span class=\"number\">1</span>])</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>c = a.unsqueeze(<span class=\"number\">0</span>)</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>c.shape</span><br><span class=\"line\">torch.Size([<span class=\"number\">1</span>,<span class=\"number\">3</span>])</span><br></pre></td></tr></table></figure></p>\n<p>在对这两个函数讲解之前，我们先统一一下定义:<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>x.shape</span><br><span class=\"line\">torch.Size([a,b,c])</span><br></pre></td></tr></table></figure></p>\n<p>这里我们说a是第一个维度（表示第一个维度的数量是a），b是第二个维度（表示第一个维度的数量是b），c是第三个维度（表示第三个维度的数量是c）。</p>\n<p>python中很多函数都是这样表示<strong>“维度”</strong>这个概念的,比如numpy.stack()。</p>\n<p>简而言之，unsqueeze（arg）是增添第arg个维度为1，以插入的形式填充。比如：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>a = torch.rand(<span class=\"number\">1</span>,<span class=\"number\">3</span>)</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>a.shape</span><br><span class=\"line\">torch.Size([<span class=\"number\">1</span>,<span class=\"number\">3</span>])</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>b = a.unsqueeze(<span class=\"number\">1</span>)</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>b.shape</span><br><span class=\"line\">torch.Size([<span class=\"number\">1</span>,<span class=\"number\">1</span>,<span class=\"number\">3</span>])</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>b</span><br><span class=\"line\">tensor([[[<span class=\"number\">0.6289</span>,<span class=\"number\">0.0814</span>,<span class=\"number\">0.9177</span>]]])</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>a</span><br><span class=\"line\">tensor([[<span class=\"number\">0.6289</span>,<span class=\"number\">0.0814</span>,<span class=\"number\">0.9177</span>]])</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>c = a.unsqueeze(<span class=\"number\">0</span>)</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>c.shape</span><br><span class=\"line\">torch.Size([<span class=\"number\">1</span>,<span class=\"number\">1</span>,<span class=\"number\">3</span>])</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>d = a.unsqueeze(<span class=\"number\">2</span>)</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>d.shape</span><br><span class=\"line\">torch.Size([<span class=\"number\">1</span>,<span class=\"number\">3</span>,<span class=\"number\">1</span>])</span><br></pre></td></tr></table></figure></p>\n<p>相反，squeeze（arg）是删除第arg个维度(如果当前维度不为1，则不会进行删除)，比如：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>d.shape</span><br><span class=\"line\">torch.Size([<span class=\"number\">1</span>,<span class=\"number\">3</span>,<span class=\"number\">1</span>])</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>e = d.squeeze(<span class=\"number\">0</span>)</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>e.shape</span><br><span class=\"line\">torch.Size([<span class=\"number\">3</span>,<span class=\"number\">1</span>])</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>e</span><br><span class=\"line\">tensor([[<span class=\"number\">0</span>,<span class=\"number\">6289</span>],</span><br><span class=\"line\">\t\t[<span class=\"number\">0.0814</span>],</span><br><span class=\"line\">        [<span class=\"number\">0.9177</span>]])</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>f = d.squeeze(<span class=\"number\">1</span>)</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>f.shape</span><br><span class=\"line\">torch.Size([<span class=\"number\">1</span>,<span class=\"number\">3</span>,<span class=\"number\">1</span>])</span><br></pre></td></tr></table></figure></p>\n<p>最后，奥哈哈，情人节快乐～～！！！<strong>(*^__^*) ……</strong></p>\n"},{"title":"Pytorch中的torch.cat()函数","author":"wangjiansen","date":"2019-02-09T03:53:00.000Z","_content":"cat是concatnate的意思：拼接，联系在一起。\n- - - \n先说cat( )的普通用法\n如果我们有两个tensor是A和B，想把他们拼接在一起，需要如下操作：\n~~~python\nC = torch.cat( (A,B),0 )  #按维数0拼接（竖着拼）\nC = torch.cat( (A,B),1 )  #按维数1拼接（横着拼）\n~~~\n\n~~~python\n>>> import torch\n>>> A=torch.ones(2,3)    #2x3的张量（矩阵）                                     \n>>> A\ntensor([[ 1.,  1.,  1.],\n        [ 1.,  1.,  1.]])\n>>> B=2*torch.ones(4,3)  #4x3的张量（矩阵）                                    \n>>> B\ntensor([[ 2.,  2.,  2.],\n        [ 2.,  2.,  2.],\n        [ 2.,  2.,  2.],\n        [ 2.,  2.,  2.]])\n>>> C=torch.cat((A,B),0)  #按维数0（行）拼接\n>>> C\ntensor([[ 1.,  1.,  1.],\n         [ 1.,  1.,  1.],\n         [ 2.,  2.,  2.],\n         [ 2.,  2.,  2.],\n         [ 2.,  2.,  2.],\n         [ 2.,  2.,  2.]])\n>>> C.size()\ntorch.Size([6, 3])\n>>> D=2*torch.ones(2,4) #2x4的张量（矩阵）\n>>> C=torch.cat((A,D),1)#按维数1（列）拼接\n>>> C\ntensor([[ 1.,  1.,  1.,  2.,  2.,  2.,  2.],\n        [ 1.,  1.,  1.,  2.,  2.,  2.,  2.]])\n>>> C.size()\ntorch.Size([2, 7])\n~~~\n其次，cat还可以把list中的tensor拼接起来。\n比如：\n\n~~~python\n>>> x = torch.Tensor([ [1],[2],[3] ])\n>>> x1 = [ x*2 for i in range(1,4) ]\n>>> x.shape\ntorch.Size([3,1])\n>>> len(x1)\n3\n>>> x1\n[tensor( [[2.],\n\t\t  [4.],\n          [6.]] ),tensor( [[2.],\n          [4.],\n          [6.]] ),tensor( [[2.],\n          [4.],\n          [6.]] )]\n>>> x2 = torch.cat( (x1),1 )\n>>> x2\ntensor([[2.,2.,2.],\n\t\t[4.,4.,4.],\n        [6.,6.,6.]])\n>>> type(x1)\nlist\n~~~\n上面的代码可以合成一行来写：\n~~~python\n>>> x2 = torch.cat( [x*2 for i in range(1,4)],1 )\n>>> x2\ntensor([[2.,2.,2.],\n\t\t[4.,4.,4.],\n        [6.,6.,6.]])\n~~~","source":"_posts/Pytorch中的torch-cat-函数.md","raw":"title: Pytorch中的torch.cat()函数\nauthor: wangjiansen\ntags:\n  - Pytorch\n  - 深度学习\ncategories:\n  - 深度学习\ndate: 2019-02-09 11:53:00\n---\ncat是concatnate的意思：拼接，联系在一起。\n- - - \n先说cat( )的普通用法\n如果我们有两个tensor是A和B，想把他们拼接在一起，需要如下操作：\n~~~python\nC = torch.cat( (A,B),0 )  #按维数0拼接（竖着拼）\nC = torch.cat( (A,B),1 )  #按维数1拼接（横着拼）\n~~~\n\n~~~python\n>>> import torch\n>>> A=torch.ones(2,3)    #2x3的张量（矩阵）                                     \n>>> A\ntensor([[ 1.,  1.,  1.],\n        [ 1.,  1.,  1.]])\n>>> B=2*torch.ones(4,3)  #4x3的张量（矩阵）                                    \n>>> B\ntensor([[ 2.,  2.,  2.],\n        [ 2.,  2.,  2.],\n        [ 2.,  2.,  2.],\n        [ 2.,  2.,  2.]])\n>>> C=torch.cat((A,B),0)  #按维数0（行）拼接\n>>> C\ntensor([[ 1.,  1.,  1.],\n         [ 1.,  1.,  1.],\n         [ 2.,  2.,  2.],\n         [ 2.,  2.,  2.],\n         [ 2.,  2.,  2.],\n         [ 2.,  2.,  2.]])\n>>> C.size()\ntorch.Size([6, 3])\n>>> D=2*torch.ones(2,4) #2x4的张量（矩阵）\n>>> C=torch.cat((A,D),1)#按维数1（列）拼接\n>>> C\ntensor([[ 1.,  1.,  1.,  2.,  2.,  2.,  2.],\n        [ 1.,  1.,  1.,  2.,  2.,  2.,  2.]])\n>>> C.size()\ntorch.Size([2, 7])\n~~~\n其次，cat还可以把list中的tensor拼接起来。\n比如：\n\n~~~python\n>>> x = torch.Tensor([ [1],[2],[3] ])\n>>> x1 = [ x*2 for i in range(1,4) ]\n>>> x.shape\ntorch.Size([3,1])\n>>> len(x1)\n3\n>>> x1\n[tensor( [[2.],\n\t\t  [4.],\n          [6.]] ),tensor( [[2.],\n          [4.],\n          [6.]] ),tensor( [[2.],\n          [4.],\n          [6.]] )]\n>>> x2 = torch.cat( (x1),1 )\n>>> x2\ntensor([[2.,2.,2.],\n\t\t[4.,4.,4.],\n        [6.,6.,6.]])\n>>> type(x1)\nlist\n~~~\n上面的代码可以合成一行来写：\n~~~python\n>>> x2 = torch.cat( [x*2 for i in range(1,4)],1 )\n>>> x2\ntensor([[2.,2.,2.],\n\t\t[4.,4.,4.],\n        [6.,6.,6.]])\n~~~","slug":"Pytorch中的torch-cat-函数","published":1,"updated":"2019-02-14T15:01:58.523Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjt8bm3tu00029gojw0qrgfbs","content":"<p>cat是concatnate的意思：拼接，联系在一起。</p>\n<hr>\n<p>先说cat( )的普通用法<br>如果我们有两个tensor是A和B，想把他们拼接在一起，需要如下操作：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">C = torch.cat( (A,B),<span class=\"number\">0</span> )  <span class=\"comment\">#按维数0拼接（竖着拼）</span></span><br><span class=\"line\">C = torch.cat( (A,B),<span class=\"number\">1</span> )  <span class=\"comment\">#按维数1拼接（横着拼）</span></span><br></pre></td></tr></table></figure></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>A=torch.ones(<span class=\"number\">2</span>,<span class=\"number\">3</span>)    <span class=\"comment\">#2x3的张量（矩阵）                                     </span></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>A</span><br><span class=\"line\">tensor([[ <span class=\"number\">1.</span>,  <span class=\"number\">1.</span>,  <span class=\"number\">1.</span>],</span><br><span class=\"line\">        [ <span class=\"number\">1.</span>,  <span class=\"number\">1.</span>,  <span class=\"number\">1.</span>]])</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>B=<span class=\"number\">2</span>*torch.ones(<span class=\"number\">4</span>,<span class=\"number\">3</span>)  <span class=\"comment\">#4x3的张量（矩阵）                                    </span></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>B</span><br><span class=\"line\">tensor([[ <span class=\"number\">2.</span>,  <span class=\"number\">2.</span>,  <span class=\"number\">2.</span>],</span><br><span class=\"line\">        [ <span class=\"number\">2.</span>,  <span class=\"number\">2.</span>,  <span class=\"number\">2.</span>],</span><br><span class=\"line\">        [ <span class=\"number\">2.</span>,  <span class=\"number\">2.</span>,  <span class=\"number\">2.</span>],</span><br><span class=\"line\">        [ <span class=\"number\">2.</span>,  <span class=\"number\">2.</span>,  <span class=\"number\">2.</span>]])</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>C=torch.cat((A,B),<span class=\"number\">0</span>)  <span class=\"comment\">#按维数0（行）拼接</span></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>C</span><br><span class=\"line\">tensor([[ <span class=\"number\">1.</span>,  <span class=\"number\">1.</span>,  <span class=\"number\">1.</span>],</span><br><span class=\"line\">         [ <span class=\"number\">1.</span>,  <span class=\"number\">1.</span>,  <span class=\"number\">1.</span>],</span><br><span class=\"line\">         [ <span class=\"number\">2.</span>,  <span class=\"number\">2.</span>,  <span class=\"number\">2.</span>],</span><br><span class=\"line\">         [ <span class=\"number\">2.</span>,  <span class=\"number\">2.</span>,  <span class=\"number\">2.</span>],</span><br><span class=\"line\">         [ <span class=\"number\">2.</span>,  <span class=\"number\">2.</span>,  <span class=\"number\">2.</span>],</span><br><span class=\"line\">         [ <span class=\"number\">2.</span>,  <span class=\"number\">2.</span>,  <span class=\"number\">2.</span>]])</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>C.size()</span><br><span class=\"line\">torch.Size([<span class=\"number\">6</span>, <span class=\"number\">3</span>])</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>D=<span class=\"number\">2</span>*torch.ones(<span class=\"number\">2</span>,<span class=\"number\">4</span>) <span class=\"comment\">#2x4的张量（矩阵）</span></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>C=torch.cat((A,D),<span class=\"number\">1</span>)<span class=\"comment\">#按维数1（列）拼接</span></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>C</span><br><span class=\"line\">tensor([[ <span class=\"number\">1.</span>,  <span class=\"number\">1.</span>,  <span class=\"number\">1.</span>,  <span class=\"number\">2.</span>,  <span class=\"number\">2.</span>,  <span class=\"number\">2.</span>,  <span class=\"number\">2.</span>],</span><br><span class=\"line\">        [ <span class=\"number\">1.</span>,  <span class=\"number\">1.</span>,  <span class=\"number\">1.</span>,  <span class=\"number\">2.</span>,  <span class=\"number\">2.</span>,  <span class=\"number\">2.</span>,  <span class=\"number\">2.</span>]])</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>C.size()</span><br><span class=\"line\">torch.Size([<span class=\"number\">2</span>, <span class=\"number\">7</span>])</span><br></pre></td></tr></table></figure>\n<p>其次，cat还可以把list中的tensor拼接起来。<br>比如：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>x = torch.Tensor([ [<span class=\"number\">1</span>],[<span class=\"number\">2</span>],[<span class=\"number\">3</span>] ])</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>x1 = [ x*<span class=\"number\">2</span> <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">1</span>,<span class=\"number\">4</span>) ]</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>x.shape</span><br><span class=\"line\">torch.Size([<span class=\"number\">3</span>,<span class=\"number\">1</span>])</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>len(x1)</span><br><span class=\"line\"><span class=\"number\">3</span></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>x1</span><br><span class=\"line\">[tensor( [[<span class=\"number\">2.</span>],</span><br><span class=\"line\">\t\t  [<span class=\"number\">4.</span>],</span><br><span class=\"line\">          [<span class=\"number\">6.</span>]] ),tensor( [[<span class=\"number\">2.</span>],</span><br><span class=\"line\">          [<span class=\"number\">4.</span>],</span><br><span class=\"line\">          [<span class=\"number\">6.</span>]] ),tensor( [[<span class=\"number\">2.</span>],</span><br><span class=\"line\">          [<span class=\"number\">4.</span>],</span><br><span class=\"line\">          [<span class=\"number\">6.</span>]] )]</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>x2 = torch.cat( (x1),<span class=\"number\">1</span> )</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>x2</span><br><span class=\"line\">tensor([[<span class=\"number\">2.</span>,<span class=\"number\">2.</span>,<span class=\"number\">2.</span>],</span><br><span class=\"line\">\t\t[<span class=\"number\">4.</span>,<span class=\"number\">4.</span>,<span class=\"number\">4.</span>],</span><br><span class=\"line\">        [<span class=\"number\">6.</span>,<span class=\"number\">6.</span>,<span class=\"number\">6.</span>]])</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>type(x1)</span><br><span class=\"line\">list</span><br></pre></td></tr></table></figure>\n<p>上面的代码可以合成一行来写：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>x2 = torch.cat( [x*<span class=\"number\">2</span> <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">1</span>,<span class=\"number\">4</span>)],<span class=\"number\">1</span> )</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>x2</span><br><span class=\"line\">tensor([[<span class=\"number\">2.</span>,<span class=\"number\">2.</span>,<span class=\"number\">2.</span>],</span><br><span class=\"line\">\t\t[<span class=\"number\">4.</span>,<span class=\"number\">4.</span>,<span class=\"number\">4.</span>],</span><br><span class=\"line\">        [<span class=\"number\">6.</span>,<span class=\"number\">6.</span>,<span class=\"number\">6.</span>]])</span><br></pre></td></tr></table></figure></p>\n","site":{"data":{}},"excerpt":"","more":"<p>cat是concatnate的意思：拼接，联系在一起。</p>\n<hr>\n<p>先说cat( )的普通用法<br>如果我们有两个tensor是A和B，想把他们拼接在一起，需要如下操作：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">C = torch.cat( (A,B),<span class=\"number\">0</span> )  <span class=\"comment\">#按维数0拼接（竖着拼）</span></span><br><span class=\"line\">C = torch.cat( (A,B),<span class=\"number\">1</span> )  <span class=\"comment\">#按维数1拼接（横着拼）</span></span><br></pre></td></tr></table></figure></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>A=torch.ones(<span class=\"number\">2</span>,<span class=\"number\">3</span>)    <span class=\"comment\">#2x3的张量（矩阵）                                     </span></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>A</span><br><span class=\"line\">tensor([[ <span class=\"number\">1.</span>,  <span class=\"number\">1.</span>,  <span class=\"number\">1.</span>],</span><br><span class=\"line\">        [ <span class=\"number\">1.</span>,  <span class=\"number\">1.</span>,  <span class=\"number\">1.</span>]])</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>B=<span class=\"number\">2</span>*torch.ones(<span class=\"number\">4</span>,<span class=\"number\">3</span>)  <span class=\"comment\">#4x3的张量（矩阵）                                    </span></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>B</span><br><span class=\"line\">tensor([[ <span class=\"number\">2.</span>,  <span class=\"number\">2.</span>,  <span class=\"number\">2.</span>],</span><br><span class=\"line\">        [ <span class=\"number\">2.</span>,  <span class=\"number\">2.</span>,  <span class=\"number\">2.</span>],</span><br><span class=\"line\">        [ <span class=\"number\">2.</span>,  <span class=\"number\">2.</span>,  <span class=\"number\">2.</span>],</span><br><span class=\"line\">        [ <span class=\"number\">2.</span>,  <span class=\"number\">2.</span>,  <span class=\"number\">2.</span>]])</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>C=torch.cat((A,B),<span class=\"number\">0</span>)  <span class=\"comment\">#按维数0（行）拼接</span></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>C</span><br><span class=\"line\">tensor([[ <span class=\"number\">1.</span>,  <span class=\"number\">1.</span>,  <span class=\"number\">1.</span>],</span><br><span class=\"line\">         [ <span class=\"number\">1.</span>,  <span class=\"number\">1.</span>,  <span class=\"number\">1.</span>],</span><br><span class=\"line\">         [ <span class=\"number\">2.</span>,  <span class=\"number\">2.</span>,  <span class=\"number\">2.</span>],</span><br><span class=\"line\">         [ <span class=\"number\">2.</span>,  <span class=\"number\">2.</span>,  <span class=\"number\">2.</span>],</span><br><span class=\"line\">         [ <span class=\"number\">2.</span>,  <span class=\"number\">2.</span>,  <span class=\"number\">2.</span>],</span><br><span class=\"line\">         [ <span class=\"number\">2.</span>,  <span class=\"number\">2.</span>,  <span class=\"number\">2.</span>]])</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>C.size()</span><br><span class=\"line\">torch.Size([<span class=\"number\">6</span>, <span class=\"number\">3</span>])</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>D=<span class=\"number\">2</span>*torch.ones(<span class=\"number\">2</span>,<span class=\"number\">4</span>) <span class=\"comment\">#2x4的张量（矩阵）</span></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>C=torch.cat((A,D),<span class=\"number\">1</span>)<span class=\"comment\">#按维数1（列）拼接</span></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>C</span><br><span class=\"line\">tensor([[ <span class=\"number\">1.</span>,  <span class=\"number\">1.</span>,  <span class=\"number\">1.</span>,  <span class=\"number\">2.</span>,  <span class=\"number\">2.</span>,  <span class=\"number\">2.</span>,  <span class=\"number\">2.</span>],</span><br><span class=\"line\">        [ <span class=\"number\">1.</span>,  <span class=\"number\">1.</span>,  <span class=\"number\">1.</span>,  <span class=\"number\">2.</span>,  <span class=\"number\">2.</span>,  <span class=\"number\">2.</span>,  <span class=\"number\">2.</span>]])</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>C.size()</span><br><span class=\"line\">torch.Size([<span class=\"number\">2</span>, <span class=\"number\">7</span>])</span><br></pre></td></tr></table></figure>\n<p>其次，cat还可以把list中的tensor拼接起来。<br>比如：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>x = torch.Tensor([ [<span class=\"number\">1</span>],[<span class=\"number\">2</span>],[<span class=\"number\">3</span>] ])</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>x1 = [ x*<span class=\"number\">2</span> <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">1</span>,<span class=\"number\">4</span>) ]</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>x.shape</span><br><span class=\"line\">torch.Size([<span class=\"number\">3</span>,<span class=\"number\">1</span>])</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>len(x1)</span><br><span class=\"line\"><span class=\"number\">3</span></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>x1</span><br><span class=\"line\">[tensor( [[<span class=\"number\">2.</span>],</span><br><span class=\"line\">\t\t  [<span class=\"number\">4.</span>],</span><br><span class=\"line\">          [<span class=\"number\">6.</span>]] ),tensor( [[<span class=\"number\">2.</span>],</span><br><span class=\"line\">          [<span class=\"number\">4.</span>],</span><br><span class=\"line\">          [<span class=\"number\">6.</span>]] ),tensor( [[<span class=\"number\">2.</span>],</span><br><span class=\"line\">          [<span class=\"number\">4.</span>],</span><br><span class=\"line\">          [<span class=\"number\">6.</span>]] )]</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>x2 = torch.cat( (x1),<span class=\"number\">1</span> )</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>x2</span><br><span class=\"line\">tensor([[<span class=\"number\">2.</span>,<span class=\"number\">2.</span>,<span class=\"number\">2.</span>],</span><br><span class=\"line\">\t\t[<span class=\"number\">4.</span>,<span class=\"number\">4.</span>,<span class=\"number\">4.</span>],</span><br><span class=\"line\">        [<span class=\"number\">6.</span>,<span class=\"number\">6.</span>,<span class=\"number\">6.</span>]])</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>type(x1)</span><br><span class=\"line\">list</span><br></pre></td></tr></table></figure>\n<p>上面的代码可以合成一行来写：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>x2 = torch.cat( [x*<span class=\"number\">2</span> <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">1</span>,<span class=\"number\">4</span>)],<span class=\"number\">1</span> )</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>x2</span><br><span class=\"line\">tensor([[<span class=\"number\">2.</span>,<span class=\"number\">2.</span>,<span class=\"number\">2.</span>],</span><br><span class=\"line\">\t\t[<span class=\"number\">4.</span>,<span class=\"number\">4.</span>,<span class=\"number\">4.</span>],</span><br><span class=\"line\">        [<span class=\"number\">6.</span>,<span class=\"number\">6.</span>,<span class=\"number\">6.</span>]])</span><br></pre></td></tr></table></figure></p>\n"},{"title":"Ubuntu18.04优化美化踩坑记录","author":"王建森","date":"2019-03-06T13:14:00.000Z","_content":"喜欢Ubuntu的一个好处就是相比win来说Linux可以更方便的对系统进行一些修改，而且安装很多东西很方便，相比mac可以充分利用硬件（mac没有N卡，配置深度学习的时候不能用显卡跑代码）。作为一个颜控，最重要的当然是Ubuntu界面很舒服很漂亮～～～～\n\n\n自从接触了Ubuntu之后就开始了折腾之旅，装双系统、分区、熟悉操作命令、配置环境、美化界面等等，对于一个小白来说，过程还是很艰辛的。很早之前就想把折腾的过程记录下来了，但是太懒啦，哈哈哈。这两天室友刚开始用Ubuntu，激励我记录一下，文章大概可以分成：**显卡配置和电源管理**、**美化系统主题**、**美化grub启动项**、**基础命令操作**，这几个大部分。\n_ _ _\n# 显卡配置和电源管理\n我的电脑是小米，刚装Ubuntu的时候真的是为发烧而生，风扇声音比我散热器的声音都大，非常hot并且导致续航时间严重缩短（当初我换新电脑的时候可主要因为原来电脑续航不够哇），上网查原因主要是显卡驱动问题和Ubuntn没有win那么好的电源管理导致的。所以主要从这两个方面入手。\n## TLP电源管理软件\n首先TLP是免费的，可以减少电脑发热量和增加笔记本电池使用时间的电源管理工具。它是轻量级的工具，没有GUI，不用进行大量配置，一般的电脑使用默认配置就可以了。但是默认的配置会导致系统把显卡驱动切换到集成显卡上，所以一会我们还要管理一下显卡的驱动，首先介绍tlp的安装，随后介绍tlp的配置文件各代表了什么含义。\n### tlp的安装\n\n```\n添加PPA：\nsudo add-apt-repository ppa:linrunner/tlp\n更新软件列表：\nsudo apt-get update\n安装TLP：\nsudo apt install tlp\n启动TLP：\nsudo tlp start\n>>> TLP started in AC mode.\n```\n\n### tlp的配置文件\n现在 TLP 已经被启动起来了，而且已经设置好了节省电池所需要的默认配置。我们可以查看该配置文件。文件路径为 /etc/default/tlp。我们需要编辑该文件来修改各项配置。配置的一些示例如下：\n```\n# ------------------------------------------------------------------------------\n# tlp - Parameters for power save\n# See full explanation: http://linrunner.de/en/tlp/docs/tlp-configuration.html\n\n# Hint: some features are disabled by default, remove the leading # to enable      #通过去掉“#”来开启参数\n# them.\n\n# Set to 0 to disable, 1 to enable TLP.    #设置为‘“1”启用TLP服务\nTLP_ENABLE=1\n\n# Operation mode when no power supply can be detected: AC, BAT\n# Concerns some desktop and embedded hardware only.\nTLP_DEFAULT_MODE=AC\n\n# Seconds laptop mode has to wait after the disk goes idle before doing a sync.\n# Non-zero value enables, zero disables laptop mode.\nDISK_IDLE_SECS_ON_AC=0\nDISK_IDLE_SECS_ON_BAT=2\n\n# Dirty page values (timeouts in secs).\nMAX_LOST_WORK_SECS_ON_AC=30\nMAX_LOST_WORK_SECS_ON_BAT=90\n\n# Hint: CPU parameters below are disabled by default, remove the leading #\n# to enable them, otherwise kernel default values are used.\n\n# Select a CPU frequency scaling governor.                      #CPU调度策略\n# Intel Core i processor with intel_pstate driver:\n#   powersave(*), performance\n# Older hardware with acpi-cpufreq driver:\n#   ondemand(*), powersave, performance, conservative\n# (*) is recommended.\n# Hint: use tlp-stat -p to show the active driver and available governors.\n# Important:\n#   You *must* disable your distribution's governor settings or conflicts will\n#   occur. ondemand is sufficient for *almost all* workloads, you should know\n#   what you're doing!\nCPU_SCALING_GOVERNOR_ON_AC=powersave\nCPU_SCALING_GOVERNOR_ON_BAT=powersave\n\n# Set the min/max frequency available for the scaling governor.\n# Possible values strongly depend on your CPU. For available frequencies see\n# the output of tlp-stat -p.\n#CPU_SCALING_MIN_FREQ_ON_AC=0\n#CPU_SCALING_MAX_FREQ_ON_AC=0\n#CPU_SCALING_MIN_FREQ_ON_BAT=0\n#CPU_SCALING_MAX_FREQ_ON_BAT=0\n\n# Set Intel P-state performance: 0..100 (%)\n# Limit the max/min P-state to control the power dissipation of the CPU.\n# Values are stated as a percentage of the available performance.\n# Requires an Intel Core i processor with intel_pstate driver.\nCPU_MIN_PERF_ON_AC=0\nCPU_MAX_PERF_ON_AC=100\nCPU_MIN_PERF_ON_BAT=0\nCPU_MAX_PERF_ON_BAT=30\n\n# Set the CPU \"turbo boost\" feature: 0=disable, 1=allow       #开启intel cpu 睿频\n# Requires an Intel Core i processor.\n# Important:\n# - This may conflict with your distribution's governor settings\n# - A value of 1 does *not* activate boosting, it just allows it\nCPU_BOOST_ON_AC=1\nCPU_BOOST_ON_BAT=1\n\n# Minimize number of used CPU cores/hyper-threads under light load conditions        #与cpu有关\nSCHED_POWERSAVE_ON_AC=0\nSCHED_POWERSAVE_ON_BAT=1\n\n# Kernel NMI Watchdog:\n#   0=disable (default, saves power), 1=enable (for kernel debugging only)\nNMI_WATCHDOG=0\n\n# Change CPU voltages aka \"undervolting\" - Kernel with PHC patch required     #调节CPU 电压以达到节能的目的 ，谨慎开启！！！\n# Frequency voltage pairs are written to:\n#   /sys/devices/system/cpu/cpu0/cpufreq/phc_controls\n# CAUTION: only use this, if you thoroughly understand what you are doing!\n#PHC_CONTROLS=\"F:V F:V F:V F:V\"\n\n# Set CPU performance versus energy savings policy:              #与CPU有关\n#   performance, normal, powersave\n# Requires kernel module msr and x86_energy_perf_policy from linux-tools\nENERGY_PERF_POLICY_ON_AC=normal\nENERGY_PERF_POLICY_ON_BAT=powersave\n\n# Hard disk devices; separate multiple devices with spaces (default: sda).\n# Devices can be specified by disk ID also (lookup with: tlp diskid).\nDISK_DEVICES=\"sda sdb\"\n\n# Hard disk advanced power management level: 1..254, 255 (max saving, min, off)\n# Levels 1..127 may spin down the disk; 255 allowable on most drives.\n# Separate values for multiple disks with spaces. Use the special value 'keep'\n# to keep the hardware default for the particular disk.\nDISK_APM_LEVEL_ON_AC=\"254 254\"\nDISK_APM_LEVEL_ON_BAT=\"128 128\"\n\n# Hard disk spin down timeout:\n#   0:        spin down disabled\n#   1..240:   timeouts from 5s to 20min (in units of 5s)\n#   241..251: timeouts from 30min to 5.5 hours (in units of 30min)\n# See 'man hdparm' for details.\n# Separate values for multiple disks with spaces. Use the special value 'keep'\n# to keep the hardware default for the particular disk.\n#DISK_SPINDOWN_TIMEOUT_ON_AC=\"0 0\"\n#DISK_SPINDOWN_TIMEOUT_ON_BAT=\"0 0\"\n\n# Select IO scheduler for the disk devices: cfq, deadline, noop (Default: cfq);     #选择磁盘驱动器I/O调度方式，建议deadline\n# Separate values for multiple disks with spaces. Use the special value 'keep'\n# to keep the kernel default scheduler for the particular disk.\nDISK_IOSCHED=\"deadline cfq\"\n\n# SATA aggressive link power management (ALPM):\n#   min_power, medium_power, max_performance\nSATA_LINKPWR_ON_AC=max_performance\nSATA_LINKPWR_ON_BAT=min_power\n\n# Exclude SATA host devices from link power management.\n# Separate multiple hosts with spaces.\n#SATA_LINKPWR_BLACKLIST=\"host1\"\n\n# Runtime Power Management for AHCI controllers and disks:      #请谨慎开启，有可能导致磁盘被锁或者数据丢失\n#   on=disable, auto=enable\n# EXPERIMENTAL ** WARNING: auto will most likely cause system lockups/data loss\n#AHCI_RUNTIME_PM_ON_AC=on\n#AHCI_RUNTIME_PM_ON_BAT=on\n\n# Seconds of inactivity before disk is suspended\nAHCI_RUNTIME_PM_TIMEOUT=15\n\n# PCI Express Active State Power Management (PCIe ASPM):\n#   default, performance, powersave\nPCIE_ASPM_ON_AC=performance\nPCIE_ASPM_ON_BAT=powersave\n\n# Radeon graphics clock speed (profile method): low, mid, high, auto, default;      #NVIDIA显卡用户请无视或者禁用\n# auto = mid on BAT, high on AC; default = use hardware defaults.\n# (Kernel >= 2.6.35 only, open-source radeon driver explicitly)\n#RADEON_POWER_PROFILE_ON_AC=high\n#RADEON_POWER_PROFILE_ON_BAT=low\n\n# Radeon dynamic power management method (DPM): battery, performance        #NVIDIA显卡用户请无视或者禁用\n# (Kernel >= 3.11 only, requires boot option radeon.dpm=1)\n#RADEON_DPM_STATE_ON_AC=performance\n#RADEON_DPM_STATE_ON_BAT=battery\n\n# Radeon DPM performance level: auto, low, high; auto is recommended .           #NVIDIA显卡用户请无视或者禁用\n#RADEON_DPM_PERF_LEVEL_ON_AC=auto\n#RADEON_DPM_PERF_LEVEL_ON_BAT=auto\n\n# WiFi power saving mode: on=enable, off=disable; not supported by all adapters.\nWIFI_PWR_ON_AC=off\nWIFI_PWR_ON_BAT=on\n\n# Disable wake on LAN: Y/N                 #禁用WOL\nWOL_DISABLE=N\n\n# Enable audio power saving for Intel HDA, AC97 devices (timeout in secs).     #与音频有关\n# A value of 0 disables, >=1 enables power save.\nSOUND_POWER_SAVE_ON_AC=0\nSOUND_POWER_SAVE_ON_BAT=1\n\n# Disable controller too (HDA only): Y/N            #与音频有关\nSOUND_POWER_SAVE_CONTROLLER=Y\n\n# Set to 1 to power off optical drive in UltraBay/MediaBay when running on\n# battery. A value of 0 disables this feature (Default).\n# Drive can be powered on again by releasing (and reinserting) the eject lever\n# or by pressing the disc eject button on newer models.\n# Note: an UltraBay/MediaBay hard disk is never powered off.\nBAY_POWEROFF_ON_BAT=0\n# Optical drive device to power off (default sr0).\nBAY_DEVICE=\"sr0\"\n\n# Runtime Power Management for PCI(e) bus devices: on=disable, auto=enable\nRUNTIME_PM_ON_AC=on\nRUNTIME_PM_ON_BAT=auto\n\n# Runtime PM for *all* PCI(e) bus devices, except blacklisted ones:\n#   0=disable, 1=enable\nRUNTIME_PM_ALL=1\n\n# Exclude PCI(e) device adresses the following list from Runtime PM\n# (separate with spaces). Use lspci to get the adresses (1st column).\n#RUNTIME_PM_BLACKLIST=\"bb:dd.f 11:22.3 44:55.6\"\n\n# Exclude PCI(e) devices assigned to the listed drivers from Runtime PM\n# (should prevent accidential power on of hybrid graphics' discrete part).\n# Default is \"radeon nouveau\"; use \"\" to disable the feature completely.\n# Separate multiple drivers with spaces.\nRUNTIME_PM_DRIVER_BLACKLIST=\"radeon nouveau\"\n\n# Set to 0 to disable, 1 to enable USB autosuspend feature.\nUSB_AUTOSUSPEND=1\n\n# Exclude listed devices from USB autosuspend (separate with spaces).\n# Use lsusb to get the ids.\n# Note: input devices (usbhid) are excluded automatically (see below)\n#USB_BLACKLIST=\"1111:2222 3333:4444\"\n\n# WWAN devices are excluded from USB autosuspend: 0=do not exclude / 1=exclude\nUSB_BLACKLIST_WWAN=1\n\n# Include listed devices into USB autosuspend even if already excluded\n# by the driver or WWAN blacklists above (separate with spaces).\n# Use lsusb to get the ids.\n#USB_WHITELIST=\"1111:2222 3333:4444\"\n\n# Set to 1 to disable autosuspend before shutdown, 0 to do nothing\n# (workaround for USB devices that cause shutdown problems).\n#USB_AUTOSUSPEND_DISABLE_ON_SHUTDOWN=1\n\n# Restore radio device state (Bluetooth, WiFi, WWAN) from previous shutdown\n# on system startup: 0=disable, 1=enable.\n# Hint: the parameters DEVICES_TO_DISABLE/ENABLE_ON_STARTUP/SHUTDOWN below\n#   are ignored when this is enabled!\nRESTORE_DEVICE_STATE_ON_STARTUP=0\n\n# Radio devices to disable on startup: bluetooth, wifi, wwan.\n# Separate multiple devices with spaces.\n#DEVICES_TO_DISABLE_ON_STARTUP=\"bluetooth wifi wwan\"\n\n# Radio devices to enable on startup: bluetooth, wifi, wwan.\n# Separate multiple devices with spaces.\n#DEVICES_TO_ENABLE_ON_STARTUP=\"wifi\"\n\n# Radio devices to disable on shutdown: bluetooth, wifi, wwan\n# (workaround for devices that are blocking shutdown).\n#DEVICES_TO_DISABLE_ON_SHUTDOWN=\"bluetooth wifi wwan\"\n\n# Radio devices to enable on shutdown: bluetooth, wifi, wwan\n# (to prevent other operating systems from missing radios).\n#DEVICES_TO_ENABLE_ON_SHUTDOWN=\"wwan\"\n\n# Radio devices to enable on AC: bluetooth, wifi, wwan\n#DEVICES_TO_ENABLE_ON_AC=\"bluetooth wifi wwan\"\n\n# Radio devices to disable on battery: bluetooth, wifi, wwan\n#DEVICES_TO_DISABLE_ON_BAT=\"bluetooth wifi wwan\"\n\n# Radio devices to disable on battery when not in use (not connected):\n# bluetooth, wifi, wwan\n#DEVICES_TO_DISABLE_ON_BAT_NOT_IN_USE=\"bluetooth wifi wwan\"\n\n# Battery charge thresholds (ThinkPad only, tp-smapi or acpi-call kernel module             #ThinkPad笔记本使用，其它品牌谨慎开启\n# required). Charging starts when the remaining capacity falls below the\n# START_CHARGE_THRESH value and stops when exceeding the STOP_CHARGE_THRESH value.\n# Main / Internal battery (values in %)\n#START_CHARGE_THRESH_BAT0=75\n#STOP_CHARGE_THRESH_BAT0=80\n# Ultrabay / Slice / Replaceable battery (values in %)\n#START_CHARGE_THRESH_BAT1=75\n#STOP_CHARGE_THRESH_BAT1=80\n\n# ------------------------------------------------------------------------------\n# tlp-rdw - Parameters for the radio device wizard\n# Possible devices: bluetooth, wifi, wwan\n\n# Hints:\n# - Parameters are disabled by default, remove the leading # to enable them.\n# - Separate multiple radio devices with spaces.\n\n# Radio devices to disable on connect.\n#DEVICES_TO_DISABLE_ON_LAN_CONNECT=\"wifi wwan\"\n#DEVICES_TO_DISABLE_ON_WIFI_CONNECT=\"wwan\"\n#DEVICES_TO_DISABLE_ON_WWAN_CONNECT=\"wifi\"\n\n# Radio devices to enable on disconnect.\n#DEVICES_TO_ENABLE_ON_LAN_DISCONNECT=\"wifi wwan\"\n#DEVICES_TO_ENABLE_ON_WIFI_DISCONNECT=\"\"\n#DEVICES_TO_ENABLE_ON_WWAN_DISCONNECT=\"\"\n\n# Radio devices to enable/disable when docked.\n#DEVICES_TO_ENABLE_ON_DOCK=\"\"\n#DEVICES_TO_DISABLE_ON_DOCK=\"\"\n\n# Radio devices to enable/disable when undocked.\n#DEVICES_TO_ENABLE_ON_UNDOCK=\"wifi\"\n#DEVICES_TO_DISABLE_ON_UNDOCK=\"\"\n```\n我主要是在cpu那个部分做了一些调整，一个i7的电脑，硬生生被调成了i5的性能，我真的是后悔当时没直接买i5。更详细的信息可以参考[官方文档](https://linrunner.de/en/tlp/tlp.html).\n## 显卡配置\n因为跑深度学习的代码需要用很多N卡配置，而默认不使用tlp的话电脑会很热，用了tlp系统会自动的把显卡切换到intel的集成显卡，这个问题一开始苦恼了我好久。其实ubuntu里面可以手动切换显卡的。前提是需要正确安装电脑对应的显卡，我们需要在N卡的驱动附加软件里面实现这个显卡切换功能。ps.Linux上显卡驱动真的是个大坑~\n![upload successful](/images/pasted-14.png)可以看到我现在用的是intel的集成显卡，而实际上我是有N卡的。\n### 正确安装对应驱动\n如果我们刚安装完ubuntu新系统的话，我们正常在上图看到的显卡类型应该是**nouveau**的驱动，nouveau是一个自由及开放源代码显卡驱动程序，是为Nvidia的显示卡所编写，也可用于属于系统芯片的NVIDIA Tegra系列，该项目的目标为利用逆向工程Nvidia的专有Linux驱动程序来创造一个开放源代码的驱动程序，所以nouveau开源驱动基本上是不能正常使用的。\n首先查看电脑目前有什么显卡，使用如下命令：\n```\nlspci | grep VGA     # 查看集成显卡\nlspci | grep NVIDIA  # 查看NVIDIA显卡\n```\n我的电脑是小米pro，可以看出显卡是GeForce MX150的。\n![upload successful](/images/pasted-16.png)\n\n下面查看应该安装什么版本的驱动，如下命令：\n\n```\nubuntu-drivers devices   # 查询所有ubuntu推荐的驱动\n```\n\n可以看出ubutnu提示我应该安装390版本的驱动。\n![upload successful](/images/pasted-17.png)\n\n然后就可以使用下面条命令安装所有推荐的驱动程序：\n\n```\nsudo ubuntu-drivers autoinstall\n```\n\n或者\n```\nsudo apt-get install nvidia-390\n```\n\n\n\n安装完成后重启，在**软件和更新**中我们可以看到已经安装了哪些驱动：\n![upload successful](/images/pasted-18.png)\n可以看到我已经安装了390和nouveau这两个驱动，勾选390那个就行。\n### 切换显卡\n随后我们就可以使用使用nvidia-settings命令了：\n```\nnvidia-settings\n```\n我们可以在这里选择我们要使用哪个显卡，切换之后reboot就好了。\n![upload successful](/images/pasted-19.png)\n\n还有最后一点注意的！！！！！！！！！！！！！\n\n**如果安装cuda的话，一定不再选择安装驱动，要不然就前功尽弃啦！！！**\n# 美化系统主题\n\n作为一个颜控党，主题这种东西如果不和为空是万万不可以的。\n从ubuntu17.10开始，官方又开始使用gnome作为默认的桌面环境，意味着我们可以轻松的使用GNOME Shell扩展了，美滋滋。先放几张图：\n![upload successful](/images/pasted-20.png)\n\n![upload successful](/images/pasted-21.png)\n\n![upload successful](/images/pasted-22.png)搞了个北欧风，十分舒服～～\n\n首先我们要安装GNOME Tweak Tool。\n\n```\nsudo apt install gnome-tweak-tool\n```\n安装之后你就能找到这个东西：\n\n![upload successful](/images/pasted-23.png)\n打开：\n![upload successful](/images/pasted-24.png)\n\n在这里可以配置不同的外观，也可以安装不同的扩展小插件，在这里我已经安装好配置主题需要的扩展了，正常来说我们需要先安装User themes这个扩展。\n![upload successful](/images/pasted-25.png)\n我们可通过chrome来方便的安装任何扩展，首先我们需要在chrome里面安装GNOME的小程序：!\n![upload successful](/images/pasted-26.png)\n\n随后浏览器打开https://extensions.gnome.org/ 安装这个扩展：\n![upload successful](/images/pasted-27.png)\n点进去，把开关调成on状态就好了：\n![upload successful](/images/pasted-28.png)\n随后我们就能使用主题包来装饰我们的系统了。\n\n对于Gnome桌面，你最需要连接的就是这个[网站](https://www.gnome-look.org/)，它提供了包括主题、图标、字体等在内的很多包。 \n因为在上述中安装了User Themes 扩展，所以我们可以把下载好的主题放置在自己的家目录下，为此，在家目录下的.local/share中新建themes、fonts、icons 三个文件夹，分别存放主题、字体和图标 。下载我们喜欢的主题和图标之后（一般是一个压缩包），我们需要把压缩包放到对应的文件夹里，并且进行解压缩，这里提供几个常用的命令：\n```\n//解压\n$ xz -d node-v6.10.1-linux-x64.tar.xz\n$ tar -xvf node-v6.10.1-linux-x64.tar\n\n//移动文件\ncp -r 文件 ~/.local/share/icons\n\n//本机gnome主题、字体、图标地址(https://www.gnome-look.org/)\n~/.local/share/themes\n~/.local/share/fonts\n~/.local/share/icons\n```\n上面的步骤完成后，我们就可以在这里进行修改了：\n![upload successful](/images/pasted-29.png)\n# 美化grub启动项\n个人是不太喜欢紫色的，尤其开机grub引导项的那个基佬紫，这的受不了啊，要想个办法改一下。\n这个部分我主要参考的是这个[博主](https://blog.csdn.net/w84963568/article/details/78884003)的文章，所以不过多的介绍了。\n# 基础命令操作\n\n未完待续。。。","source":"_posts/Ubuntu18-04深度优化美化记录.md","raw":"title: Ubuntu18.04优化美化踩坑记录\nauthor: 王建森\ntags:\n  - Ubuntu\ncategories:\n  - Ubuntu\ndate: 2019-03-06 21:14:00\n---\n喜欢Ubuntu的一个好处就是相比win来说Linux可以更方便的对系统进行一些修改，而且安装很多东西很方便，相比mac可以充分利用硬件（mac没有N卡，配置深度学习的时候不能用显卡跑代码）。作为一个颜控，最重要的当然是Ubuntu界面很舒服很漂亮～～～～\n\n\n自从接触了Ubuntu之后就开始了折腾之旅，装双系统、分区、熟悉操作命令、配置环境、美化界面等等，对于一个小白来说，过程还是很艰辛的。很早之前就想把折腾的过程记录下来了，但是太懒啦，哈哈哈。这两天室友刚开始用Ubuntu，激励我记录一下，文章大概可以分成：**显卡配置和电源管理**、**美化系统主题**、**美化grub启动项**、**基础命令操作**，这几个大部分。\n_ _ _\n# 显卡配置和电源管理\n我的电脑是小米，刚装Ubuntu的时候真的是为发烧而生，风扇声音比我散热器的声音都大，非常hot并且导致续航时间严重缩短（当初我换新电脑的时候可主要因为原来电脑续航不够哇），上网查原因主要是显卡驱动问题和Ubuntn没有win那么好的电源管理导致的。所以主要从这两个方面入手。\n## TLP电源管理软件\n首先TLP是免费的，可以减少电脑发热量和增加笔记本电池使用时间的电源管理工具。它是轻量级的工具，没有GUI，不用进行大量配置，一般的电脑使用默认配置就可以了。但是默认的配置会导致系统把显卡驱动切换到集成显卡上，所以一会我们还要管理一下显卡的驱动，首先介绍tlp的安装，随后介绍tlp的配置文件各代表了什么含义。\n### tlp的安装\n\n```\n添加PPA：\nsudo add-apt-repository ppa:linrunner/tlp\n更新软件列表：\nsudo apt-get update\n安装TLP：\nsudo apt install tlp\n启动TLP：\nsudo tlp start\n>>> TLP started in AC mode.\n```\n\n### tlp的配置文件\n现在 TLP 已经被启动起来了，而且已经设置好了节省电池所需要的默认配置。我们可以查看该配置文件。文件路径为 /etc/default/tlp。我们需要编辑该文件来修改各项配置。配置的一些示例如下：\n```\n# ------------------------------------------------------------------------------\n# tlp - Parameters for power save\n# See full explanation: http://linrunner.de/en/tlp/docs/tlp-configuration.html\n\n# Hint: some features are disabled by default, remove the leading # to enable      #通过去掉“#”来开启参数\n# them.\n\n# Set to 0 to disable, 1 to enable TLP.    #设置为‘“1”启用TLP服务\nTLP_ENABLE=1\n\n# Operation mode when no power supply can be detected: AC, BAT\n# Concerns some desktop and embedded hardware only.\nTLP_DEFAULT_MODE=AC\n\n# Seconds laptop mode has to wait after the disk goes idle before doing a sync.\n# Non-zero value enables, zero disables laptop mode.\nDISK_IDLE_SECS_ON_AC=0\nDISK_IDLE_SECS_ON_BAT=2\n\n# Dirty page values (timeouts in secs).\nMAX_LOST_WORK_SECS_ON_AC=30\nMAX_LOST_WORK_SECS_ON_BAT=90\n\n# Hint: CPU parameters below are disabled by default, remove the leading #\n# to enable them, otherwise kernel default values are used.\n\n# Select a CPU frequency scaling governor.                      #CPU调度策略\n# Intel Core i processor with intel_pstate driver:\n#   powersave(*), performance\n# Older hardware with acpi-cpufreq driver:\n#   ondemand(*), powersave, performance, conservative\n# (*) is recommended.\n# Hint: use tlp-stat -p to show the active driver and available governors.\n# Important:\n#   You *must* disable your distribution's governor settings or conflicts will\n#   occur. ondemand is sufficient for *almost all* workloads, you should know\n#   what you're doing!\nCPU_SCALING_GOVERNOR_ON_AC=powersave\nCPU_SCALING_GOVERNOR_ON_BAT=powersave\n\n# Set the min/max frequency available for the scaling governor.\n# Possible values strongly depend on your CPU. For available frequencies see\n# the output of tlp-stat -p.\n#CPU_SCALING_MIN_FREQ_ON_AC=0\n#CPU_SCALING_MAX_FREQ_ON_AC=0\n#CPU_SCALING_MIN_FREQ_ON_BAT=0\n#CPU_SCALING_MAX_FREQ_ON_BAT=0\n\n# Set Intel P-state performance: 0..100 (%)\n# Limit the max/min P-state to control the power dissipation of the CPU.\n# Values are stated as a percentage of the available performance.\n# Requires an Intel Core i processor with intel_pstate driver.\nCPU_MIN_PERF_ON_AC=0\nCPU_MAX_PERF_ON_AC=100\nCPU_MIN_PERF_ON_BAT=0\nCPU_MAX_PERF_ON_BAT=30\n\n# Set the CPU \"turbo boost\" feature: 0=disable, 1=allow       #开启intel cpu 睿频\n# Requires an Intel Core i processor.\n# Important:\n# - This may conflict with your distribution's governor settings\n# - A value of 1 does *not* activate boosting, it just allows it\nCPU_BOOST_ON_AC=1\nCPU_BOOST_ON_BAT=1\n\n# Minimize number of used CPU cores/hyper-threads under light load conditions        #与cpu有关\nSCHED_POWERSAVE_ON_AC=0\nSCHED_POWERSAVE_ON_BAT=1\n\n# Kernel NMI Watchdog:\n#   0=disable (default, saves power), 1=enable (for kernel debugging only)\nNMI_WATCHDOG=0\n\n# Change CPU voltages aka \"undervolting\" - Kernel with PHC patch required     #调节CPU 电压以达到节能的目的 ，谨慎开启！！！\n# Frequency voltage pairs are written to:\n#   /sys/devices/system/cpu/cpu0/cpufreq/phc_controls\n# CAUTION: only use this, if you thoroughly understand what you are doing!\n#PHC_CONTROLS=\"F:V F:V F:V F:V\"\n\n# Set CPU performance versus energy savings policy:              #与CPU有关\n#   performance, normal, powersave\n# Requires kernel module msr and x86_energy_perf_policy from linux-tools\nENERGY_PERF_POLICY_ON_AC=normal\nENERGY_PERF_POLICY_ON_BAT=powersave\n\n# Hard disk devices; separate multiple devices with spaces (default: sda).\n# Devices can be specified by disk ID also (lookup with: tlp diskid).\nDISK_DEVICES=\"sda sdb\"\n\n# Hard disk advanced power management level: 1..254, 255 (max saving, min, off)\n# Levels 1..127 may spin down the disk; 255 allowable on most drives.\n# Separate values for multiple disks with spaces. Use the special value 'keep'\n# to keep the hardware default for the particular disk.\nDISK_APM_LEVEL_ON_AC=\"254 254\"\nDISK_APM_LEVEL_ON_BAT=\"128 128\"\n\n# Hard disk spin down timeout:\n#   0:        spin down disabled\n#   1..240:   timeouts from 5s to 20min (in units of 5s)\n#   241..251: timeouts from 30min to 5.5 hours (in units of 30min)\n# See 'man hdparm' for details.\n# Separate values for multiple disks with spaces. Use the special value 'keep'\n# to keep the hardware default for the particular disk.\n#DISK_SPINDOWN_TIMEOUT_ON_AC=\"0 0\"\n#DISK_SPINDOWN_TIMEOUT_ON_BAT=\"0 0\"\n\n# Select IO scheduler for the disk devices: cfq, deadline, noop (Default: cfq);     #选择磁盘驱动器I/O调度方式，建议deadline\n# Separate values for multiple disks with spaces. Use the special value 'keep'\n# to keep the kernel default scheduler for the particular disk.\nDISK_IOSCHED=\"deadline cfq\"\n\n# SATA aggressive link power management (ALPM):\n#   min_power, medium_power, max_performance\nSATA_LINKPWR_ON_AC=max_performance\nSATA_LINKPWR_ON_BAT=min_power\n\n# Exclude SATA host devices from link power management.\n# Separate multiple hosts with spaces.\n#SATA_LINKPWR_BLACKLIST=\"host1\"\n\n# Runtime Power Management for AHCI controllers and disks:      #请谨慎开启，有可能导致磁盘被锁或者数据丢失\n#   on=disable, auto=enable\n# EXPERIMENTAL ** WARNING: auto will most likely cause system lockups/data loss\n#AHCI_RUNTIME_PM_ON_AC=on\n#AHCI_RUNTIME_PM_ON_BAT=on\n\n# Seconds of inactivity before disk is suspended\nAHCI_RUNTIME_PM_TIMEOUT=15\n\n# PCI Express Active State Power Management (PCIe ASPM):\n#   default, performance, powersave\nPCIE_ASPM_ON_AC=performance\nPCIE_ASPM_ON_BAT=powersave\n\n# Radeon graphics clock speed (profile method): low, mid, high, auto, default;      #NVIDIA显卡用户请无视或者禁用\n# auto = mid on BAT, high on AC; default = use hardware defaults.\n# (Kernel >= 2.6.35 only, open-source radeon driver explicitly)\n#RADEON_POWER_PROFILE_ON_AC=high\n#RADEON_POWER_PROFILE_ON_BAT=low\n\n# Radeon dynamic power management method (DPM): battery, performance        #NVIDIA显卡用户请无视或者禁用\n# (Kernel >= 3.11 only, requires boot option radeon.dpm=1)\n#RADEON_DPM_STATE_ON_AC=performance\n#RADEON_DPM_STATE_ON_BAT=battery\n\n# Radeon DPM performance level: auto, low, high; auto is recommended .           #NVIDIA显卡用户请无视或者禁用\n#RADEON_DPM_PERF_LEVEL_ON_AC=auto\n#RADEON_DPM_PERF_LEVEL_ON_BAT=auto\n\n# WiFi power saving mode: on=enable, off=disable; not supported by all adapters.\nWIFI_PWR_ON_AC=off\nWIFI_PWR_ON_BAT=on\n\n# Disable wake on LAN: Y/N                 #禁用WOL\nWOL_DISABLE=N\n\n# Enable audio power saving for Intel HDA, AC97 devices (timeout in secs).     #与音频有关\n# A value of 0 disables, >=1 enables power save.\nSOUND_POWER_SAVE_ON_AC=0\nSOUND_POWER_SAVE_ON_BAT=1\n\n# Disable controller too (HDA only): Y/N            #与音频有关\nSOUND_POWER_SAVE_CONTROLLER=Y\n\n# Set to 1 to power off optical drive in UltraBay/MediaBay when running on\n# battery. A value of 0 disables this feature (Default).\n# Drive can be powered on again by releasing (and reinserting) the eject lever\n# or by pressing the disc eject button on newer models.\n# Note: an UltraBay/MediaBay hard disk is never powered off.\nBAY_POWEROFF_ON_BAT=0\n# Optical drive device to power off (default sr0).\nBAY_DEVICE=\"sr0\"\n\n# Runtime Power Management for PCI(e) bus devices: on=disable, auto=enable\nRUNTIME_PM_ON_AC=on\nRUNTIME_PM_ON_BAT=auto\n\n# Runtime PM for *all* PCI(e) bus devices, except blacklisted ones:\n#   0=disable, 1=enable\nRUNTIME_PM_ALL=1\n\n# Exclude PCI(e) device adresses the following list from Runtime PM\n# (separate with spaces). Use lspci to get the adresses (1st column).\n#RUNTIME_PM_BLACKLIST=\"bb:dd.f 11:22.3 44:55.6\"\n\n# Exclude PCI(e) devices assigned to the listed drivers from Runtime PM\n# (should prevent accidential power on of hybrid graphics' discrete part).\n# Default is \"radeon nouveau\"; use \"\" to disable the feature completely.\n# Separate multiple drivers with spaces.\nRUNTIME_PM_DRIVER_BLACKLIST=\"radeon nouveau\"\n\n# Set to 0 to disable, 1 to enable USB autosuspend feature.\nUSB_AUTOSUSPEND=1\n\n# Exclude listed devices from USB autosuspend (separate with spaces).\n# Use lsusb to get the ids.\n# Note: input devices (usbhid) are excluded automatically (see below)\n#USB_BLACKLIST=\"1111:2222 3333:4444\"\n\n# WWAN devices are excluded from USB autosuspend: 0=do not exclude / 1=exclude\nUSB_BLACKLIST_WWAN=1\n\n# Include listed devices into USB autosuspend even if already excluded\n# by the driver or WWAN blacklists above (separate with spaces).\n# Use lsusb to get the ids.\n#USB_WHITELIST=\"1111:2222 3333:4444\"\n\n# Set to 1 to disable autosuspend before shutdown, 0 to do nothing\n# (workaround for USB devices that cause shutdown problems).\n#USB_AUTOSUSPEND_DISABLE_ON_SHUTDOWN=1\n\n# Restore radio device state (Bluetooth, WiFi, WWAN) from previous shutdown\n# on system startup: 0=disable, 1=enable.\n# Hint: the parameters DEVICES_TO_DISABLE/ENABLE_ON_STARTUP/SHUTDOWN below\n#   are ignored when this is enabled!\nRESTORE_DEVICE_STATE_ON_STARTUP=0\n\n# Radio devices to disable on startup: bluetooth, wifi, wwan.\n# Separate multiple devices with spaces.\n#DEVICES_TO_DISABLE_ON_STARTUP=\"bluetooth wifi wwan\"\n\n# Radio devices to enable on startup: bluetooth, wifi, wwan.\n# Separate multiple devices with spaces.\n#DEVICES_TO_ENABLE_ON_STARTUP=\"wifi\"\n\n# Radio devices to disable on shutdown: bluetooth, wifi, wwan\n# (workaround for devices that are blocking shutdown).\n#DEVICES_TO_DISABLE_ON_SHUTDOWN=\"bluetooth wifi wwan\"\n\n# Radio devices to enable on shutdown: bluetooth, wifi, wwan\n# (to prevent other operating systems from missing radios).\n#DEVICES_TO_ENABLE_ON_SHUTDOWN=\"wwan\"\n\n# Radio devices to enable on AC: bluetooth, wifi, wwan\n#DEVICES_TO_ENABLE_ON_AC=\"bluetooth wifi wwan\"\n\n# Radio devices to disable on battery: bluetooth, wifi, wwan\n#DEVICES_TO_DISABLE_ON_BAT=\"bluetooth wifi wwan\"\n\n# Radio devices to disable on battery when not in use (not connected):\n# bluetooth, wifi, wwan\n#DEVICES_TO_DISABLE_ON_BAT_NOT_IN_USE=\"bluetooth wifi wwan\"\n\n# Battery charge thresholds (ThinkPad only, tp-smapi or acpi-call kernel module             #ThinkPad笔记本使用，其它品牌谨慎开启\n# required). Charging starts when the remaining capacity falls below the\n# START_CHARGE_THRESH value and stops when exceeding the STOP_CHARGE_THRESH value.\n# Main / Internal battery (values in %)\n#START_CHARGE_THRESH_BAT0=75\n#STOP_CHARGE_THRESH_BAT0=80\n# Ultrabay / Slice / Replaceable battery (values in %)\n#START_CHARGE_THRESH_BAT1=75\n#STOP_CHARGE_THRESH_BAT1=80\n\n# ------------------------------------------------------------------------------\n# tlp-rdw - Parameters for the radio device wizard\n# Possible devices: bluetooth, wifi, wwan\n\n# Hints:\n# - Parameters are disabled by default, remove the leading # to enable them.\n# - Separate multiple radio devices with spaces.\n\n# Radio devices to disable on connect.\n#DEVICES_TO_DISABLE_ON_LAN_CONNECT=\"wifi wwan\"\n#DEVICES_TO_DISABLE_ON_WIFI_CONNECT=\"wwan\"\n#DEVICES_TO_DISABLE_ON_WWAN_CONNECT=\"wifi\"\n\n# Radio devices to enable on disconnect.\n#DEVICES_TO_ENABLE_ON_LAN_DISCONNECT=\"wifi wwan\"\n#DEVICES_TO_ENABLE_ON_WIFI_DISCONNECT=\"\"\n#DEVICES_TO_ENABLE_ON_WWAN_DISCONNECT=\"\"\n\n# Radio devices to enable/disable when docked.\n#DEVICES_TO_ENABLE_ON_DOCK=\"\"\n#DEVICES_TO_DISABLE_ON_DOCK=\"\"\n\n# Radio devices to enable/disable when undocked.\n#DEVICES_TO_ENABLE_ON_UNDOCK=\"wifi\"\n#DEVICES_TO_DISABLE_ON_UNDOCK=\"\"\n```\n我主要是在cpu那个部分做了一些调整，一个i7的电脑，硬生生被调成了i5的性能，我真的是后悔当时没直接买i5。更详细的信息可以参考[官方文档](https://linrunner.de/en/tlp/tlp.html).\n## 显卡配置\n因为跑深度学习的代码需要用很多N卡配置，而默认不使用tlp的话电脑会很热，用了tlp系统会自动的把显卡切换到intel的集成显卡，这个问题一开始苦恼了我好久。其实ubuntu里面可以手动切换显卡的。前提是需要正确安装电脑对应的显卡，我们需要在N卡的驱动附加软件里面实现这个显卡切换功能。ps.Linux上显卡驱动真的是个大坑~\n![upload successful](/images/pasted-14.png)可以看到我现在用的是intel的集成显卡，而实际上我是有N卡的。\n### 正确安装对应驱动\n如果我们刚安装完ubuntu新系统的话，我们正常在上图看到的显卡类型应该是**nouveau**的驱动，nouveau是一个自由及开放源代码显卡驱动程序，是为Nvidia的显示卡所编写，也可用于属于系统芯片的NVIDIA Tegra系列，该项目的目标为利用逆向工程Nvidia的专有Linux驱动程序来创造一个开放源代码的驱动程序，所以nouveau开源驱动基本上是不能正常使用的。\n首先查看电脑目前有什么显卡，使用如下命令：\n```\nlspci | grep VGA     # 查看集成显卡\nlspci | grep NVIDIA  # 查看NVIDIA显卡\n```\n我的电脑是小米pro，可以看出显卡是GeForce MX150的。\n![upload successful](/images/pasted-16.png)\n\n下面查看应该安装什么版本的驱动，如下命令：\n\n```\nubuntu-drivers devices   # 查询所有ubuntu推荐的驱动\n```\n\n可以看出ubutnu提示我应该安装390版本的驱动。\n![upload successful](/images/pasted-17.png)\n\n然后就可以使用下面条命令安装所有推荐的驱动程序：\n\n```\nsudo ubuntu-drivers autoinstall\n```\n\n或者\n```\nsudo apt-get install nvidia-390\n```\n\n\n\n安装完成后重启，在**软件和更新**中我们可以看到已经安装了哪些驱动：\n![upload successful](/images/pasted-18.png)\n可以看到我已经安装了390和nouveau这两个驱动，勾选390那个就行。\n### 切换显卡\n随后我们就可以使用使用nvidia-settings命令了：\n```\nnvidia-settings\n```\n我们可以在这里选择我们要使用哪个显卡，切换之后reboot就好了。\n![upload successful](/images/pasted-19.png)\n\n还有最后一点注意的！！！！！！！！！！！！！\n\n**如果安装cuda的话，一定不再选择安装驱动，要不然就前功尽弃啦！！！**\n# 美化系统主题\n\n作为一个颜控党，主题这种东西如果不和为空是万万不可以的。\n从ubuntu17.10开始，官方又开始使用gnome作为默认的桌面环境，意味着我们可以轻松的使用GNOME Shell扩展了，美滋滋。先放几张图：\n![upload successful](/images/pasted-20.png)\n\n![upload successful](/images/pasted-21.png)\n\n![upload successful](/images/pasted-22.png)搞了个北欧风，十分舒服～～\n\n首先我们要安装GNOME Tweak Tool。\n\n```\nsudo apt install gnome-tweak-tool\n```\n安装之后你就能找到这个东西：\n\n![upload successful](/images/pasted-23.png)\n打开：\n![upload successful](/images/pasted-24.png)\n\n在这里可以配置不同的外观，也可以安装不同的扩展小插件，在这里我已经安装好配置主题需要的扩展了，正常来说我们需要先安装User themes这个扩展。\n![upload successful](/images/pasted-25.png)\n我们可通过chrome来方便的安装任何扩展，首先我们需要在chrome里面安装GNOME的小程序：!\n![upload successful](/images/pasted-26.png)\n\n随后浏览器打开https://extensions.gnome.org/ 安装这个扩展：\n![upload successful](/images/pasted-27.png)\n点进去，把开关调成on状态就好了：\n![upload successful](/images/pasted-28.png)\n随后我们就能使用主题包来装饰我们的系统了。\n\n对于Gnome桌面，你最需要连接的就是这个[网站](https://www.gnome-look.org/)，它提供了包括主题、图标、字体等在内的很多包。 \n因为在上述中安装了User Themes 扩展，所以我们可以把下载好的主题放置在自己的家目录下，为此，在家目录下的.local/share中新建themes、fonts、icons 三个文件夹，分别存放主题、字体和图标 。下载我们喜欢的主题和图标之后（一般是一个压缩包），我们需要把压缩包放到对应的文件夹里，并且进行解压缩，这里提供几个常用的命令：\n```\n//解压\n$ xz -d node-v6.10.1-linux-x64.tar.xz\n$ tar -xvf node-v6.10.1-linux-x64.tar\n\n//移动文件\ncp -r 文件 ~/.local/share/icons\n\n//本机gnome主题、字体、图标地址(https://www.gnome-look.org/)\n~/.local/share/themes\n~/.local/share/fonts\n~/.local/share/icons\n```\n上面的步骤完成后，我们就可以在这里进行修改了：\n![upload successful](/images/pasted-29.png)\n# 美化grub启动项\n个人是不太喜欢紫色的，尤其开机grub引导项的那个基佬紫，这的受不了啊，要想个办法改一下。\n这个部分我主要参考的是这个[博主](https://blog.csdn.net/w84963568/article/details/78884003)的文章，所以不过多的介绍了。\n# 基础命令操作\n\n未完待续。。。","slug":"Ubuntu18-04深度优化美化记录","published":1,"updated":"2019-03-14T08:55:38.091Z","_id":"cjt8bm3u900069goj2hfoens1","comments":1,"layout":"post","photos":[],"link":"","content":"<p>喜欢Ubuntu的一个好处就是相比win来说Linux可以更方便的对系统进行一些修改，而且安装很多东西很方便，相比mac可以充分利用硬件（mac没有N卡，配置深度学习的时候不能用显卡跑代码）。作为一个颜控，最重要的当然是Ubuntu界面很舒服很漂亮～～～～</p>\n<p>自从接触了Ubuntu之后就开始了折腾之旅，装双系统、分区、熟悉操作命令、配置环境、美化界面等等，对于一个小白来说，过程还是很艰辛的。很早之前就想把折腾的过程记录下来了，但是太懒啦，哈哈哈。这两天室友刚开始用Ubuntu，激励我记录一下，文章大概可以分成：<strong>显卡配置和电源管理</strong>、<strong>美化系统主题</strong>、<strong>美化grub启动项</strong>、<strong>基础命令操作</strong>，这几个大部分。</p>\n<hr>\n<h1 id=\"显卡配置和电源管理\"><a href=\"#显卡配置和电源管理\" class=\"headerlink\" title=\"显卡配置和电源管理\"></a>显卡配置和电源管理</h1><p>我的电脑是小米，刚装Ubuntu的时候真的是为发烧而生，风扇声音比我散热器的声音都大，非常hot并且导致续航时间严重缩短（当初我换新电脑的时候可主要因为原来电脑续航不够哇），上网查原因主要是显卡驱动问题和Ubuntn没有win那么好的电源管理导致的。所以主要从这两个方面入手。</p>\n<h2 id=\"TLP电源管理软件\"><a href=\"#TLP电源管理软件\" class=\"headerlink\" title=\"TLP电源管理软件\"></a>TLP电源管理软件</h2><p>首先TLP是免费的，可以减少电脑发热量和增加笔记本电池使用时间的电源管理工具。它是轻量级的工具，没有GUI，不用进行大量配置，一般的电脑使用默认配置就可以了。但是默认的配置会导致系统把显卡驱动切换到集成显卡上，所以一会我们还要管理一下显卡的驱动，首先介绍tlp的安装，随后介绍tlp的配置文件各代表了什么含义。</p>\n<h3 id=\"tlp的安装\"><a href=\"#tlp的安装\" class=\"headerlink\" title=\"tlp的安装\"></a>tlp的安装</h3><figure class=\"highlight smali\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">添加PPA：</span><br><span class=\"line\">sudo<span class=\"built_in\"> add-apt-repository </span>ppa:linrunner/tlp</span><br><span class=\"line\">更新软件列表：</span><br><span class=\"line\">sudo apt-get update</span><br><span class=\"line\">安装TLP：</span><br><span class=\"line\">sudo apt install tlp</span><br><span class=\"line\">启动TLP：</span><br><span class=\"line\">sudo tlp start</span><br><span class=\"line\">&gt;&gt;&gt; TLP started in AC mode.</span><br></pre></td></tr></table></figure>\n<h3 id=\"tlp的配置文件\"><a href=\"#tlp的配置文件\" class=\"headerlink\" title=\"tlp的配置文件\"></a>tlp的配置文件</h3><p>现在 TLP 已经被启动起来了，而且已经设置好了节省电池所需要的默认配置。我们可以查看该配置文件。文件路径为 /etc/default/tlp。我们需要编辑该文件来修改各项配置。配置的一些示例如下：<br><figure class=\"highlight vala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br><span class=\"line\">146</span><br><span class=\"line\">147</span><br><span class=\"line\">148</span><br><span class=\"line\">149</span><br><span class=\"line\">150</span><br><span class=\"line\">151</span><br><span class=\"line\">152</span><br><span class=\"line\">153</span><br><span class=\"line\">154</span><br><span class=\"line\">155</span><br><span class=\"line\">156</span><br><span class=\"line\">157</span><br><span class=\"line\">158</span><br><span class=\"line\">159</span><br><span class=\"line\">160</span><br><span class=\"line\">161</span><br><span class=\"line\">162</span><br><span class=\"line\">163</span><br><span class=\"line\">164</span><br><span class=\"line\">165</span><br><span class=\"line\">166</span><br><span class=\"line\">167</span><br><span class=\"line\">168</span><br><span class=\"line\">169</span><br><span class=\"line\">170</span><br><span class=\"line\">171</span><br><span class=\"line\">172</span><br><span class=\"line\">173</span><br><span class=\"line\">174</span><br><span class=\"line\">175</span><br><span class=\"line\">176</span><br><span class=\"line\">177</span><br><span class=\"line\">178</span><br><span class=\"line\">179</span><br><span class=\"line\">180</span><br><span class=\"line\">181</span><br><span class=\"line\">182</span><br><span class=\"line\">183</span><br><span class=\"line\">184</span><br><span class=\"line\">185</span><br><span class=\"line\">186</span><br><span class=\"line\">187</span><br><span class=\"line\">188</span><br><span class=\"line\">189</span><br><span class=\"line\">190</span><br><span class=\"line\">191</span><br><span class=\"line\">192</span><br><span class=\"line\">193</span><br><span class=\"line\">194</span><br><span class=\"line\">195</span><br><span class=\"line\">196</span><br><span class=\"line\">197</span><br><span class=\"line\">198</span><br><span class=\"line\">199</span><br><span class=\"line\">200</span><br><span class=\"line\">201</span><br><span class=\"line\">202</span><br><span class=\"line\">203</span><br><span class=\"line\">204</span><br><span class=\"line\">205</span><br><span class=\"line\">206</span><br><span class=\"line\">207</span><br><span class=\"line\">208</span><br><span class=\"line\">209</span><br><span class=\"line\">210</span><br><span class=\"line\">211</span><br><span class=\"line\">212</span><br><span class=\"line\">213</span><br><span class=\"line\">214</span><br><span class=\"line\">215</span><br><span class=\"line\">216</span><br><span class=\"line\">217</span><br><span class=\"line\">218</span><br><span class=\"line\">219</span><br><span class=\"line\">220</span><br><span class=\"line\">221</span><br><span class=\"line\">222</span><br><span class=\"line\">223</span><br><span class=\"line\">224</span><br><span class=\"line\">225</span><br><span class=\"line\">226</span><br><span class=\"line\">227</span><br><span class=\"line\">228</span><br><span class=\"line\">229</span><br><span class=\"line\">230</span><br><span class=\"line\">231</span><br><span class=\"line\">232</span><br><span class=\"line\">233</span><br><span class=\"line\">234</span><br><span class=\"line\">235</span><br><span class=\"line\">236</span><br><span class=\"line\">237</span><br><span class=\"line\">238</span><br><span class=\"line\">239</span><br><span class=\"line\">240</span><br><span class=\"line\">241</span><br><span class=\"line\">242</span><br><span class=\"line\">243</span><br><span class=\"line\">244</span><br><span class=\"line\">245</span><br><span class=\"line\">246</span><br><span class=\"line\">247</span><br><span class=\"line\">248</span><br><span class=\"line\">249</span><br><span class=\"line\">250</span><br><span class=\"line\">251</span><br><span class=\"line\">252</span><br><span class=\"line\">253</span><br><span class=\"line\">254</span><br><span class=\"line\">255</span><br><span class=\"line\">256</span><br><span class=\"line\">257</span><br><span class=\"line\">258</span><br><span class=\"line\">259</span><br><span class=\"line\">260</span><br><span class=\"line\">261</span><br><span class=\"line\">262</span><br><span class=\"line\">263</span><br><span class=\"line\">264</span><br><span class=\"line\">265</span><br><span class=\"line\">266</span><br><span class=\"line\">267</span><br><span class=\"line\">268</span><br><span class=\"line\">269</span><br><span class=\"line\">270</span><br><span class=\"line\">271</span><br><span class=\"line\">272</span><br><span class=\"line\">273</span><br><span class=\"line\">274</span><br><span class=\"line\">275</span><br><span class=\"line\">276</span><br><span class=\"line\">277</span><br><span class=\"line\">278</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\"># ------------------------------------------------------------------------------</span></span><br><span class=\"line\"><span class=\"meta\"># tlp - Parameters for power save</span></span><br><span class=\"line\"><span class=\"meta\"># See full explanation: http://linrunner.de/en/tlp/docs/tlp-configuration.html</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Hint: some features are disabled by default, remove the leading # to enable      #通过去掉“#”来开启参数</span></span><br><span class=\"line\"><span class=\"meta\"># them.</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Set to 0 to disable, 1 to enable TLP.    #设置为‘“1”启用TLP服务</span></span><br><span class=\"line\">TLP_ENABLE=<span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Operation mode when no power supply can be detected: AC, BAT</span></span><br><span class=\"line\"><span class=\"meta\"># Concerns some desktop and embedded hardware only.</span></span><br><span class=\"line\">TLP_DEFAULT_MODE=AC</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Seconds laptop mode has to wait after the disk goes idle before doing a sync.</span></span><br><span class=\"line\"><span class=\"meta\"># Non-zero value enables, zero disables laptop mode.</span></span><br><span class=\"line\">DISK_IDLE_SECS_ON_AC=<span class=\"number\">0</span></span><br><span class=\"line\">DISK_IDLE_SECS_ON_BAT=<span class=\"number\">2</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Dirty page values (timeouts in secs).</span></span><br><span class=\"line\">MAX_LOST_WORK_SECS_ON_AC=<span class=\"number\">30</span></span><br><span class=\"line\">MAX_LOST_WORK_SECS_ON_BAT=<span class=\"number\">90</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Hint: CPU parameters below are disabled by default, remove the leading #</span></span><br><span class=\"line\"><span class=\"meta\"># to enable them, otherwise kernel default values are used.</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Select a CPU frequency scaling governor.                      #CPU调度策略</span></span><br><span class=\"line\"><span class=\"meta\"># Intel Core i processor with intel_pstate driver:</span></span><br><span class=\"line\"><span class=\"meta\">#   powersave(*), performance</span></span><br><span class=\"line\"><span class=\"meta\"># Older hardware with acpi-cpufreq driver:</span></span><br><span class=\"line\"><span class=\"meta\">#   ondemand(*), powersave, performance, conservative</span></span><br><span class=\"line\"><span class=\"meta\"># (*) is recommended.</span></span><br><span class=\"line\"><span class=\"meta\"># Hint: use tlp-stat -p to show the active driver and available governors.</span></span><br><span class=\"line\"><span class=\"meta\"># Important:</span></span><br><span class=\"line\"><span class=\"meta\">#   You *must* disable your distribution's governor settings or conflicts will</span></span><br><span class=\"line\"><span class=\"meta\">#   occur. ondemand is sufficient for *almost all* workloads, you should know</span></span><br><span class=\"line\"><span class=\"meta\">#   what you're doing!</span></span><br><span class=\"line\">CPU_SCALING_GOVERNOR_ON_AC=powersave</span><br><span class=\"line\">CPU_SCALING_GOVERNOR_ON_BAT=powersave</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Set the min/max frequency available for the scaling governor.</span></span><br><span class=\"line\"><span class=\"meta\"># Possible values strongly depend on your CPU. For available frequencies see</span></span><br><span class=\"line\"><span class=\"meta\"># the output of tlp-stat -p.</span></span><br><span class=\"line\"><span class=\"meta\">#CPU_SCALING_MIN_FREQ_ON_AC=0</span></span><br><span class=\"line\"><span class=\"meta\">#CPU_SCALING_MAX_FREQ_ON_AC=0</span></span><br><span class=\"line\"><span class=\"meta\">#CPU_SCALING_MIN_FREQ_ON_BAT=0</span></span><br><span class=\"line\"><span class=\"meta\">#CPU_SCALING_MAX_FREQ_ON_BAT=0</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Set Intel P-state performance: 0..100 (%)</span></span><br><span class=\"line\"><span class=\"meta\"># Limit the max/min P-state to control the power dissipation of the CPU.</span></span><br><span class=\"line\"><span class=\"meta\"># Values are stated as a percentage of the available performance.</span></span><br><span class=\"line\"><span class=\"meta\"># Requires an Intel Core i processor with intel_pstate driver.</span></span><br><span class=\"line\">CPU_MIN_PERF_ON_AC=<span class=\"number\">0</span></span><br><span class=\"line\">CPU_MAX_PERF_ON_AC=<span class=\"number\">100</span></span><br><span class=\"line\">CPU_MIN_PERF_ON_BAT=<span class=\"number\">0</span></span><br><span class=\"line\">CPU_MAX_PERF_ON_BAT=<span class=\"number\">30</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Set the CPU \"turbo boost\" feature: 0=disable, 1=allow       #开启intel cpu 睿频</span></span><br><span class=\"line\"><span class=\"meta\"># Requires an Intel Core i processor.</span></span><br><span class=\"line\"><span class=\"meta\"># Important:</span></span><br><span class=\"line\"><span class=\"meta\"># - This may conflict with your distribution's governor settings</span></span><br><span class=\"line\"><span class=\"meta\"># - A value of 1 does *not* activate boosting, it just allows it</span></span><br><span class=\"line\">CPU_BOOST_ON_AC=<span class=\"number\">1</span></span><br><span class=\"line\">CPU_BOOST_ON_BAT=<span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Minimize number of used CPU cores/hyper-threads under light load conditions        #与cpu有关</span></span><br><span class=\"line\">SCHED_POWERSAVE_ON_AC=<span class=\"number\">0</span></span><br><span class=\"line\">SCHED_POWERSAVE_ON_BAT=<span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Kernel NMI Watchdog:</span></span><br><span class=\"line\"><span class=\"meta\">#   0=disable (default, saves power), 1=enable (for kernel debugging only)</span></span><br><span class=\"line\">NMI_WATCHDOG=<span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Change CPU voltages aka \"undervolting\" - Kernel with PHC patch required     #调节CPU 电压以达到节能的目的 ，谨慎开启！！！</span></span><br><span class=\"line\"><span class=\"meta\"># Frequency voltage pairs are written to:</span></span><br><span class=\"line\"><span class=\"meta\">#   /sys/devices/system/cpu/cpu0/cpufreq/phc_controls</span></span><br><span class=\"line\"><span class=\"meta\"># CAUTION: only use this, if you thoroughly understand what you are doing!</span></span><br><span class=\"line\"><span class=\"meta\">#PHC_CONTROLS=\"F:V F:V F:V F:V\"</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Set CPU performance versus energy savings policy:              #与CPU有关</span></span><br><span class=\"line\"><span class=\"meta\">#   performance, normal, powersave</span></span><br><span class=\"line\"><span class=\"meta\"># Requires kernel module msr and x86_energy_perf_policy from linux-tools</span></span><br><span class=\"line\">ENERGY_PERF_POLICY_ON_AC=normal</span><br><span class=\"line\">ENERGY_PERF_POLICY_ON_BAT=powersave</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Hard disk devices; separate multiple devices with spaces (default: sda).</span></span><br><span class=\"line\"><span class=\"meta\"># Devices can be specified by disk ID also (lookup with: tlp diskid).</span></span><br><span class=\"line\">DISK_DEVICES=<span class=\"string\">\"sda sdb\"</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Hard disk advanced power management level: 1..254, 255 (max saving, min, off)</span></span><br><span class=\"line\"><span class=\"meta\"># Levels 1..127 may spin down the disk; 255 allowable on most drives.</span></span><br><span class=\"line\"><span class=\"meta\"># Separate values for multiple disks with spaces. Use the special value 'keep'</span></span><br><span class=\"line\"><span class=\"meta\"># to keep the hardware default for the particular disk.</span></span><br><span class=\"line\">DISK_APM_LEVEL_ON_AC=<span class=\"string\">\"254 254\"</span></span><br><span class=\"line\">DISK_APM_LEVEL_ON_BAT=<span class=\"string\">\"128 128\"</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Hard disk spin down timeout:</span></span><br><span class=\"line\"><span class=\"meta\">#   0:        spin down disabled</span></span><br><span class=\"line\"><span class=\"meta\">#   1..240:   timeouts from 5s to 20min (in units of 5s)</span></span><br><span class=\"line\"><span class=\"meta\">#   241..251: timeouts from 30min to 5.5 hours (in units of 30min)</span></span><br><span class=\"line\"><span class=\"meta\"># See 'man hdparm' for details.</span></span><br><span class=\"line\"><span class=\"meta\"># Separate values for multiple disks with spaces. Use the special value 'keep'</span></span><br><span class=\"line\"><span class=\"meta\"># to keep the hardware default for the particular disk.</span></span><br><span class=\"line\"><span class=\"meta\">#DISK_SPINDOWN_TIMEOUT_ON_AC=\"0 0\"</span></span><br><span class=\"line\"><span class=\"meta\">#DISK_SPINDOWN_TIMEOUT_ON_BAT=\"0 0\"</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Select IO scheduler for the disk devices: cfq, deadline, noop (Default: cfq);     #选择磁盘驱动器I/O调度方式，建议deadline</span></span><br><span class=\"line\"><span class=\"meta\"># Separate values for multiple disks with spaces. Use the special value 'keep'</span></span><br><span class=\"line\"><span class=\"meta\"># to keep the kernel default scheduler for the particular disk.</span></span><br><span class=\"line\">DISK_IOSCHED=<span class=\"string\">\"deadline cfq\"</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># SATA aggressive link power management (ALPM):</span></span><br><span class=\"line\"><span class=\"meta\">#   min_power, medium_power, max_performance</span></span><br><span class=\"line\">SATA_LINKPWR_ON_AC=max_performance</span><br><span class=\"line\">SATA_LINKPWR_ON_BAT=min_power</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Exclude SATA host devices from link power management.</span></span><br><span class=\"line\"><span class=\"meta\"># Separate multiple hosts with spaces.</span></span><br><span class=\"line\"><span class=\"meta\">#SATA_LINKPWR_BLACKLIST=\"host1\"</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Runtime Power Management for AHCI controllers and disks:      #请谨慎开启，有可能导致磁盘被锁或者数据丢失</span></span><br><span class=\"line\"><span class=\"meta\">#   on=disable, auto=enable</span></span><br><span class=\"line\"><span class=\"meta\"># EXPERIMENTAL ** WARNING: auto will most likely cause system lockups/data loss</span></span><br><span class=\"line\"><span class=\"meta\">#AHCI_RUNTIME_PM_ON_AC=on</span></span><br><span class=\"line\"><span class=\"meta\">#AHCI_RUNTIME_PM_ON_BAT=on</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Seconds of inactivity before disk is suspended</span></span><br><span class=\"line\">AHCI_RUNTIME_PM_TIMEOUT=<span class=\"number\">15</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># PCI Express Active State Power Management (PCIe ASPM):</span></span><br><span class=\"line\"><span class=\"meta\">#   default, performance, powersave</span></span><br><span class=\"line\">PCIE_ASPM_ON_AC=performance</span><br><span class=\"line\">PCIE_ASPM_ON_BAT=powersave</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Radeon graphics clock speed (profile method): low, mid, high, auto, default;      #NVIDIA显卡用户请无视或者禁用</span></span><br><span class=\"line\"><span class=\"meta\"># auto = mid on BAT, high on AC; default = use hardware defaults.</span></span><br><span class=\"line\"><span class=\"meta\"># (Kernel &gt;= 2.6.35 only, open-source radeon driver explicitly)</span></span><br><span class=\"line\"><span class=\"meta\">#RADEON_POWER_PROFILE_ON_AC=high</span></span><br><span class=\"line\"><span class=\"meta\">#RADEON_POWER_PROFILE_ON_BAT=low</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Radeon dynamic power management method (DPM): battery, performance        #NVIDIA显卡用户请无视或者禁用</span></span><br><span class=\"line\"><span class=\"meta\"># (Kernel &gt;= 3.11 only, requires boot option radeon.dpm=1)</span></span><br><span class=\"line\"><span class=\"meta\">#RADEON_DPM_STATE_ON_AC=performance</span></span><br><span class=\"line\"><span class=\"meta\">#RADEON_DPM_STATE_ON_BAT=battery</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Radeon DPM performance level: auto, low, high; auto is recommended .           #NVIDIA显卡用户请无视或者禁用</span></span><br><span class=\"line\"><span class=\"meta\">#RADEON_DPM_PERF_LEVEL_ON_AC=auto</span></span><br><span class=\"line\"><span class=\"meta\">#RADEON_DPM_PERF_LEVEL_ON_BAT=auto</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># WiFi power saving mode: on=enable, off=disable; not supported by all adapters.</span></span><br><span class=\"line\">WIFI_PWR_ON_AC=off</span><br><span class=\"line\">WIFI_PWR_ON_BAT=on</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Disable wake on LAN: Y/N                 #禁用WOL</span></span><br><span class=\"line\">WOL_DISABLE=N</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Enable audio power saving for Intel HDA, AC97 devices (timeout in secs).     #与音频有关</span></span><br><span class=\"line\"><span class=\"meta\"># A value of 0 disables, &gt;=1 enables power save.</span></span><br><span class=\"line\">SOUND_POWER_SAVE_ON_AC=<span class=\"number\">0</span></span><br><span class=\"line\">SOUND_POWER_SAVE_ON_BAT=<span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Disable controller too (HDA only): Y/N            #与音频有关</span></span><br><span class=\"line\">SOUND_POWER_SAVE_CONTROLLER=Y</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Set to 1 to power off optical drive in UltraBay/MediaBay when running on</span></span><br><span class=\"line\"><span class=\"meta\"># battery. A value of 0 disables this feature (Default).</span></span><br><span class=\"line\"><span class=\"meta\"># Drive can be powered on again by releasing (and reinserting) the eject lever</span></span><br><span class=\"line\"><span class=\"meta\"># or by pressing the disc eject button on newer models.</span></span><br><span class=\"line\"><span class=\"meta\"># Note: an UltraBay/MediaBay hard disk is never powered off.</span></span><br><span class=\"line\">BAY_POWEROFF_ON_BAT=<span class=\"number\">0</span></span><br><span class=\"line\"><span class=\"meta\"># Optical drive device to power off (default sr0).</span></span><br><span class=\"line\">BAY_DEVICE=<span class=\"string\">\"sr0\"</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Runtime Power Management for PCI(e) bus devices: on=disable, auto=enable</span></span><br><span class=\"line\">RUNTIME_PM_ON_AC=on</span><br><span class=\"line\">RUNTIME_PM_ON_BAT=auto</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Runtime PM for *all* PCI(e) bus devices, except blacklisted ones:</span></span><br><span class=\"line\"><span class=\"meta\">#   0=disable, 1=enable</span></span><br><span class=\"line\">RUNTIME_PM_ALL=<span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Exclude PCI(e) device adresses the following list from Runtime PM</span></span><br><span class=\"line\"><span class=\"meta\"># (separate with spaces). Use lspci to get the adresses (1st column).</span></span><br><span class=\"line\"><span class=\"meta\">#RUNTIME_PM_BLACKLIST=\"bb:dd.f 11:22.3 44:55.6\"</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Exclude PCI(e) devices assigned to the listed drivers from Runtime PM</span></span><br><span class=\"line\"><span class=\"meta\"># (should prevent accidential power on of hybrid graphics' discrete part).</span></span><br><span class=\"line\"><span class=\"meta\"># Default is \"radeon nouveau\"; use \"\" to disable the feature completely.</span></span><br><span class=\"line\"><span class=\"meta\"># Separate multiple drivers with spaces.</span></span><br><span class=\"line\">RUNTIME_PM_DRIVER_BLACKLIST=<span class=\"string\">\"radeon nouveau\"</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Set to 0 to disable, 1 to enable USB autosuspend feature.</span></span><br><span class=\"line\">USB_AUTOSUSPEND=<span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Exclude listed devices from USB autosuspend (separate with spaces).</span></span><br><span class=\"line\"><span class=\"meta\"># Use lsusb to get the ids.</span></span><br><span class=\"line\"><span class=\"meta\"># Note: input devices (usbhid) are excluded automatically (see below)</span></span><br><span class=\"line\"><span class=\"meta\">#USB_BLACKLIST=\"1111:2222 3333:4444\"</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># WWAN devices are excluded from USB autosuspend: 0=do not exclude / 1=exclude</span></span><br><span class=\"line\">USB_BLACKLIST_WWAN=<span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Include listed devices into USB autosuspend even if already excluded</span></span><br><span class=\"line\"><span class=\"meta\"># by the driver or WWAN blacklists above (separate with spaces).</span></span><br><span class=\"line\"><span class=\"meta\"># Use lsusb to get the ids.</span></span><br><span class=\"line\"><span class=\"meta\">#USB_WHITELIST=\"1111:2222 3333:4444\"</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Set to 1 to disable autosuspend before shutdown, 0 to do nothing</span></span><br><span class=\"line\"><span class=\"meta\"># (workaround for USB devices that cause shutdown problems).</span></span><br><span class=\"line\"><span class=\"meta\">#USB_AUTOSUSPEND_DISABLE_ON_SHUTDOWN=1</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Restore radio device state (Bluetooth, WiFi, WWAN) from previous shutdown</span></span><br><span class=\"line\"><span class=\"meta\"># on system startup: 0=disable, 1=enable.</span></span><br><span class=\"line\"><span class=\"meta\"># Hint: the parameters DEVICES_TO_DISABLE/ENABLE_ON_STARTUP/SHUTDOWN below</span></span><br><span class=\"line\"><span class=\"meta\">#   are ignored when this is enabled!</span></span><br><span class=\"line\">RESTORE_DEVICE_STATE_ON_STARTUP=<span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Radio devices to disable on startup: bluetooth, wifi, wwan.</span></span><br><span class=\"line\"><span class=\"meta\"># Separate multiple devices with spaces.</span></span><br><span class=\"line\"><span class=\"meta\">#DEVICES_TO_DISABLE_ON_STARTUP=\"bluetooth wifi wwan\"</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Radio devices to enable on startup: bluetooth, wifi, wwan.</span></span><br><span class=\"line\"><span class=\"meta\"># Separate multiple devices with spaces.</span></span><br><span class=\"line\"><span class=\"meta\">#DEVICES_TO_ENABLE_ON_STARTUP=\"wifi\"</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Radio devices to disable on shutdown: bluetooth, wifi, wwan</span></span><br><span class=\"line\"><span class=\"meta\"># (workaround for devices that are blocking shutdown).</span></span><br><span class=\"line\"><span class=\"meta\">#DEVICES_TO_DISABLE_ON_SHUTDOWN=\"bluetooth wifi wwan\"</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Radio devices to enable on shutdown: bluetooth, wifi, wwan</span></span><br><span class=\"line\"><span class=\"meta\"># (to prevent other operating systems from missing radios).</span></span><br><span class=\"line\"><span class=\"meta\">#DEVICES_TO_ENABLE_ON_SHUTDOWN=\"wwan\"</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Radio devices to enable on AC: bluetooth, wifi, wwan</span></span><br><span class=\"line\"><span class=\"meta\">#DEVICES_TO_ENABLE_ON_AC=\"bluetooth wifi wwan\"</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Radio devices to disable on battery: bluetooth, wifi, wwan</span></span><br><span class=\"line\"><span class=\"meta\">#DEVICES_TO_DISABLE_ON_BAT=\"bluetooth wifi wwan\"</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Radio devices to disable on battery when not in use (not connected):</span></span><br><span class=\"line\"><span class=\"meta\"># bluetooth, wifi, wwan</span></span><br><span class=\"line\"><span class=\"meta\">#DEVICES_TO_DISABLE_ON_BAT_NOT_IN_USE=\"bluetooth wifi wwan\"</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Battery charge thresholds (ThinkPad only, tp-smapi or acpi-call kernel module             #ThinkPad笔记本使用，其它品牌谨慎开启</span></span><br><span class=\"line\"><span class=\"meta\"># required). Charging starts when the remaining capacity falls below the</span></span><br><span class=\"line\"><span class=\"meta\"># START_CHARGE_THRESH value and stops when exceeding the STOP_CHARGE_THRESH value.</span></span><br><span class=\"line\"><span class=\"meta\"># Main / Internal battery (values in %)</span></span><br><span class=\"line\"><span class=\"meta\">#START_CHARGE_THRESH_BAT0=75</span></span><br><span class=\"line\"><span class=\"meta\">#STOP_CHARGE_THRESH_BAT0=80</span></span><br><span class=\"line\"><span class=\"meta\"># Ultrabay / Slice / Replaceable battery (values in %)</span></span><br><span class=\"line\"><span class=\"meta\">#START_CHARGE_THRESH_BAT1=75</span></span><br><span class=\"line\"><span class=\"meta\">#STOP_CHARGE_THRESH_BAT1=80</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># ------------------------------------------------------------------------------</span></span><br><span class=\"line\"><span class=\"meta\"># tlp-rdw - Parameters for the radio device wizard</span></span><br><span class=\"line\"><span class=\"meta\"># Possible devices: bluetooth, wifi, wwan</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Hints:</span></span><br><span class=\"line\"><span class=\"meta\"># - Parameters are disabled by default, remove the leading # to enable them.</span></span><br><span class=\"line\"><span class=\"meta\"># - Separate multiple radio devices with spaces.</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Radio devices to disable on connect.</span></span><br><span class=\"line\"><span class=\"meta\">#DEVICES_TO_DISABLE_ON_LAN_CONNECT=\"wifi wwan\"</span></span><br><span class=\"line\"><span class=\"meta\">#DEVICES_TO_DISABLE_ON_WIFI_CONNECT=\"wwan\"</span></span><br><span class=\"line\"><span class=\"meta\">#DEVICES_TO_DISABLE_ON_WWAN_CONNECT=\"wifi\"</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Radio devices to enable on disconnect.</span></span><br><span class=\"line\"><span class=\"meta\">#DEVICES_TO_ENABLE_ON_LAN_DISCONNECT=\"wifi wwan\"</span></span><br><span class=\"line\"><span class=\"meta\">#DEVICES_TO_ENABLE_ON_WIFI_DISCONNECT=\"\"</span></span><br><span class=\"line\"><span class=\"meta\">#DEVICES_TO_ENABLE_ON_WWAN_DISCONNECT=\"\"</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Radio devices to enable/disable when docked.</span></span><br><span class=\"line\"><span class=\"meta\">#DEVICES_TO_ENABLE_ON_DOCK=\"\"</span></span><br><span class=\"line\"><span class=\"meta\">#DEVICES_TO_DISABLE_ON_DOCK=\"\"</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Radio devices to enable/disable when undocked.</span></span><br><span class=\"line\"><span class=\"meta\">#DEVICES_TO_ENABLE_ON_UNDOCK=\"wifi\"</span></span><br><span class=\"line\"><span class=\"meta\">#DEVICES_TO_DISABLE_ON_UNDOCK=\"\"</span></span><br></pre></td></tr></table></figure></p>\n<p>我主要是在cpu那个部分做了一些调整，一个i7的电脑，硬生生被调成了i5的性能，我真的是后悔当时没直接买i5。更详细的信息可以参考<a href=\"https://linrunner.de/en/tlp/tlp.html\" target=\"_blank\" rel=\"noopener\">官方文档</a>.</p>\n<h2 id=\"显卡配置\"><a href=\"#显卡配置\" class=\"headerlink\" title=\"显卡配置\"></a>显卡配置</h2><p>因为跑深度学习的代码需要用很多N卡配置，而默认不使用tlp的话电脑会很热，用了tlp系统会自动的把显卡切换到intel的集成显卡，这个问题一开始苦恼了我好久。其实ubuntu里面可以手动切换显卡的。前提是需要正确安装电脑对应的显卡，我们需要在N卡的驱动附加软件里面实现这个显卡切换功能。ps.Linux上显卡驱动真的是个大坑~<br><img src=\"/images/pasted-14.png\" alt=\"upload successful\">可以看到我现在用的是intel的集成显卡，而实际上我是有N卡的。</p>\n<h3 id=\"正确安装对应驱动\"><a href=\"#正确安装对应驱动\" class=\"headerlink\" title=\"正确安装对应驱动\"></a>正确安装对应驱动</h3><p>如果我们刚安装完ubuntu新系统的话，我们正常在上图看到的显卡类型应该是<strong>nouveau</strong>的驱动，nouveau是一个自由及开放源代码显卡驱动程序，是为Nvidia的显示卡所编写，也可用于属于系统芯片的NVIDIA Tegra系列，该项目的目标为利用逆向工程Nvidia的专有Linux驱动程序来创造一个开放源代码的驱动程序，所以nouveau开源驱动基本上是不能正常使用的。<br>首先查看电脑目前有什么显卡，使用如下命令：<br><figure class=\"highlight perl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">lspci | <span class=\"keyword\">grep</span> VGA     <span class=\"comment\"># 查看集成显卡</span></span><br><span class=\"line\">lspci | <span class=\"keyword\">grep</span> NVIDIA  <span class=\"comment\"># 查看NVIDIA显卡</span></span><br></pre></td></tr></table></figure></p>\n<p>我的电脑是小米pro，可以看出显卡是GeForce MX150的。<br><img src=\"/images/pasted-16.png\" alt=\"upload successful\"></p>\n<p>下面查看应该安装什么版本的驱动，如下命令：</p>\n<figure class=\"highlight 1c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ubuntu-drivers devices   <span class=\"meta\"># 查询所有ubuntu推荐的驱动</span></span><br></pre></td></tr></table></figure>\n<p>可以看出ubutnu提示我应该安装390版本的驱动。<br><img src=\"/images/pasted-17.png\" alt=\"upload successful\"></p>\n<p>然后就可以使用下面条命令安装所有推荐的驱动程序：</p>\n<figure class=\"highlight ebnf\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attribute\">sudo ubuntu-drivers autoinstall</span></span><br></pre></td></tr></table></figure>\n<p>或者<br><figure class=\"highlight routeros\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo apt-<span class=\"builtin-name\">get</span> install nvidia-390</span><br></pre></td></tr></table></figure></p>\n<p>安装完成后重启，在<strong>软件和更新</strong>中我们可以看到已经安装了哪些驱动：<br><img src=\"/images/pasted-18.png\" alt=\"upload successful\"><br>可以看到我已经安装了390和nouveau这两个驱动，勾选390那个就行。</p>\n<h3 id=\"切换显卡\"><a href=\"#切换显卡\" class=\"headerlink\" title=\"切换显卡\"></a>切换显卡</h3><p>随后我们就可以使用使用nvidia-settings命令了：<br><figure class=\"highlight ebnf\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attribute\">nvidia-settings</span></span><br></pre></td></tr></table></figure></p>\n<p>我们可以在这里选择我们要使用哪个显卡，切换之后reboot就好了。<br><img src=\"/images/pasted-19.png\" alt=\"upload successful\"></p>\n<p>还有最后一点注意的！！！！！！！！！！！！！</p>\n<p><strong>如果安装cuda的话，一定不再选择安装驱动，要不然就前功尽弃啦！！！</strong></p>\n<h1 id=\"美化系统主题\"><a href=\"#美化系统主题\" class=\"headerlink\" title=\"美化系统主题\"></a>美化系统主题</h1><p>作为一个颜控党，主题这种东西如果不和为空是万万不可以的。<br>从ubuntu17.10开始，官方又开始使用gnome作为默认的桌面环境，意味着我们可以轻松的使用GNOME Shell扩展了，美滋滋。先放几张图：<br><img src=\"/images/pasted-20.png\" alt=\"upload successful\"></p>\n<p><img src=\"/images/pasted-21.png\" alt=\"upload successful\"></p>\n<p><img src=\"/images/pasted-22.png\" alt=\"upload successful\">搞了个北欧风，十分舒服～～</p>\n<p>首先我们要安装GNOME Tweak Tool。</p>\n<figure class=\"highlight cmake\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo apt <span class=\"keyword\">install</span> gnome-tweak-tool</span><br></pre></td></tr></table></figure>\n<p>安装之后你就能找到这个东西：</p>\n<p><img src=\"/images/pasted-23.png\" alt=\"upload successful\"><br>打开：<br><img src=\"/images/pasted-24.png\" alt=\"upload successful\"></p>\n<p>在这里可以配置不同的外观，也可以安装不同的扩展小插件，在这里我已经安装好配置主题需要的扩展了，正常来说我们需要先安装User themes这个扩展。<br><img src=\"/images/pasted-25.png\" alt=\"upload successful\"><br>我们可通过chrome来方便的安装任何扩展，首先我们需要在chrome里面安装GNOME的小程序：!<br><img src=\"/images/pasted-26.png\" alt=\"upload successful\"></p>\n<p>随后浏览器打开<a href=\"https://extensions.gnome.org/\" target=\"_blank\" rel=\"noopener\">https://extensions.gnome.org/</a> 安装这个扩展：<br><img src=\"/images/pasted-27.png\" alt=\"upload successful\"><br>点进去，把开关调成on状态就好了：<br><img src=\"/images/pasted-28.png\" alt=\"upload successful\"><br>随后我们就能使用主题包来装饰我们的系统了。</p>\n<p>对于Gnome桌面，你最需要连接的就是这个<a href=\"https://www.gnome-look.org/\" target=\"_blank\" rel=\"noopener\">网站</a>，它提供了包括主题、图标、字体等在内的很多包。<br>因为在上述中安装了User Themes 扩展，所以我们可以把下载好的主题放置在自己的家目录下，为此，在家目录下的.local/share中新建themes、fonts、icons 三个文件夹，分别存放主题、字体和图标 。下载我们喜欢的主题和图标之后（一般是一个压缩包），我们需要把压缩包放到对应的文件夹里，并且进行解压缩，这里提供几个常用的命令：<br><figure class=\"highlight crmsh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">//解压</span><br><span class=\"line\">$ xz -d <span class=\"keyword\">node</span><span class=\"title\">-v6</span>.<span class=\"number\">10.1</span>-linux-x64.tar.xz</span><br><span class=\"line\">$ tar -xvf <span class=\"keyword\">node</span><span class=\"title\">-v6</span>.<span class=\"number\">10.1</span>-linux-x64.tar</span><br><span class=\"line\"></span><br><span class=\"line\">//移动文件</span><br><span class=\"line\">cp -r 文件 ~/.local/share/icons</span><br><span class=\"line\"></span><br><span class=\"line\">//本机gnome主题、字体、图标地址(https://www.gnome-look.org/)</span><br><span class=\"line\">~/.local/share/themes</span><br><span class=\"line\">~/.local/share/fonts</span><br><span class=\"line\">~/.local/share/icons</span><br></pre></td></tr></table></figure></p>\n<p>上面的步骤完成后，我们就可以在这里进行修改了：<br><img src=\"/images/pasted-29.png\" alt=\"upload successful\"></p>\n<h1 id=\"美化grub启动项\"><a href=\"#美化grub启动项\" class=\"headerlink\" title=\"美化grub启动项\"></a>美化grub启动项</h1><p>个人是不太喜欢紫色的，尤其开机grub引导项的那个基佬紫，这的受不了啊，要想个办法改一下。<br>这个部分我主要参考的是这个<a href=\"https://blog.csdn.net/w84963568/article/details/78884003\" target=\"_blank\" rel=\"noopener\">博主</a>的文章，所以不过多的介绍了。</p>\n<h1 id=\"基础命令操作\"><a href=\"#基础命令操作\" class=\"headerlink\" title=\"基础命令操作\"></a>基础命令操作</h1><p>未完待续。。。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>喜欢Ubuntu的一个好处就是相比win来说Linux可以更方便的对系统进行一些修改，而且安装很多东西很方便，相比mac可以充分利用硬件（mac没有N卡，配置深度学习的时候不能用显卡跑代码）。作为一个颜控，最重要的当然是Ubuntu界面很舒服很漂亮～～～～</p>\n<p>自从接触了Ubuntu之后就开始了折腾之旅，装双系统、分区、熟悉操作命令、配置环境、美化界面等等，对于一个小白来说，过程还是很艰辛的。很早之前就想把折腾的过程记录下来了，但是太懒啦，哈哈哈。这两天室友刚开始用Ubuntu，激励我记录一下，文章大概可以分成：<strong>显卡配置和电源管理</strong>、<strong>美化系统主题</strong>、<strong>美化grub启动项</strong>、<strong>基础命令操作</strong>，这几个大部分。</p>\n<hr>\n<h1 id=\"显卡配置和电源管理\"><a href=\"#显卡配置和电源管理\" class=\"headerlink\" title=\"显卡配置和电源管理\"></a>显卡配置和电源管理</h1><p>我的电脑是小米，刚装Ubuntu的时候真的是为发烧而生，风扇声音比我散热器的声音都大，非常hot并且导致续航时间严重缩短（当初我换新电脑的时候可主要因为原来电脑续航不够哇），上网查原因主要是显卡驱动问题和Ubuntn没有win那么好的电源管理导致的。所以主要从这两个方面入手。</p>\n<h2 id=\"TLP电源管理软件\"><a href=\"#TLP电源管理软件\" class=\"headerlink\" title=\"TLP电源管理软件\"></a>TLP电源管理软件</h2><p>首先TLP是免费的，可以减少电脑发热量和增加笔记本电池使用时间的电源管理工具。它是轻量级的工具，没有GUI，不用进行大量配置，一般的电脑使用默认配置就可以了。但是默认的配置会导致系统把显卡驱动切换到集成显卡上，所以一会我们还要管理一下显卡的驱动，首先介绍tlp的安装，随后介绍tlp的配置文件各代表了什么含义。</p>\n<h3 id=\"tlp的安装\"><a href=\"#tlp的安装\" class=\"headerlink\" title=\"tlp的安装\"></a>tlp的安装</h3><figure class=\"highlight smali\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">添加PPA：</span><br><span class=\"line\">sudo<span class=\"built_in\"> add-apt-repository </span>ppa:linrunner/tlp</span><br><span class=\"line\">更新软件列表：</span><br><span class=\"line\">sudo apt-get update</span><br><span class=\"line\">安装TLP：</span><br><span class=\"line\">sudo apt install tlp</span><br><span class=\"line\">启动TLP：</span><br><span class=\"line\">sudo tlp start</span><br><span class=\"line\">&gt;&gt;&gt; TLP started in AC mode.</span><br></pre></td></tr></table></figure>\n<h3 id=\"tlp的配置文件\"><a href=\"#tlp的配置文件\" class=\"headerlink\" title=\"tlp的配置文件\"></a>tlp的配置文件</h3><p>现在 TLP 已经被启动起来了，而且已经设置好了节省电池所需要的默认配置。我们可以查看该配置文件。文件路径为 /etc/default/tlp。我们需要编辑该文件来修改各项配置。配置的一些示例如下：<br><figure class=\"highlight vala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br><span class=\"line\">146</span><br><span class=\"line\">147</span><br><span class=\"line\">148</span><br><span class=\"line\">149</span><br><span class=\"line\">150</span><br><span class=\"line\">151</span><br><span class=\"line\">152</span><br><span class=\"line\">153</span><br><span class=\"line\">154</span><br><span class=\"line\">155</span><br><span class=\"line\">156</span><br><span class=\"line\">157</span><br><span class=\"line\">158</span><br><span class=\"line\">159</span><br><span class=\"line\">160</span><br><span class=\"line\">161</span><br><span class=\"line\">162</span><br><span class=\"line\">163</span><br><span class=\"line\">164</span><br><span class=\"line\">165</span><br><span class=\"line\">166</span><br><span class=\"line\">167</span><br><span class=\"line\">168</span><br><span class=\"line\">169</span><br><span class=\"line\">170</span><br><span class=\"line\">171</span><br><span class=\"line\">172</span><br><span class=\"line\">173</span><br><span class=\"line\">174</span><br><span class=\"line\">175</span><br><span class=\"line\">176</span><br><span class=\"line\">177</span><br><span class=\"line\">178</span><br><span class=\"line\">179</span><br><span class=\"line\">180</span><br><span class=\"line\">181</span><br><span class=\"line\">182</span><br><span class=\"line\">183</span><br><span class=\"line\">184</span><br><span class=\"line\">185</span><br><span class=\"line\">186</span><br><span class=\"line\">187</span><br><span class=\"line\">188</span><br><span class=\"line\">189</span><br><span class=\"line\">190</span><br><span class=\"line\">191</span><br><span class=\"line\">192</span><br><span class=\"line\">193</span><br><span class=\"line\">194</span><br><span class=\"line\">195</span><br><span class=\"line\">196</span><br><span class=\"line\">197</span><br><span class=\"line\">198</span><br><span class=\"line\">199</span><br><span class=\"line\">200</span><br><span class=\"line\">201</span><br><span class=\"line\">202</span><br><span class=\"line\">203</span><br><span class=\"line\">204</span><br><span class=\"line\">205</span><br><span class=\"line\">206</span><br><span class=\"line\">207</span><br><span class=\"line\">208</span><br><span class=\"line\">209</span><br><span class=\"line\">210</span><br><span class=\"line\">211</span><br><span class=\"line\">212</span><br><span class=\"line\">213</span><br><span class=\"line\">214</span><br><span class=\"line\">215</span><br><span class=\"line\">216</span><br><span class=\"line\">217</span><br><span class=\"line\">218</span><br><span class=\"line\">219</span><br><span class=\"line\">220</span><br><span class=\"line\">221</span><br><span class=\"line\">222</span><br><span class=\"line\">223</span><br><span class=\"line\">224</span><br><span class=\"line\">225</span><br><span class=\"line\">226</span><br><span class=\"line\">227</span><br><span class=\"line\">228</span><br><span class=\"line\">229</span><br><span class=\"line\">230</span><br><span class=\"line\">231</span><br><span class=\"line\">232</span><br><span class=\"line\">233</span><br><span class=\"line\">234</span><br><span class=\"line\">235</span><br><span class=\"line\">236</span><br><span class=\"line\">237</span><br><span class=\"line\">238</span><br><span class=\"line\">239</span><br><span class=\"line\">240</span><br><span class=\"line\">241</span><br><span class=\"line\">242</span><br><span class=\"line\">243</span><br><span class=\"line\">244</span><br><span class=\"line\">245</span><br><span class=\"line\">246</span><br><span class=\"line\">247</span><br><span class=\"line\">248</span><br><span class=\"line\">249</span><br><span class=\"line\">250</span><br><span class=\"line\">251</span><br><span class=\"line\">252</span><br><span class=\"line\">253</span><br><span class=\"line\">254</span><br><span class=\"line\">255</span><br><span class=\"line\">256</span><br><span class=\"line\">257</span><br><span class=\"line\">258</span><br><span class=\"line\">259</span><br><span class=\"line\">260</span><br><span class=\"line\">261</span><br><span class=\"line\">262</span><br><span class=\"line\">263</span><br><span class=\"line\">264</span><br><span class=\"line\">265</span><br><span class=\"line\">266</span><br><span class=\"line\">267</span><br><span class=\"line\">268</span><br><span class=\"line\">269</span><br><span class=\"line\">270</span><br><span class=\"line\">271</span><br><span class=\"line\">272</span><br><span class=\"line\">273</span><br><span class=\"line\">274</span><br><span class=\"line\">275</span><br><span class=\"line\">276</span><br><span class=\"line\">277</span><br><span class=\"line\">278</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\"># ------------------------------------------------------------------------------</span></span><br><span class=\"line\"><span class=\"meta\"># tlp - Parameters for power save</span></span><br><span class=\"line\"><span class=\"meta\"># See full explanation: http://linrunner.de/en/tlp/docs/tlp-configuration.html</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Hint: some features are disabled by default, remove the leading # to enable      #通过去掉“#”来开启参数</span></span><br><span class=\"line\"><span class=\"meta\"># them.</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Set to 0 to disable, 1 to enable TLP.    #设置为‘“1”启用TLP服务</span></span><br><span class=\"line\">TLP_ENABLE=<span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Operation mode when no power supply can be detected: AC, BAT</span></span><br><span class=\"line\"><span class=\"meta\"># Concerns some desktop and embedded hardware only.</span></span><br><span class=\"line\">TLP_DEFAULT_MODE=AC</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Seconds laptop mode has to wait after the disk goes idle before doing a sync.</span></span><br><span class=\"line\"><span class=\"meta\"># Non-zero value enables, zero disables laptop mode.</span></span><br><span class=\"line\">DISK_IDLE_SECS_ON_AC=<span class=\"number\">0</span></span><br><span class=\"line\">DISK_IDLE_SECS_ON_BAT=<span class=\"number\">2</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Dirty page values (timeouts in secs).</span></span><br><span class=\"line\">MAX_LOST_WORK_SECS_ON_AC=<span class=\"number\">30</span></span><br><span class=\"line\">MAX_LOST_WORK_SECS_ON_BAT=<span class=\"number\">90</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Hint: CPU parameters below are disabled by default, remove the leading #</span></span><br><span class=\"line\"><span class=\"meta\"># to enable them, otherwise kernel default values are used.</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Select a CPU frequency scaling governor.                      #CPU调度策略</span></span><br><span class=\"line\"><span class=\"meta\"># Intel Core i processor with intel_pstate driver:</span></span><br><span class=\"line\"><span class=\"meta\">#   powersave(*), performance</span></span><br><span class=\"line\"><span class=\"meta\"># Older hardware with acpi-cpufreq driver:</span></span><br><span class=\"line\"><span class=\"meta\">#   ondemand(*), powersave, performance, conservative</span></span><br><span class=\"line\"><span class=\"meta\"># (*) is recommended.</span></span><br><span class=\"line\"><span class=\"meta\"># Hint: use tlp-stat -p to show the active driver and available governors.</span></span><br><span class=\"line\"><span class=\"meta\"># Important:</span></span><br><span class=\"line\"><span class=\"meta\">#   You *must* disable your distribution's governor settings or conflicts will</span></span><br><span class=\"line\"><span class=\"meta\">#   occur. ondemand is sufficient for *almost all* workloads, you should know</span></span><br><span class=\"line\"><span class=\"meta\">#   what you're doing!</span></span><br><span class=\"line\">CPU_SCALING_GOVERNOR_ON_AC=powersave</span><br><span class=\"line\">CPU_SCALING_GOVERNOR_ON_BAT=powersave</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Set the min/max frequency available for the scaling governor.</span></span><br><span class=\"line\"><span class=\"meta\"># Possible values strongly depend on your CPU. For available frequencies see</span></span><br><span class=\"line\"><span class=\"meta\"># the output of tlp-stat -p.</span></span><br><span class=\"line\"><span class=\"meta\">#CPU_SCALING_MIN_FREQ_ON_AC=0</span></span><br><span class=\"line\"><span class=\"meta\">#CPU_SCALING_MAX_FREQ_ON_AC=0</span></span><br><span class=\"line\"><span class=\"meta\">#CPU_SCALING_MIN_FREQ_ON_BAT=0</span></span><br><span class=\"line\"><span class=\"meta\">#CPU_SCALING_MAX_FREQ_ON_BAT=0</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Set Intel P-state performance: 0..100 (%)</span></span><br><span class=\"line\"><span class=\"meta\"># Limit the max/min P-state to control the power dissipation of the CPU.</span></span><br><span class=\"line\"><span class=\"meta\"># Values are stated as a percentage of the available performance.</span></span><br><span class=\"line\"><span class=\"meta\"># Requires an Intel Core i processor with intel_pstate driver.</span></span><br><span class=\"line\">CPU_MIN_PERF_ON_AC=<span class=\"number\">0</span></span><br><span class=\"line\">CPU_MAX_PERF_ON_AC=<span class=\"number\">100</span></span><br><span class=\"line\">CPU_MIN_PERF_ON_BAT=<span class=\"number\">0</span></span><br><span class=\"line\">CPU_MAX_PERF_ON_BAT=<span class=\"number\">30</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Set the CPU \"turbo boost\" feature: 0=disable, 1=allow       #开启intel cpu 睿频</span></span><br><span class=\"line\"><span class=\"meta\"># Requires an Intel Core i processor.</span></span><br><span class=\"line\"><span class=\"meta\"># Important:</span></span><br><span class=\"line\"><span class=\"meta\"># - This may conflict with your distribution's governor settings</span></span><br><span class=\"line\"><span class=\"meta\"># - A value of 1 does *not* activate boosting, it just allows it</span></span><br><span class=\"line\">CPU_BOOST_ON_AC=<span class=\"number\">1</span></span><br><span class=\"line\">CPU_BOOST_ON_BAT=<span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Minimize number of used CPU cores/hyper-threads under light load conditions        #与cpu有关</span></span><br><span class=\"line\">SCHED_POWERSAVE_ON_AC=<span class=\"number\">0</span></span><br><span class=\"line\">SCHED_POWERSAVE_ON_BAT=<span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Kernel NMI Watchdog:</span></span><br><span class=\"line\"><span class=\"meta\">#   0=disable (default, saves power), 1=enable (for kernel debugging only)</span></span><br><span class=\"line\">NMI_WATCHDOG=<span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Change CPU voltages aka \"undervolting\" - Kernel with PHC patch required     #调节CPU 电压以达到节能的目的 ，谨慎开启！！！</span></span><br><span class=\"line\"><span class=\"meta\"># Frequency voltage pairs are written to:</span></span><br><span class=\"line\"><span class=\"meta\">#   /sys/devices/system/cpu/cpu0/cpufreq/phc_controls</span></span><br><span class=\"line\"><span class=\"meta\"># CAUTION: only use this, if you thoroughly understand what you are doing!</span></span><br><span class=\"line\"><span class=\"meta\">#PHC_CONTROLS=\"F:V F:V F:V F:V\"</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Set CPU performance versus energy savings policy:              #与CPU有关</span></span><br><span class=\"line\"><span class=\"meta\">#   performance, normal, powersave</span></span><br><span class=\"line\"><span class=\"meta\"># Requires kernel module msr and x86_energy_perf_policy from linux-tools</span></span><br><span class=\"line\">ENERGY_PERF_POLICY_ON_AC=normal</span><br><span class=\"line\">ENERGY_PERF_POLICY_ON_BAT=powersave</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Hard disk devices; separate multiple devices with spaces (default: sda).</span></span><br><span class=\"line\"><span class=\"meta\"># Devices can be specified by disk ID also (lookup with: tlp diskid).</span></span><br><span class=\"line\">DISK_DEVICES=<span class=\"string\">\"sda sdb\"</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Hard disk advanced power management level: 1..254, 255 (max saving, min, off)</span></span><br><span class=\"line\"><span class=\"meta\"># Levels 1..127 may spin down the disk; 255 allowable on most drives.</span></span><br><span class=\"line\"><span class=\"meta\"># Separate values for multiple disks with spaces. Use the special value 'keep'</span></span><br><span class=\"line\"><span class=\"meta\"># to keep the hardware default for the particular disk.</span></span><br><span class=\"line\">DISK_APM_LEVEL_ON_AC=<span class=\"string\">\"254 254\"</span></span><br><span class=\"line\">DISK_APM_LEVEL_ON_BAT=<span class=\"string\">\"128 128\"</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Hard disk spin down timeout:</span></span><br><span class=\"line\"><span class=\"meta\">#   0:        spin down disabled</span></span><br><span class=\"line\"><span class=\"meta\">#   1..240:   timeouts from 5s to 20min (in units of 5s)</span></span><br><span class=\"line\"><span class=\"meta\">#   241..251: timeouts from 30min to 5.5 hours (in units of 30min)</span></span><br><span class=\"line\"><span class=\"meta\"># See 'man hdparm' for details.</span></span><br><span class=\"line\"><span class=\"meta\"># Separate values for multiple disks with spaces. Use the special value 'keep'</span></span><br><span class=\"line\"><span class=\"meta\"># to keep the hardware default for the particular disk.</span></span><br><span class=\"line\"><span class=\"meta\">#DISK_SPINDOWN_TIMEOUT_ON_AC=\"0 0\"</span></span><br><span class=\"line\"><span class=\"meta\">#DISK_SPINDOWN_TIMEOUT_ON_BAT=\"0 0\"</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Select IO scheduler for the disk devices: cfq, deadline, noop (Default: cfq);     #选择磁盘驱动器I/O调度方式，建议deadline</span></span><br><span class=\"line\"><span class=\"meta\"># Separate values for multiple disks with spaces. Use the special value 'keep'</span></span><br><span class=\"line\"><span class=\"meta\"># to keep the kernel default scheduler for the particular disk.</span></span><br><span class=\"line\">DISK_IOSCHED=<span class=\"string\">\"deadline cfq\"</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># SATA aggressive link power management (ALPM):</span></span><br><span class=\"line\"><span class=\"meta\">#   min_power, medium_power, max_performance</span></span><br><span class=\"line\">SATA_LINKPWR_ON_AC=max_performance</span><br><span class=\"line\">SATA_LINKPWR_ON_BAT=min_power</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Exclude SATA host devices from link power management.</span></span><br><span class=\"line\"><span class=\"meta\"># Separate multiple hosts with spaces.</span></span><br><span class=\"line\"><span class=\"meta\">#SATA_LINKPWR_BLACKLIST=\"host1\"</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Runtime Power Management for AHCI controllers and disks:      #请谨慎开启，有可能导致磁盘被锁或者数据丢失</span></span><br><span class=\"line\"><span class=\"meta\">#   on=disable, auto=enable</span></span><br><span class=\"line\"><span class=\"meta\"># EXPERIMENTAL ** WARNING: auto will most likely cause system lockups/data loss</span></span><br><span class=\"line\"><span class=\"meta\">#AHCI_RUNTIME_PM_ON_AC=on</span></span><br><span class=\"line\"><span class=\"meta\">#AHCI_RUNTIME_PM_ON_BAT=on</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Seconds of inactivity before disk is suspended</span></span><br><span class=\"line\">AHCI_RUNTIME_PM_TIMEOUT=<span class=\"number\">15</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># PCI Express Active State Power Management (PCIe ASPM):</span></span><br><span class=\"line\"><span class=\"meta\">#   default, performance, powersave</span></span><br><span class=\"line\">PCIE_ASPM_ON_AC=performance</span><br><span class=\"line\">PCIE_ASPM_ON_BAT=powersave</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Radeon graphics clock speed (profile method): low, mid, high, auto, default;      #NVIDIA显卡用户请无视或者禁用</span></span><br><span class=\"line\"><span class=\"meta\"># auto = mid on BAT, high on AC; default = use hardware defaults.</span></span><br><span class=\"line\"><span class=\"meta\"># (Kernel &gt;= 2.6.35 only, open-source radeon driver explicitly)</span></span><br><span class=\"line\"><span class=\"meta\">#RADEON_POWER_PROFILE_ON_AC=high</span></span><br><span class=\"line\"><span class=\"meta\">#RADEON_POWER_PROFILE_ON_BAT=low</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Radeon dynamic power management method (DPM): battery, performance        #NVIDIA显卡用户请无视或者禁用</span></span><br><span class=\"line\"><span class=\"meta\"># (Kernel &gt;= 3.11 only, requires boot option radeon.dpm=1)</span></span><br><span class=\"line\"><span class=\"meta\">#RADEON_DPM_STATE_ON_AC=performance</span></span><br><span class=\"line\"><span class=\"meta\">#RADEON_DPM_STATE_ON_BAT=battery</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Radeon DPM performance level: auto, low, high; auto is recommended .           #NVIDIA显卡用户请无视或者禁用</span></span><br><span class=\"line\"><span class=\"meta\">#RADEON_DPM_PERF_LEVEL_ON_AC=auto</span></span><br><span class=\"line\"><span class=\"meta\">#RADEON_DPM_PERF_LEVEL_ON_BAT=auto</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># WiFi power saving mode: on=enable, off=disable; not supported by all adapters.</span></span><br><span class=\"line\">WIFI_PWR_ON_AC=off</span><br><span class=\"line\">WIFI_PWR_ON_BAT=on</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Disable wake on LAN: Y/N                 #禁用WOL</span></span><br><span class=\"line\">WOL_DISABLE=N</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Enable audio power saving for Intel HDA, AC97 devices (timeout in secs).     #与音频有关</span></span><br><span class=\"line\"><span class=\"meta\"># A value of 0 disables, &gt;=1 enables power save.</span></span><br><span class=\"line\">SOUND_POWER_SAVE_ON_AC=<span class=\"number\">0</span></span><br><span class=\"line\">SOUND_POWER_SAVE_ON_BAT=<span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Disable controller too (HDA only): Y/N            #与音频有关</span></span><br><span class=\"line\">SOUND_POWER_SAVE_CONTROLLER=Y</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Set to 1 to power off optical drive in UltraBay/MediaBay when running on</span></span><br><span class=\"line\"><span class=\"meta\"># battery. A value of 0 disables this feature (Default).</span></span><br><span class=\"line\"><span class=\"meta\"># Drive can be powered on again by releasing (and reinserting) the eject lever</span></span><br><span class=\"line\"><span class=\"meta\"># or by pressing the disc eject button on newer models.</span></span><br><span class=\"line\"><span class=\"meta\"># Note: an UltraBay/MediaBay hard disk is never powered off.</span></span><br><span class=\"line\">BAY_POWEROFF_ON_BAT=<span class=\"number\">0</span></span><br><span class=\"line\"><span class=\"meta\"># Optical drive device to power off (default sr0).</span></span><br><span class=\"line\">BAY_DEVICE=<span class=\"string\">\"sr0\"</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Runtime Power Management for PCI(e) bus devices: on=disable, auto=enable</span></span><br><span class=\"line\">RUNTIME_PM_ON_AC=on</span><br><span class=\"line\">RUNTIME_PM_ON_BAT=auto</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Runtime PM for *all* PCI(e) bus devices, except blacklisted ones:</span></span><br><span class=\"line\"><span class=\"meta\">#   0=disable, 1=enable</span></span><br><span class=\"line\">RUNTIME_PM_ALL=<span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Exclude PCI(e) device adresses the following list from Runtime PM</span></span><br><span class=\"line\"><span class=\"meta\"># (separate with spaces). Use lspci to get the adresses (1st column).</span></span><br><span class=\"line\"><span class=\"meta\">#RUNTIME_PM_BLACKLIST=\"bb:dd.f 11:22.3 44:55.6\"</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Exclude PCI(e) devices assigned to the listed drivers from Runtime PM</span></span><br><span class=\"line\"><span class=\"meta\"># (should prevent accidential power on of hybrid graphics' discrete part).</span></span><br><span class=\"line\"><span class=\"meta\"># Default is \"radeon nouveau\"; use \"\" to disable the feature completely.</span></span><br><span class=\"line\"><span class=\"meta\"># Separate multiple drivers with spaces.</span></span><br><span class=\"line\">RUNTIME_PM_DRIVER_BLACKLIST=<span class=\"string\">\"radeon nouveau\"</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Set to 0 to disable, 1 to enable USB autosuspend feature.</span></span><br><span class=\"line\">USB_AUTOSUSPEND=<span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Exclude listed devices from USB autosuspend (separate with spaces).</span></span><br><span class=\"line\"><span class=\"meta\"># Use lsusb to get the ids.</span></span><br><span class=\"line\"><span class=\"meta\"># Note: input devices (usbhid) are excluded automatically (see below)</span></span><br><span class=\"line\"><span class=\"meta\">#USB_BLACKLIST=\"1111:2222 3333:4444\"</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># WWAN devices are excluded from USB autosuspend: 0=do not exclude / 1=exclude</span></span><br><span class=\"line\">USB_BLACKLIST_WWAN=<span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Include listed devices into USB autosuspend even if already excluded</span></span><br><span class=\"line\"><span class=\"meta\"># by the driver or WWAN blacklists above (separate with spaces).</span></span><br><span class=\"line\"><span class=\"meta\"># Use lsusb to get the ids.</span></span><br><span class=\"line\"><span class=\"meta\">#USB_WHITELIST=\"1111:2222 3333:4444\"</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Set to 1 to disable autosuspend before shutdown, 0 to do nothing</span></span><br><span class=\"line\"><span class=\"meta\"># (workaround for USB devices that cause shutdown problems).</span></span><br><span class=\"line\"><span class=\"meta\">#USB_AUTOSUSPEND_DISABLE_ON_SHUTDOWN=1</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Restore radio device state (Bluetooth, WiFi, WWAN) from previous shutdown</span></span><br><span class=\"line\"><span class=\"meta\"># on system startup: 0=disable, 1=enable.</span></span><br><span class=\"line\"><span class=\"meta\"># Hint: the parameters DEVICES_TO_DISABLE/ENABLE_ON_STARTUP/SHUTDOWN below</span></span><br><span class=\"line\"><span class=\"meta\">#   are ignored when this is enabled!</span></span><br><span class=\"line\">RESTORE_DEVICE_STATE_ON_STARTUP=<span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Radio devices to disable on startup: bluetooth, wifi, wwan.</span></span><br><span class=\"line\"><span class=\"meta\"># Separate multiple devices with spaces.</span></span><br><span class=\"line\"><span class=\"meta\">#DEVICES_TO_DISABLE_ON_STARTUP=\"bluetooth wifi wwan\"</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Radio devices to enable on startup: bluetooth, wifi, wwan.</span></span><br><span class=\"line\"><span class=\"meta\"># Separate multiple devices with spaces.</span></span><br><span class=\"line\"><span class=\"meta\">#DEVICES_TO_ENABLE_ON_STARTUP=\"wifi\"</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Radio devices to disable on shutdown: bluetooth, wifi, wwan</span></span><br><span class=\"line\"><span class=\"meta\"># (workaround for devices that are blocking shutdown).</span></span><br><span class=\"line\"><span class=\"meta\">#DEVICES_TO_DISABLE_ON_SHUTDOWN=\"bluetooth wifi wwan\"</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Radio devices to enable on shutdown: bluetooth, wifi, wwan</span></span><br><span class=\"line\"><span class=\"meta\"># (to prevent other operating systems from missing radios).</span></span><br><span class=\"line\"><span class=\"meta\">#DEVICES_TO_ENABLE_ON_SHUTDOWN=\"wwan\"</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Radio devices to enable on AC: bluetooth, wifi, wwan</span></span><br><span class=\"line\"><span class=\"meta\">#DEVICES_TO_ENABLE_ON_AC=\"bluetooth wifi wwan\"</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Radio devices to disable on battery: bluetooth, wifi, wwan</span></span><br><span class=\"line\"><span class=\"meta\">#DEVICES_TO_DISABLE_ON_BAT=\"bluetooth wifi wwan\"</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Radio devices to disable on battery when not in use (not connected):</span></span><br><span class=\"line\"><span class=\"meta\"># bluetooth, wifi, wwan</span></span><br><span class=\"line\"><span class=\"meta\">#DEVICES_TO_DISABLE_ON_BAT_NOT_IN_USE=\"bluetooth wifi wwan\"</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Battery charge thresholds (ThinkPad only, tp-smapi or acpi-call kernel module             #ThinkPad笔记本使用，其它品牌谨慎开启</span></span><br><span class=\"line\"><span class=\"meta\"># required). Charging starts when the remaining capacity falls below the</span></span><br><span class=\"line\"><span class=\"meta\"># START_CHARGE_THRESH value and stops when exceeding the STOP_CHARGE_THRESH value.</span></span><br><span class=\"line\"><span class=\"meta\"># Main / Internal battery (values in %)</span></span><br><span class=\"line\"><span class=\"meta\">#START_CHARGE_THRESH_BAT0=75</span></span><br><span class=\"line\"><span class=\"meta\">#STOP_CHARGE_THRESH_BAT0=80</span></span><br><span class=\"line\"><span class=\"meta\"># Ultrabay / Slice / Replaceable battery (values in %)</span></span><br><span class=\"line\"><span class=\"meta\">#START_CHARGE_THRESH_BAT1=75</span></span><br><span class=\"line\"><span class=\"meta\">#STOP_CHARGE_THRESH_BAT1=80</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># ------------------------------------------------------------------------------</span></span><br><span class=\"line\"><span class=\"meta\"># tlp-rdw - Parameters for the radio device wizard</span></span><br><span class=\"line\"><span class=\"meta\"># Possible devices: bluetooth, wifi, wwan</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Hints:</span></span><br><span class=\"line\"><span class=\"meta\"># - Parameters are disabled by default, remove the leading # to enable them.</span></span><br><span class=\"line\"><span class=\"meta\"># - Separate multiple radio devices with spaces.</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Radio devices to disable on connect.</span></span><br><span class=\"line\"><span class=\"meta\">#DEVICES_TO_DISABLE_ON_LAN_CONNECT=\"wifi wwan\"</span></span><br><span class=\"line\"><span class=\"meta\">#DEVICES_TO_DISABLE_ON_WIFI_CONNECT=\"wwan\"</span></span><br><span class=\"line\"><span class=\"meta\">#DEVICES_TO_DISABLE_ON_WWAN_CONNECT=\"wifi\"</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Radio devices to enable on disconnect.</span></span><br><span class=\"line\"><span class=\"meta\">#DEVICES_TO_ENABLE_ON_LAN_DISCONNECT=\"wifi wwan\"</span></span><br><span class=\"line\"><span class=\"meta\">#DEVICES_TO_ENABLE_ON_WIFI_DISCONNECT=\"\"</span></span><br><span class=\"line\"><span class=\"meta\">#DEVICES_TO_ENABLE_ON_WWAN_DISCONNECT=\"\"</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Radio devices to enable/disable when docked.</span></span><br><span class=\"line\"><span class=\"meta\">#DEVICES_TO_ENABLE_ON_DOCK=\"\"</span></span><br><span class=\"line\"><span class=\"meta\">#DEVICES_TO_DISABLE_ON_DOCK=\"\"</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\"># Radio devices to enable/disable when undocked.</span></span><br><span class=\"line\"><span class=\"meta\">#DEVICES_TO_ENABLE_ON_UNDOCK=\"wifi\"</span></span><br><span class=\"line\"><span class=\"meta\">#DEVICES_TO_DISABLE_ON_UNDOCK=\"\"</span></span><br></pre></td></tr></table></figure></p>\n<p>我主要是在cpu那个部分做了一些调整，一个i7的电脑，硬生生被调成了i5的性能，我真的是后悔当时没直接买i5。更详细的信息可以参考<a href=\"https://linrunner.de/en/tlp/tlp.html\" target=\"_blank\" rel=\"noopener\">官方文档</a>.</p>\n<h2 id=\"显卡配置\"><a href=\"#显卡配置\" class=\"headerlink\" title=\"显卡配置\"></a>显卡配置</h2><p>因为跑深度学习的代码需要用很多N卡配置，而默认不使用tlp的话电脑会很热，用了tlp系统会自动的把显卡切换到intel的集成显卡，这个问题一开始苦恼了我好久。其实ubuntu里面可以手动切换显卡的。前提是需要正确安装电脑对应的显卡，我们需要在N卡的驱动附加软件里面实现这个显卡切换功能。ps.Linux上显卡驱动真的是个大坑~<br><img src=\"/images/pasted-14.png\" alt=\"upload successful\">可以看到我现在用的是intel的集成显卡，而实际上我是有N卡的。</p>\n<h3 id=\"正确安装对应驱动\"><a href=\"#正确安装对应驱动\" class=\"headerlink\" title=\"正确安装对应驱动\"></a>正确安装对应驱动</h3><p>如果我们刚安装完ubuntu新系统的话，我们正常在上图看到的显卡类型应该是<strong>nouveau</strong>的驱动，nouveau是一个自由及开放源代码显卡驱动程序，是为Nvidia的显示卡所编写，也可用于属于系统芯片的NVIDIA Tegra系列，该项目的目标为利用逆向工程Nvidia的专有Linux驱动程序来创造一个开放源代码的驱动程序，所以nouveau开源驱动基本上是不能正常使用的。<br>首先查看电脑目前有什么显卡，使用如下命令：<br><figure class=\"highlight perl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">lspci | <span class=\"keyword\">grep</span> VGA     <span class=\"comment\"># 查看集成显卡</span></span><br><span class=\"line\">lspci | <span class=\"keyword\">grep</span> NVIDIA  <span class=\"comment\"># 查看NVIDIA显卡</span></span><br></pre></td></tr></table></figure></p>\n<p>我的电脑是小米pro，可以看出显卡是GeForce MX150的。<br><img src=\"/images/pasted-16.png\" alt=\"upload successful\"></p>\n<p>下面查看应该安装什么版本的驱动，如下命令：</p>\n<figure class=\"highlight 1c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ubuntu-drivers devices   <span class=\"meta\"># 查询所有ubuntu推荐的驱动</span></span><br></pre></td></tr></table></figure>\n<p>可以看出ubutnu提示我应该安装390版本的驱动。<br><img src=\"/images/pasted-17.png\" alt=\"upload successful\"></p>\n<p>然后就可以使用下面条命令安装所有推荐的驱动程序：</p>\n<figure class=\"highlight ebnf\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attribute\">sudo ubuntu-drivers autoinstall</span></span><br></pre></td></tr></table></figure>\n<p>或者<br><figure class=\"highlight routeros\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo apt-<span class=\"builtin-name\">get</span> install nvidia-390</span><br></pre></td></tr></table></figure></p>\n<p>安装完成后重启，在<strong>软件和更新</strong>中我们可以看到已经安装了哪些驱动：<br><img src=\"/images/pasted-18.png\" alt=\"upload successful\"><br>可以看到我已经安装了390和nouveau这两个驱动，勾选390那个就行。</p>\n<h3 id=\"切换显卡\"><a href=\"#切换显卡\" class=\"headerlink\" title=\"切换显卡\"></a>切换显卡</h3><p>随后我们就可以使用使用nvidia-settings命令了：<br><figure class=\"highlight ebnf\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attribute\">nvidia-settings</span></span><br></pre></td></tr></table></figure></p>\n<p>我们可以在这里选择我们要使用哪个显卡，切换之后reboot就好了。<br><img src=\"/images/pasted-19.png\" alt=\"upload successful\"></p>\n<p>还有最后一点注意的！！！！！！！！！！！！！</p>\n<p><strong>如果安装cuda的话，一定不再选择安装驱动，要不然就前功尽弃啦！！！</strong></p>\n<h1 id=\"美化系统主题\"><a href=\"#美化系统主题\" class=\"headerlink\" title=\"美化系统主题\"></a>美化系统主题</h1><p>作为一个颜控党，主题这种东西如果不和为空是万万不可以的。<br>从ubuntu17.10开始，官方又开始使用gnome作为默认的桌面环境，意味着我们可以轻松的使用GNOME Shell扩展了，美滋滋。先放几张图：<br><img src=\"/images/pasted-20.png\" alt=\"upload successful\"></p>\n<p><img src=\"/images/pasted-21.png\" alt=\"upload successful\"></p>\n<p><img src=\"/images/pasted-22.png\" alt=\"upload successful\">搞了个北欧风，十分舒服～～</p>\n<p>首先我们要安装GNOME Tweak Tool。</p>\n<figure class=\"highlight cmake\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo apt <span class=\"keyword\">install</span> gnome-tweak-tool</span><br></pre></td></tr></table></figure>\n<p>安装之后你就能找到这个东西：</p>\n<p><img src=\"/images/pasted-23.png\" alt=\"upload successful\"><br>打开：<br><img src=\"/images/pasted-24.png\" alt=\"upload successful\"></p>\n<p>在这里可以配置不同的外观，也可以安装不同的扩展小插件，在这里我已经安装好配置主题需要的扩展了，正常来说我们需要先安装User themes这个扩展。<br><img src=\"/images/pasted-25.png\" alt=\"upload successful\"><br>我们可通过chrome来方便的安装任何扩展，首先我们需要在chrome里面安装GNOME的小程序：!<br><img src=\"/images/pasted-26.png\" alt=\"upload successful\"></p>\n<p>随后浏览器打开<a href=\"https://extensions.gnome.org/\" target=\"_blank\" rel=\"noopener\">https://extensions.gnome.org/</a> 安装这个扩展：<br><img src=\"/images/pasted-27.png\" alt=\"upload successful\"><br>点进去，把开关调成on状态就好了：<br><img src=\"/images/pasted-28.png\" alt=\"upload successful\"><br>随后我们就能使用主题包来装饰我们的系统了。</p>\n<p>对于Gnome桌面，你最需要连接的就是这个<a href=\"https://www.gnome-look.org/\" target=\"_blank\" rel=\"noopener\">网站</a>，它提供了包括主题、图标、字体等在内的很多包。<br>因为在上述中安装了User Themes 扩展，所以我们可以把下载好的主题放置在自己的家目录下，为此，在家目录下的.local/share中新建themes、fonts、icons 三个文件夹，分别存放主题、字体和图标 。下载我们喜欢的主题和图标之后（一般是一个压缩包），我们需要把压缩包放到对应的文件夹里，并且进行解压缩，这里提供几个常用的命令：<br><figure class=\"highlight crmsh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">//解压</span><br><span class=\"line\">$ xz -d <span class=\"keyword\">node</span><span class=\"title\">-v6</span>.<span class=\"number\">10.1</span>-linux-x64.tar.xz</span><br><span class=\"line\">$ tar -xvf <span class=\"keyword\">node</span><span class=\"title\">-v6</span>.<span class=\"number\">10.1</span>-linux-x64.tar</span><br><span class=\"line\"></span><br><span class=\"line\">//移动文件</span><br><span class=\"line\">cp -r 文件 ~/.local/share/icons</span><br><span class=\"line\"></span><br><span class=\"line\">//本机gnome主题、字体、图标地址(https://www.gnome-look.org/)</span><br><span class=\"line\">~/.local/share/themes</span><br><span class=\"line\">~/.local/share/fonts</span><br><span class=\"line\">~/.local/share/icons</span><br></pre></td></tr></table></figure></p>\n<p>上面的步骤完成后，我们就可以在这里进行修改了：<br><img src=\"/images/pasted-29.png\" alt=\"upload successful\"></p>\n<h1 id=\"美化grub启动项\"><a href=\"#美化grub启动项\" class=\"headerlink\" title=\"美化grub启动项\"></a>美化grub启动项</h1><p>个人是不太喜欢紫色的，尤其开机grub引导项的那个基佬紫，这的受不了啊，要想个办法改一下。<br>这个部分我主要参考的是这个<a href=\"https://blog.csdn.net/w84963568/article/details/78884003\" target=\"_blank\" rel=\"noopener\">博主</a>的文章，所以不过多的介绍了。</p>\n<h1 id=\"基础命令操作\"><a href=\"#基础命令操作\" class=\"headerlink\" title=\"基础命令操作\"></a>基础命令操作</h1><p>未完待续。。。</p>\n"},{"title":"交叉熵的数学原理及应用——Pytorch中的CrossEntropyLoss()函数","author":"wangjiansen","date":"2019-02-15T10:47:00.000Z","_content":"分类问题中，交叉熵函数是比较常用也是比较基础的损失函数，原来就是了解，但一直搞不懂他是怎么来的？为什么交叉熵能够表征真实样本标签和预测概率之间的差值？趁着这次学习把这些概念系统学习了一下。\n# 交叉熵的数学原理\n首先说起交叉熵，脑子里就会出现这个东西：\n$$L=-[y\\log{\\hat{y}}+(1-y)\\log{(1-\\hat{y})}]\n$$\n随后我们脑子里可能还会出现Sigmoid()这个函数:\n$$\n{g(s)}=\\frac{1}{1+e^{-s}}\n$$\npytorch中的CrossEntropyLoss()函数实际就是先把输出结果进行sigmoid，随后再放到传统的交叉熵函数中，就会得到结果。\n那我们就先从sigmoid开始说起，我们知道sigmoid的作用其实是把前一层的输入映射到0~1这个区间上，可以认为上一层某个样本的输入数据越大，就代表这个样本标签属于1的概率就越大，反之，上一层某样本的输入数据越小，这个样本标签属于0的概率就越大，而且通过sigmoid函数的图像我们可以看出来，随着输入数值的增大，其对概率增大的作用效果是逐渐减弱的，反之同理，这就是非线性映射的一个好处，让模型对处于中间范围的输入数据更敏感。下面是sigmoid函数图：\n\n<img src=\"/images/pasted-10.png\" width=\"40%\" height=\"50%\">\n\n既然经过sigmoid之后的数据能表示样本所属某个标签的概率，那么举个例子，我们模型预测某个样本标签为1的概率是：\n$$\n\\hat{y}=P(y=1|x)\n$$\n那么自然的，这个样本标签不为1的概率是： \n$$\n1-\\hat{y}=P(y=0|x)\n$$\n从极大似然的角度来说就是：\n$$\nP(y|x)=\\hat{y}^{y}(1-\\hat{y})^{1-y}\n$$\n\n上式可以理解为，某一个样本x，我们通过模型预测出其属于样本标签为y的概率，因为y是我们给的正确结果，所以我们当然希望上式越大越好。\n\n下一步我们要在**P(y|x)**  的外面套上一层log函数，相当于进行了一次非线性的映射。log函数是不会改变单调性的，所以我们也希望**log(P(y|x))** 越大越好。\n$$\n\\log{(P(y|x))}=\\log{(\\hat{y}^{y}(1-\\hat{y})^{1-y})}=y\\log{\\hat{y}}+(1-y)\\log{(1-\\hat{y})}\n$$\n这样，就得到了我们一开始说的交叉熵的形式了，但是等一等，好像还差一个符号。\n\n因为一般来说我们相用上述公式做loss函数来使用，所以我们想要loss越小越好，这样符合我们的直观理解，所以我们只要**-log(P(y|x))** 就达到了我们的目的。\n$$\nL=-[y\\log{\\hat{y}}+(1-y)\\log{(1-\\hat{y})}]\n$$\n上面是二分类问题的交叉熵，如果是有多分类，就对每个标签类别下的可能概率分别求相应的负log对数然后求和就好了：\n$$\nL=-\\sum_{i=1}^n y^{(i)}\\log{\\hat{y}^{(i)}}\n$$\n是不是突然也感觉有些理解了，(*^__^*) ……\n# Pytorch中的函数 CrossEntropyLoss()\n\n上面是对交叉熵进行了推导，下面要结合pytorch中的函数 CrossEntropyLoss()  来说一说具体怎么使用了。\n\n举个小例子，假设我们有个一样本，他经过我们的神经网络后会输出一个5维的向量，分别代表这个样本分别属于这5种标签的数值（注意此时我们的5个数求和还并不等于1，需要先经过softmax处理，下面会说），我们还会从数据集中得到该样本的正确分类结果，下面我们要把经过神经网络的5维向量和正确的分类结果放到CrossEntropyLoss() 中，看看会发生什么：\n~~~python\nimport torch\nimport torch.nn as nn\nimport math\nloss = nn.CrossEntropyLoss()\ninput = torch.randn(1,5,requires_grad=True)\ntarget = torch.empty(1,dtype=torch.long).random_(5)\noutput = loss(input,target)\n\nprint(\"输入为5类：\")\nprint(input)\nprint(\"要计算的loss的类别：\")\nprint(target)\nprint(\"要计算的loss的结果：\")\nprint(output)\n\nfirst = 0\nfor i in range(1):\n\tfirst -= input[i][target[i]]\nsecond = 0\nfor i in range(1):\n\tfor i in range(5):\n\t\tsecond += math.exp(input[i][j])\nres = 0\nres += first + printmath.log(second)\nprint(\"手动的计算结果\")\nprint(res)\n~~~\n看一看我们的input和target：\n\n<img src=\"/images/pasted-11.png\" width=\"90%\" height=\"50%\">\n\n可以看到我们的target就是一个只有一个数的数组形式（不是向量，不是矩阵，只是一个简单的数组，而且里面就一个数），input是一个5维的向量，但这，在计算交叉熵之前，我们需要先获得下面交叉熵公式的$\\hat{y}^{(i)}$.\n$$\nL=-\\sum_{i=1}^n y^{(i)}\\log{\\hat{y}^{(i)}}\n$$\n此处的$\\hat{y}^{(i)}$需要我们将输入的input向量进行softmax处理，使得input变成对应属于每个标签的概率值，对每个**input[i]** 进行如下处理：\n$$\n\\hat{y}=P(\\hat{y}=i|x)=\\frac{e^{input[i]}}{ \\sum_{j=0}^ne^{input[j]} }\n$$\n\n这样我们就得到了交叉熵公式中$\\hat{y}^{(i)}$\n\n\n随后我们就可以把$\\hat{y}^{(i)}$带入公式了，下面我们还缺$y^{(i)}$就可以了，而奇怪的是我们输入的target是一个只有一个数的数组啊，而$\\hat{y}^{(i)}$是一个5维的向量，这什么情况？\n\n原来CrossEntropyLoss() 会把target变成ont-hot形式（网上别人说的，想等有时间去看看函数的源代码随后补充一下这里），我们现在例子的样本标签是【4】（从0开始计算）。那么转换成one-hot编码就是【0，0，0，0，1】，所以我们的$y^{(i)}$最后也会变成一个5维的向量的向量，并且不是该样本标签的数值为0，这样我们在计算交叉熵的时候只计算$y^{(i)}$给定的那一项的sorce就好了，所以我们的公式最后变成了：\n$$\nL(input,target)=-\\log{\\frac{e^{input[target]}}{ \\sum_{j=0}^ne^{input[j]} }}=-input[target]+\\log{(\\sum_{j=0}^ne^{input[j]})}\n$$\n\n好，安装上面我们的推导来运行一下程序：\n\n![upload successful](/images/pasted-12.png)\n\n破发科特~~~~~~\n开学快乐(*^__^*) ……","source":"_posts/交叉熵的数学原理及应用——Pytorch中的CrossEntropyLoss()函数.md","raw":"title: 交叉熵的数学原理及应用——Pytorch中的CrossEntropyLoss()函数\nauthor: wangjiansen\ntags:\n  - 深度学习\n  - Pytorch\ncategories:\n  - 深度学习\ndate: 2019-02-15 18:47:00\n---\n分类问题中，交叉熵函数是比较常用也是比较基础的损失函数，原来就是了解，但一直搞不懂他是怎么来的？为什么交叉熵能够表征真实样本标签和预测概率之间的差值？趁着这次学习把这些概念系统学习了一下。\n# 交叉熵的数学原理\n首先说起交叉熵，脑子里就会出现这个东西：\n$$L=-[y\\log{\\hat{y}}+(1-y)\\log{(1-\\hat{y})}]\n$$\n随后我们脑子里可能还会出现Sigmoid()这个函数:\n$$\n{g(s)}=\\frac{1}{1+e^{-s}}\n$$\npytorch中的CrossEntropyLoss()函数实际就是先把输出结果进行sigmoid，随后再放到传统的交叉熵函数中，就会得到结果。\n那我们就先从sigmoid开始说起，我们知道sigmoid的作用其实是把前一层的输入映射到0~1这个区间上，可以认为上一层某个样本的输入数据越大，就代表这个样本标签属于1的概率就越大，反之，上一层某样本的输入数据越小，这个样本标签属于0的概率就越大，而且通过sigmoid函数的图像我们可以看出来，随着输入数值的增大，其对概率增大的作用效果是逐渐减弱的，反之同理，这就是非线性映射的一个好处，让模型对处于中间范围的输入数据更敏感。下面是sigmoid函数图：\n\n<img src=\"/images/pasted-10.png\" width=\"40%\" height=\"50%\">\n\n既然经过sigmoid之后的数据能表示样本所属某个标签的概率，那么举个例子，我们模型预测某个样本标签为1的概率是：\n$$\n\\hat{y}=P(y=1|x)\n$$\n那么自然的，这个样本标签不为1的概率是： \n$$\n1-\\hat{y}=P(y=0|x)\n$$\n从极大似然的角度来说就是：\n$$\nP(y|x)=\\hat{y}^{y}(1-\\hat{y})^{1-y}\n$$\n\n上式可以理解为，某一个样本x，我们通过模型预测出其属于样本标签为y的概率，因为y是我们给的正确结果，所以我们当然希望上式越大越好。\n\n下一步我们要在**P(y|x)**  的外面套上一层log函数，相当于进行了一次非线性的映射。log函数是不会改变单调性的，所以我们也希望**log(P(y|x))** 越大越好。\n$$\n\\log{(P(y|x))}=\\log{(\\hat{y}^{y}(1-\\hat{y})^{1-y})}=y\\log{\\hat{y}}+(1-y)\\log{(1-\\hat{y})}\n$$\n这样，就得到了我们一开始说的交叉熵的形式了，但是等一等，好像还差一个符号。\n\n因为一般来说我们相用上述公式做loss函数来使用，所以我们想要loss越小越好，这样符合我们的直观理解，所以我们只要**-log(P(y|x))** 就达到了我们的目的。\n$$\nL=-[y\\log{\\hat{y}}+(1-y)\\log{(1-\\hat{y})}]\n$$\n上面是二分类问题的交叉熵，如果是有多分类，就对每个标签类别下的可能概率分别求相应的负log对数然后求和就好了：\n$$\nL=-\\sum_{i=1}^n y^{(i)}\\log{\\hat{y}^{(i)}}\n$$\n是不是突然也感觉有些理解了，(*^__^*) ……\n# Pytorch中的函数 CrossEntropyLoss()\n\n上面是对交叉熵进行了推导，下面要结合pytorch中的函数 CrossEntropyLoss()  来说一说具体怎么使用了。\n\n举个小例子，假设我们有个一样本，他经过我们的神经网络后会输出一个5维的向量，分别代表这个样本分别属于这5种标签的数值（注意此时我们的5个数求和还并不等于1，需要先经过softmax处理，下面会说），我们还会从数据集中得到该样本的正确分类结果，下面我们要把经过神经网络的5维向量和正确的分类结果放到CrossEntropyLoss() 中，看看会发生什么：\n~~~python\nimport torch\nimport torch.nn as nn\nimport math\nloss = nn.CrossEntropyLoss()\ninput = torch.randn(1,5,requires_grad=True)\ntarget = torch.empty(1,dtype=torch.long).random_(5)\noutput = loss(input,target)\n\nprint(\"输入为5类：\")\nprint(input)\nprint(\"要计算的loss的类别：\")\nprint(target)\nprint(\"要计算的loss的结果：\")\nprint(output)\n\nfirst = 0\nfor i in range(1):\n\tfirst -= input[i][target[i]]\nsecond = 0\nfor i in range(1):\n\tfor i in range(5):\n\t\tsecond += math.exp(input[i][j])\nres = 0\nres += first + printmath.log(second)\nprint(\"手动的计算结果\")\nprint(res)\n~~~\n看一看我们的input和target：\n\n<img src=\"/images/pasted-11.png\" width=\"90%\" height=\"50%\">\n\n可以看到我们的target就是一个只有一个数的数组形式（不是向量，不是矩阵，只是一个简单的数组，而且里面就一个数），input是一个5维的向量，但这，在计算交叉熵之前，我们需要先获得下面交叉熵公式的$\\hat{y}^{(i)}$.\n$$\nL=-\\sum_{i=1}^n y^{(i)}\\log{\\hat{y}^{(i)}}\n$$\n此处的$\\hat{y}^{(i)}$需要我们将输入的input向量进行softmax处理，使得input变成对应属于每个标签的概率值，对每个**input[i]** 进行如下处理：\n$$\n\\hat{y}=P(\\hat{y}=i|x)=\\frac{e^{input[i]}}{ \\sum_{j=0}^ne^{input[j]} }\n$$\n\n这样我们就得到了交叉熵公式中$\\hat{y}^{(i)}$\n\n\n随后我们就可以把$\\hat{y}^{(i)}$带入公式了，下面我们还缺$y^{(i)}$就可以了，而奇怪的是我们输入的target是一个只有一个数的数组啊，而$\\hat{y}^{(i)}$是一个5维的向量，这什么情况？\n\n原来CrossEntropyLoss() 会把target变成ont-hot形式（网上别人说的，想等有时间去看看函数的源代码随后补充一下这里），我们现在例子的样本标签是【4】（从0开始计算）。那么转换成one-hot编码就是【0，0，0，0，1】，所以我们的$y^{(i)}$最后也会变成一个5维的向量的向量，并且不是该样本标签的数值为0，这样我们在计算交叉熵的时候只计算$y^{(i)}$给定的那一项的sorce就好了，所以我们的公式最后变成了：\n$$\nL(input,target)=-\\log{\\frac{e^{input[target]}}{ \\sum_{j=0}^ne^{input[j]} }}=-input[target]+\\log{(\\sum_{j=0}^ne^{input[j]})}\n$$\n\n好，安装上面我们的推导来运行一下程序：\n\n![upload successful](/images/pasted-12.png)\n\n破发科特~~~~~~\n开学快乐(*^__^*) ……","slug":"交叉熵的数学原理及应用——Pytorch中的CrossEntropyLoss()函数","published":1,"updated":"2019-03-11T07:57:32.216Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjt8bm3ue00089gojd1ox472p","content":"<p>分类问题中，交叉熵函数是比较常用也是比较基础的损失函数，原来就是了解，但一直搞不懂他是怎么来的？为什么交叉熵能够表征真实样本标签和预测概率之间的差值？趁着这次学习把这些概念系统学习了一下。</p>\n<h1 id=\"交叉熵的数学原理\"><a href=\"#交叉熵的数学原理\" class=\"headerlink\" title=\"交叉熵的数学原理\"></a>交叉熵的数学原理</h1><p>首先说起交叉熵，脑子里就会出现这个东西：<br>$$L=-[y\\log{\\hat{y}}+(1-y)\\log{(1-\\hat{y})}]<br>$$<br>随后我们脑子里可能还会出现Sigmoid()这个函数:<br>$$<br>{g(s)}=\\frac{1}{1+e^{-s}}<br>$$<br>pytorch中的CrossEntropyLoss()函数实际就是先把输出结果进行sigmoid，随后再放到传统的交叉熵函数中，就会得到结果。<br>那我们就先从sigmoid开始说起，我们知道sigmoid的作用其实是把前一层的输入映射到0~1这个区间上，可以认为上一层某个样本的输入数据越大，就代表这个样本标签属于1的概率就越大，反之，上一层某样本的输入数据越小，这个样本标签属于0的概率就越大，而且通过sigmoid函数的图像我们可以看出来，随着输入数值的增大，其对概率增大的作用效果是逐渐减弱的，反之同理，这就是非线性映射的一个好处，让模型对处于中间范围的输入数据更敏感。下面是sigmoid函数图：</p>\n<p><img src=\"/images/pasted-10.png\" width=\"40%\" height=\"50%\"></p>\n<p>既然经过sigmoid之后的数据能表示样本所属某个标签的概率，那么举个例子，我们模型预测某个样本标签为1的概率是：<br>$$<br>\\hat{y}=P(y=1|x)<br>$$<br>那么自然的，这个样本标签不为1的概率是：<br>$$<br>1-\\hat{y}=P(y=0|x)<br>$$<br>从极大似然的角度来说就是：<br>$$<br>P(y|x)=\\hat{y}^{y}(1-\\hat{y})^{1-y}<br>$$</p>\n<p>上式可以理解为，某一个样本x，我们通过模型预测出其属于样本标签为y的概率，因为y是我们给的正确结果，所以我们当然希望上式越大越好。</p>\n<p>下一步我们要在<strong>P(y|x)</strong>  的外面套上一层log函数，相当于进行了一次非线性的映射。log函数是不会改变单调性的，所以我们也希望<strong>log(P(y|x))</strong> 越大越好。<br>$$<br>\\log{(P(y|x))}=\\log{(\\hat{y}^{y}(1-\\hat{y})^{1-y})}=y\\log{\\hat{y}}+(1-y)\\log{(1-\\hat{y})}<br>$$<br>这样，就得到了我们一开始说的交叉熵的形式了，但是等一等，好像还差一个符号。</p>\n<p>因为一般来说我们相用上述公式做loss函数来使用，所以我们想要loss越小越好，这样符合我们的直观理解，所以我们只要<strong>-log(P(y|x))</strong> 就达到了我们的目的。<br>$$<br>L=-[y\\log{\\hat{y}}+(1-y)\\log{(1-\\hat{y})}]<br>$$<br>上面是二分类问题的交叉熵，如果是有多分类，就对每个标签类别下的可能概率分别求相应的负log对数然后求和就好了：<br>$$<br>L=-\\sum_{i=1}^n y^{(i)}\\log{\\hat{y}^{(i)}}<br>$$<br>是不是突然也感觉有些理解了，(<em>^__^</em>) ……</p>\n<h1 id=\"Pytorch中的函数-CrossEntropyLoss\"><a href=\"#Pytorch中的函数-CrossEntropyLoss\" class=\"headerlink\" title=\"Pytorch中的函数 CrossEntropyLoss()\"></a>Pytorch中的函数 CrossEntropyLoss()</h1><p>上面是对交叉熵进行了推导，下面要结合pytorch中的函数 CrossEntropyLoss()  来说一说具体怎么使用了。</p>\n<p>举个小例子，假设我们有个一样本，他经过我们的神经网络后会输出一个5维的向量，分别代表这个样本分别属于这5种标签的数值（注意此时我们的5个数求和还并不等于1，需要先经过softmax处理，下面会说），我们还会从数据集中得到该样本的正确分类结果，下面我们要把经过神经网络的5维向量和正确的分类结果放到CrossEntropyLoss() 中，看看会发生什么：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn <span class=\"keyword\">as</span> nn</span><br><span class=\"line\"><span class=\"keyword\">import</span> math</span><br><span class=\"line\">loss = nn.CrossEntropyLoss()</span><br><span class=\"line\">input = torch.randn(<span class=\"number\">1</span>,<span class=\"number\">5</span>,requires_grad=<span class=\"keyword\">True</span>)</span><br><span class=\"line\">target = torch.empty(<span class=\"number\">1</span>,dtype=torch.long).random_(<span class=\"number\">5</span>)</span><br><span class=\"line\">output = loss(input,target)</span><br><span class=\"line\"></span><br><span class=\"line\">print(<span class=\"string\">\"输入为5类：\"</span>)</span><br><span class=\"line\">print(input)</span><br><span class=\"line\">print(<span class=\"string\">\"要计算的loss的类别：\"</span>)</span><br><span class=\"line\">print(target)</span><br><span class=\"line\">print(<span class=\"string\">\"要计算的loss的结果：\"</span>)</span><br><span class=\"line\">print(output)</span><br><span class=\"line\"></span><br><span class=\"line\">first = <span class=\"number\">0</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">1</span>):</span><br><span class=\"line\">\tfirst -= input[i][target[i]]</span><br><span class=\"line\">second = <span class=\"number\">0</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">1</span>):</span><br><span class=\"line\">\t<span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">5</span>):</span><br><span class=\"line\">\t\tsecond += math.exp(input[i][j])</span><br><span class=\"line\">res = <span class=\"number\">0</span></span><br><span class=\"line\">res += first + printmath.log(second)</span><br><span class=\"line\">print(<span class=\"string\">\"手动的计算结果\"</span>)</span><br><span class=\"line\">print(res)</span><br></pre></td></tr></table></figure></p>\n<p>看一看我们的input和target：</p>\n<p><img src=\"/images/pasted-11.png\" width=\"90%\" height=\"50%\"></p>\n<p>可以看到我们的target就是一个只有一个数的数组形式（不是向量，不是矩阵，只是一个简单的数组，而且里面就一个数），input是一个5维的向量，但这，在计算交叉熵之前，我们需要先获得下面交叉熵公式的$\\hat{y}^{(i)}$.<br>$$<br>L=-\\sum_{i=1}^n y^{(i)}\\log{\\hat{y}^{(i)}}<br>$$<br>此处的$\\hat{y}^{(i)}$需要我们将输入的input向量进行softmax处理，使得input变成对应属于每个标签的概率值，对每个<strong>input[i]</strong> 进行如下处理：<br>$$<br>\\hat{y}=P(\\hat{y}=i|x)=\\frac{e^{input[i]}}{ \\sum_{j=0}^ne^{input[j]} }<br>$$</p>\n<p>这样我们就得到了交叉熵公式中$\\hat{y}^{(i)}$</p>\n<p>随后我们就可以把$\\hat{y}^{(i)}$带入公式了，下面我们还缺$y^{(i)}$就可以了，而奇怪的是我们输入的target是一个只有一个数的数组啊，而$\\hat{y}^{(i)}$是一个5维的向量，这什么情况？</p>\n<p>原来CrossEntropyLoss() 会把target变成ont-hot形式（网上别人说的，想等有时间去看看函数的源代码随后补充一下这里），我们现在例子的样本标签是【4】（从0开始计算）。那么转换成one-hot编码就是【0，0，0，0，1】，所以我们的$y^{(i)}$最后也会变成一个5维的向量的向量，并且不是该样本标签的数值为0，这样我们在计算交叉熵的时候只计算$y^{(i)}$给定的那一项的sorce就好了，所以我们的公式最后变成了：<br>$$<br>L(input,target)=-\\log{\\frac{e^{input[target]}}{ \\sum_{j=0}^ne^{input[j]} }}=-input[target]+\\log{(\\sum_{j=0}^ne^{input[j]})}<br>$$</p>\n<p>好，安装上面我们的推导来运行一下程序：</p>\n<p><img src=\"/images/pasted-12.png\" alt=\"upload successful\"></p>\n<p>破发科特<del>~</del>~<br>开学快乐(<em>^__^</em>) ……</p>\n","site":{"data":{}},"excerpt":"","more":"<p>分类问题中，交叉熵函数是比较常用也是比较基础的损失函数，原来就是了解，但一直搞不懂他是怎么来的？为什么交叉熵能够表征真实样本标签和预测概率之间的差值？趁着这次学习把这些概念系统学习了一下。</p>\n<h1 id=\"交叉熵的数学原理\"><a href=\"#交叉熵的数学原理\" class=\"headerlink\" title=\"交叉熵的数学原理\"></a>交叉熵的数学原理</h1><p>首先说起交叉熵，脑子里就会出现这个东西：<br>$$L=-[y\\log{\\hat{y}}+(1-y)\\log{(1-\\hat{y})}]<br>$$<br>随后我们脑子里可能还会出现Sigmoid()这个函数:<br>$$<br>{g(s)}=\\frac{1}{1+e^{-s}}<br>$$<br>pytorch中的CrossEntropyLoss()函数实际就是先把输出结果进行sigmoid，随后再放到传统的交叉熵函数中，就会得到结果。<br>那我们就先从sigmoid开始说起，我们知道sigmoid的作用其实是把前一层的输入映射到0~1这个区间上，可以认为上一层某个样本的输入数据越大，就代表这个样本标签属于1的概率就越大，反之，上一层某样本的输入数据越小，这个样本标签属于0的概率就越大，而且通过sigmoid函数的图像我们可以看出来，随着输入数值的增大，其对概率增大的作用效果是逐渐减弱的，反之同理，这就是非线性映射的一个好处，让模型对处于中间范围的输入数据更敏感。下面是sigmoid函数图：</p>\n<p><img src=\"/images/pasted-10.png\" width=\"40%\" height=\"50%\"></p>\n<p>既然经过sigmoid之后的数据能表示样本所属某个标签的概率，那么举个例子，我们模型预测某个样本标签为1的概率是：<br>$$<br>\\hat{y}=P(y=1|x)<br>$$<br>那么自然的，这个样本标签不为1的概率是：<br>$$<br>1-\\hat{y}=P(y=0|x)<br>$$<br>从极大似然的角度来说就是：<br>$$<br>P(y|x)=\\hat{y}^{y}(1-\\hat{y})^{1-y}<br>$$</p>\n<p>上式可以理解为，某一个样本x，我们通过模型预测出其属于样本标签为y的概率，因为y是我们给的正确结果，所以我们当然希望上式越大越好。</p>\n<p>下一步我们要在<strong>P(y|x)</strong>  的外面套上一层log函数，相当于进行了一次非线性的映射。log函数是不会改变单调性的，所以我们也希望<strong>log(P(y|x))</strong> 越大越好。<br>$$<br>\\log{(P(y|x))}=\\log{(\\hat{y}^{y}(1-\\hat{y})^{1-y})}=y\\log{\\hat{y}}+(1-y)\\log{(1-\\hat{y})}<br>$$<br>这样，就得到了我们一开始说的交叉熵的形式了，但是等一等，好像还差一个符号。</p>\n<p>因为一般来说我们相用上述公式做loss函数来使用，所以我们想要loss越小越好，这样符合我们的直观理解，所以我们只要<strong>-log(P(y|x))</strong> 就达到了我们的目的。<br>$$<br>L=-[y\\log{\\hat{y}}+(1-y)\\log{(1-\\hat{y})}]<br>$$<br>上面是二分类问题的交叉熵，如果是有多分类，就对每个标签类别下的可能概率分别求相应的负log对数然后求和就好了：<br>$$<br>L=-\\sum_{i=1}^n y^{(i)}\\log{\\hat{y}^{(i)}}<br>$$<br>是不是突然也感觉有些理解了，(<em>^__^</em>) ……</p>\n<h1 id=\"Pytorch中的函数-CrossEntropyLoss\"><a href=\"#Pytorch中的函数-CrossEntropyLoss\" class=\"headerlink\" title=\"Pytorch中的函数 CrossEntropyLoss()\"></a>Pytorch中的函数 CrossEntropyLoss()</h1><p>上面是对交叉熵进行了推导，下面要结合pytorch中的函数 CrossEntropyLoss()  来说一说具体怎么使用了。</p>\n<p>举个小例子，假设我们有个一样本，他经过我们的神经网络后会输出一个5维的向量，分别代表这个样本分别属于这5种标签的数值（注意此时我们的5个数求和还并不等于1，需要先经过softmax处理，下面会说），我们还会从数据集中得到该样本的正确分类结果，下面我们要把经过神经网络的5维向量和正确的分类结果放到CrossEntropyLoss() 中，看看会发生什么：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn <span class=\"keyword\">as</span> nn</span><br><span class=\"line\"><span class=\"keyword\">import</span> math</span><br><span class=\"line\">loss = nn.CrossEntropyLoss()</span><br><span class=\"line\">input = torch.randn(<span class=\"number\">1</span>,<span class=\"number\">5</span>,requires_grad=<span class=\"keyword\">True</span>)</span><br><span class=\"line\">target = torch.empty(<span class=\"number\">1</span>,dtype=torch.long).random_(<span class=\"number\">5</span>)</span><br><span class=\"line\">output = loss(input,target)</span><br><span class=\"line\"></span><br><span class=\"line\">print(<span class=\"string\">\"输入为5类：\"</span>)</span><br><span class=\"line\">print(input)</span><br><span class=\"line\">print(<span class=\"string\">\"要计算的loss的类别：\"</span>)</span><br><span class=\"line\">print(target)</span><br><span class=\"line\">print(<span class=\"string\">\"要计算的loss的结果：\"</span>)</span><br><span class=\"line\">print(output)</span><br><span class=\"line\"></span><br><span class=\"line\">first = <span class=\"number\">0</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">1</span>):</span><br><span class=\"line\">\tfirst -= input[i][target[i]]</span><br><span class=\"line\">second = <span class=\"number\">0</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">1</span>):</span><br><span class=\"line\">\t<span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">5</span>):</span><br><span class=\"line\">\t\tsecond += math.exp(input[i][j])</span><br><span class=\"line\">res = <span class=\"number\">0</span></span><br><span class=\"line\">res += first + printmath.log(second)</span><br><span class=\"line\">print(<span class=\"string\">\"手动的计算结果\"</span>)</span><br><span class=\"line\">print(res)</span><br></pre></td></tr></table></figure></p>\n<p>看一看我们的input和target：</p>\n<p><img src=\"/images/pasted-11.png\" width=\"90%\" height=\"50%\"></p>\n<p>可以看到我们的target就是一个只有一个数的数组形式（不是向量，不是矩阵，只是一个简单的数组，而且里面就一个数），input是一个5维的向量，但这，在计算交叉熵之前，我们需要先获得下面交叉熵公式的$\\hat{y}^{(i)}$.<br>$$<br>L=-\\sum_{i=1}^n y^{(i)}\\log{\\hat{y}^{(i)}}<br>$$<br>此处的$\\hat{y}^{(i)}$需要我们将输入的input向量进行softmax处理，使得input变成对应属于每个标签的概率值，对每个<strong>input[i]</strong> 进行如下处理：<br>$$<br>\\hat{y}=P(\\hat{y}=i|x)=\\frac{e^{input[i]}}{ \\sum_{j=0}^ne^{input[j]} }<br>$$</p>\n<p>这样我们就得到了交叉熵公式中$\\hat{y}^{(i)}$</p>\n<p>随后我们就可以把$\\hat{y}^{(i)}$带入公式了，下面我们还缺$y^{(i)}$就可以了，而奇怪的是我们输入的target是一个只有一个数的数组啊，而$\\hat{y}^{(i)}$是一个5维的向量，这什么情况？</p>\n<p>原来CrossEntropyLoss() 会把target变成ont-hot形式（网上别人说的，想等有时间去看看函数的源代码随后补充一下这里），我们现在例子的样本标签是【4】（从0开始计算）。那么转换成one-hot编码就是【0，0，0，0，1】，所以我们的$y^{(i)}$最后也会变成一个5维的向量的向量，并且不是该样本标签的数值为0，这样我们在计算交叉熵的时候只计算$y^{(i)}$给定的那一项的sorce就好了，所以我们的公式最后变成了：<br>$$<br>L(input,target)=-\\log{\\frac{e^{input[target]}}{ \\sum_{j=0}^ne^{input[j]} }}=-input[target]+\\log{(\\sum_{j=0}^ne^{input[j]})}<br>$$</p>\n<p>好，安装上面我们的推导来运行一下程序：</p>\n<p><img src=\"/images/pasted-12.png\" alt=\"upload successful\"></p>\n<p>破发科特<del>~</del>~<br>开学快乐(<em>^__^</em>) ……</p>\n"},{"title":"Pytorch中的自动求导函数backward()所需参数的含义","author":"Jeason","date":"2019-02-01T11:53:00.000Z","_content":"正常来说backward( )函数是要传入参数的，一直没弄明白backward需要传入的参数具体含义，但是没关系，生命在与折腾，咱们来折腾一下，嘿嘿。\n\n# 对标量自动求导\n\n首先，如果out.backward()中的out是一个标量的话（相当于一个神经网络有一个样本，这个样本有两个属性，神经网络有一个输出）那么此时我的backward函数是不需要输入任何参数的。\n```python\nimport torch\nfrom torch.autograd import Variable\n    \na = Variable(torch.Tensor([2,3]),requires_grad=True)\nb = a + 3\nc = b * 3\nout = c.mean()\nout.backward()\nprint('input:')\nprint(a.data)\nprint('output:')\nprint(out.data.item())\nprint('input gradients are:')\nprint(a.grad)\n```\n运行结果：\n![upload successful](/images/pasted-3.png)\n不难看出，我们构建了这样的一个函数：\n$$out=3[(a_1+3)+(a_2+3)]\\over2$$\n所以其求导也很容易看出：\n$${\\partial out \\over \\partial a_1}={\\partial out \\over \\partial a_2 }={3\\over2}  $$\n这是对其进行标量自动求导的结果.\n# 对向量自动求导\n如果out.backward()中的out是一个向量（或者理解成1xN的矩阵）的话，我们对向量进行自动求导，看看会发生什么？\n\n先构建这样的一个模型（相当于一个神经网络有一个样本，这个样本有两个属性，神经网络有两个输出）：\n```python\nimport torch\nfrom torch.autograd import Variable\n    \na = Variable(torch.Tensor([[2.,4.]]),requires_grad=True)\nb = torch.zeros(1,2)\nb[0,0] = a[0,0] ** 2 \nb[0,1] = a[0,1] ** 3 \nout = 2 * b\n#其参数要传入和out维度一样的矩阵\nout.backward(torch.FloatTensor([[1.,1.]]))\nprint('input:')\nprint(a.data)\nprint('output:')\nprint(out.data)\nprint('input gradients are:')\nprint(a.grad)\n```\n模型也很简单，不难看出out求导出来的雅克比应该是：\n$$\\begin{pmatrix}\n{\\partial out_1 \\over \\partial a_1}=4a_1&{\\partial out_1 \\over \\partial a_2}=0\\\\\\\\\n{\\partial out_2 \\over \\partial a_1}=0&{\\partial out_2 \\over \\partial a_2}=6a_2^2\\\\\\\\\n\\end{pmatrix}$$\n\n因为a1 = 2，a2 = 4，所以上面的矩阵应该是:$\\begin{pmatrix}\n8&0\\\\\\\\\n0&96\\\\\\\\\n\\end{pmatrix}\n$\n运行的结果：\n![upload successful](/images/pasted-4.png)\n嗯，的确是8和96，但是仔细想一想，和咱们想要的雅克比矩阵的形式也不一样啊。难道是backward自动把0给省略了？\n\n咱们继续试试，这次在上一个模型的基础上进行小修改，如下：\n```python\nimport torch\nfrom torch.autograd import Variable\n    \na = Variable(torch.Tensor([[2.,4.]]),requires_grad=True)\nb = torch.zeros(1,2)\nb[0,0] = a[0,0] ** 2 + a[0,1] \nb[0,1] = a[0,1] ** 3 + a[0,0]\nout = 2 * b\n#其参数要传入和out维度一样的矩阵\nout.backward(torch.FloatTensor([[1.,1.]]))\nprint('input:')\nprint(a.data)\nprint('output:')\nprint(out.data)\nprint('input gradients are:')\nprint(a.grad)\n```\n可以看出这个模型的雅克比应该是：\n$$\\begin{pmatrix}\n{\\partial out_1 \\over \\partial a_1}=4a_1&{\\partial out_1 \\over \\partial a_2}=2\\\\\\\\\n{\\partial out_2 \\over \\partial a_1}=2&{\\partial out_2 \\over \\partial a_2}=6a_2^2\\\\\\\\\n\\end{pmatrix}$$\n运行一下：\n![upload successful](/images/pasted-5.png)\n等等，什么鬼？正常来说不应该是$\\begin{pmatrix}\n8&2\\\\\\\\\n2&96\\\\\\\\\n\\end{pmatrix}\n$么？\n我是谁？我再哪？为什么就给我2个数，而且是  8 + 2 = 10 ，96 + 2 = 98 。难道都是加的 2 ？\n想一想，刚才咱们backward中传的参数是 [ [ 1 , 1 ] ]，难道安装这个关系对应求和了？\n咱们换个参数来试一试，程序中只更改传入的参数为[ [ 1 , 2 ] ]：\n```python\nimport torch\nfrom torch.autograd import Variable\n    \na = Variable(torch.Tensor([[2.,4.]]),requires_grad=True)\nb = torch.zeros(1,2)\nb[0,0] = a[0,0] ** 2 + a[0,1] \nb[0,1] = a[0,1] ** 3 + a[0,0]\nout = 2 * b\n#其参数要传入和out维度一样的矩阵\nout.backward(torch.FloatTensor([[1.,2.]]))\nprint('input:')\nprint(a.data)\nprint('output:')\nprint(out.data)\nprint('input gradients are:')\nprint(a.grad)\n```\n![upload successful](/images/pasted-6.png)\n嗯，这回可以理解了，我们传入的参数，是对原来模型正常求导出来的雅克比矩阵进行线性操作，可以把我们传进的参数（设为arg）看成一个列向量，那么我们得到的结果就是:\n${({M_{Jacobo}}{M_{arg}})}^T$\n在这个题目中，我们得到的实际是：\n$$\\begin{cases}\n{1\\*{\\partial out_1 \\over \\partial a_1}}+{2\\*{\\partial out_1 \\over \\partial a_2}}=12\\\\\\\\\n{1\\*{\\partial out_2 \\over \\partial a_1}}+{2\\*{\\partial out_2 \\over \\partial a_2}}=12\\\\\\\\\n\\end{cases}\n$$\n看起来一切完美的解释了，但是就在我刚刚打字的一刻，我意识到官方文档中说k.backward()传入的参数应该和k具有相同的维度，所以如果按上述去解释是解释不通的。\n哪里出问题了呢？\n\n仔细看了一下，原来是这样的：在对雅克比矩阵进行线性操作的时候，应该把我们传进的参数（设为arg）看成一个行向量（不是列向量），那么我们得到的结果就是:${({M_{arg}}{M_{Jacobo}})}^T$\n也就是：\n$$\\begin{cases}\n{1\\*{\\partial out_1 \\over \\partial a_1}}+{2\\*{\\partial out_2 \\over \\partial a_1}}=12\\\\\\\\\n{1\\*{\\partial out_1 \\over \\partial a_2}}+{2\\*{\\partial out_2 \\over \\partial a_2}}=12\\\\\\\\\n\\end{cases}\n$$\n这回我们就解释的通了。\n\n现在我们来输出一下雅克比矩阵吧，为了不引起歧义，我们让雅克比矩阵的每个数值都不一样（一开始分析错了就是因为雅克比矩阵中有相同的数据），所以模型小改动如下：\n```python\nimport torch\nfrom torch.autograd import Variable\n    \na = Variable(torch.Tensor([[2.,4.]]),requires_grad=True)\nb = torch.zeros(1,2)\nb[0,0] = a[0,0] ** 2 + a[0,1] \nb[0,1] = a[0,1] ** 3 + a[0,0] * 2\nout = 2 * b\n#其参数要传入和out维度一样的矩阵\nout.backward(torch.FloatTensor([[1,0]]),retain_graph=True)\nA_temp = copy.deepcopy(a.grad)\na.grad.zero_()\nout.backward(torch.FloatTensor([[0,1]]))\nB_temp = a.grad\nprint('jacobian matrix is:')\nprint(torch.cat( (A_temp,B_temp),0 ))\n```\n如果没问题的话咱们的雅克比矩阵应该是 [ [ 8 , 2 ] , [ 4 , 96 ] ]\n\n好了，下面是见证奇迹的时刻了,不要眨眼睛奥，千万不要眨眼睛......\n3\n2\n1\n砰............\n![upload successful](/images/pasted-7.png)\n好了，现在总结一下：因为经过了复杂的神经网络之后，out中每个数值都是由很多输入样本的属性（也就是输入数据）线性或者非线性组合而成的，那么out中的每个数值和输入数据的每个数值都有关联，也就是说【out】中的每个数都可以对【a】中每个数求导，那么我们backward（）的参数[k1,k2,k3....kn]的含义就是：\n\n\n$$\\begin{cases}\n{\\partial out \\over \\partial a_1}={k_1\\*{\\partial out_1 \\over \\partial a_1}}+{k_2\\*{\\partial out_2 \\over \\partial a_1}}+{k_3\\*{\\partial out_3 \\over \\partial a_1}}+...+{k_n\\*{\\partial out_n \\over \\partial a_1}}\\\\\\\\\n{\\partial out \\over \\partial a_2}={k_1\\*{\\partial out_1 \\over \\partial a_2}}+{k_2\\*{\\partial out_2 \\over \\partial a_2}}+{k_3\\*{\\partial out_3 \\over \\partial a_2}}+...+{k_n\\*{\\partial out_n \\over \\partial a_2}}\\\\\\\\\n...\\\\\\\\\n{\\partial out \\over \\partial a_n}={k_1\\*{\\partial out_1 \\over \\partial a_n}}+{k_2\\*{\\partial out_2 \\over \\partial a_n}}+{k_3\\*{\\partial out_3 \\over \\partial a_n}}+...+{k_n\\*{\\partial out_n \\over \\partial a_n}}\\\\\\\\\n\\end{cases}\n$$\n也可以理解成每个out分量对an求导时的权重。\n\n# 对矩阵自动求导\n现在，如果out是一个矩阵呢？\n下面的例子也可以理解为：相当于一个神经网络有两个样本，每个样本有两个属性，神经网络有两个输出。\n```python\nimport torch\nfrom torch.autograd import Variable\nfrom torch import nn\n\na = Variable(torch.FloatTensor([[2,3],[1,2]]),requires_grad=True)\nw = Variable( torch.zeros(2,1),requires_grad=True )\nout = torch.mm(a,w)\nout.backward(torch.FloatTensor([[1.],[1.]]),retain_graph=True)\nprint(\"gradients are:{}\".format(w.grad.data))\n```\n如果前面的例子理解了，那么这个也很好理解，backward输入的参数k是一个2x1的矩阵，2代表的就是样本数量，就是在前面的基础上，再对每个样本进行加权求和。\n结果是：\n\n![upload successful](/images/pasted-8.png)\n\n如果有兴趣，也可以拓展一下多个样本的多分类问题，猜一下k的维度应该是【输入样本的个数 X 分类的个数】\n\n好啦，纠结我好久的pytorch自动求导原理算是彻底搞懂啦~~~\n\n","source":"_posts/今天我发布了新的博客.md","raw":"title: Pytorch中的自动求导函数backward()所需参数的含义\ntags: \n- 深度学习\n- Pytorch\ncategories: \n- 深度学习\nauthor: Jeason\ndate: 2019-02-01 19:53:00\n---\n正常来说backward( )函数是要传入参数的，一直没弄明白backward需要传入的参数具体含义，但是没关系，生命在与折腾，咱们来折腾一下，嘿嘿。\n\n# 对标量自动求导\n\n首先，如果out.backward()中的out是一个标量的话（相当于一个神经网络有一个样本，这个样本有两个属性，神经网络有一个输出）那么此时我的backward函数是不需要输入任何参数的。\n```python\nimport torch\nfrom torch.autograd import Variable\n    \na = Variable(torch.Tensor([2,3]),requires_grad=True)\nb = a + 3\nc = b * 3\nout = c.mean()\nout.backward()\nprint('input:')\nprint(a.data)\nprint('output:')\nprint(out.data.item())\nprint('input gradients are:')\nprint(a.grad)\n```\n运行结果：\n![upload successful](/images/pasted-3.png)\n不难看出，我们构建了这样的一个函数：\n$$out=3[(a_1+3)+(a_2+3)]\\over2$$\n所以其求导也很容易看出：\n$${\\partial out \\over \\partial a_1}={\\partial out \\over \\partial a_2 }={3\\over2}  $$\n这是对其进行标量自动求导的结果.\n# 对向量自动求导\n如果out.backward()中的out是一个向量（或者理解成1xN的矩阵）的话，我们对向量进行自动求导，看看会发生什么？\n\n先构建这样的一个模型（相当于一个神经网络有一个样本，这个样本有两个属性，神经网络有两个输出）：\n```python\nimport torch\nfrom torch.autograd import Variable\n    \na = Variable(torch.Tensor([[2.,4.]]),requires_grad=True)\nb = torch.zeros(1,2)\nb[0,0] = a[0,0] ** 2 \nb[0,1] = a[0,1] ** 3 \nout = 2 * b\n#其参数要传入和out维度一样的矩阵\nout.backward(torch.FloatTensor([[1.,1.]]))\nprint('input:')\nprint(a.data)\nprint('output:')\nprint(out.data)\nprint('input gradients are:')\nprint(a.grad)\n```\n模型也很简单，不难看出out求导出来的雅克比应该是：\n$$\\begin{pmatrix}\n{\\partial out_1 \\over \\partial a_1}=4a_1&{\\partial out_1 \\over \\partial a_2}=0\\\\\\\\\n{\\partial out_2 \\over \\partial a_1}=0&{\\partial out_2 \\over \\partial a_2}=6a_2^2\\\\\\\\\n\\end{pmatrix}$$\n\n因为a1 = 2，a2 = 4，所以上面的矩阵应该是:$\\begin{pmatrix}\n8&0\\\\\\\\\n0&96\\\\\\\\\n\\end{pmatrix}\n$\n运行的结果：\n![upload successful](/images/pasted-4.png)\n嗯，的确是8和96，但是仔细想一想，和咱们想要的雅克比矩阵的形式也不一样啊。难道是backward自动把0给省略了？\n\n咱们继续试试，这次在上一个模型的基础上进行小修改，如下：\n```python\nimport torch\nfrom torch.autograd import Variable\n    \na = Variable(torch.Tensor([[2.,4.]]),requires_grad=True)\nb = torch.zeros(1,2)\nb[0,0] = a[0,0] ** 2 + a[0,1] \nb[0,1] = a[0,1] ** 3 + a[0,0]\nout = 2 * b\n#其参数要传入和out维度一样的矩阵\nout.backward(torch.FloatTensor([[1.,1.]]))\nprint('input:')\nprint(a.data)\nprint('output:')\nprint(out.data)\nprint('input gradients are:')\nprint(a.grad)\n```\n可以看出这个模型的雅克比应该是：\n$$\\begin{pmatrix}\n{\\partial out_1 \\over \\partial a_1}=4a_1&{\\partial out_1 \\over \\partial a_2}=2\\\\\\\\\n{\\partial out_2 \\over \\partial a_1}=2&{\\partial out_2 \\over \\partial a_2}=6a_2^2\\\\\\\\\n\\end{pmatrix}$$\n运行一下：\n![upload successful](/images/pasted-5.png)\n等等，什么鬼？正常来说不应该是$\\begin{pmatrix}\n8&2\\\\\\\\\n2&96\\\\\\\\\n\\end{pmatrix}\n$么？\n我是谁？我再哪？为什么就给我2个数，而且是  8 + 2 = 10 ，96 + 2 = 98 。难道都是加的 2 ？\n想一想，刚才咱们backward中传的参数是 [ [ 1 , 1 ] ]，难道安装这个关系对应求和了？\n咱们换个参数来试一试，程序中只更改传入的参数为[ [ 1 , 2 ] ]：\n```python\nimport torch\nfrom torch.autograd import Variable\n    \na = Variable(torch.Tensor([[2.,4.]]),requires_grad=True)\nb = torch.zeros(1,2)\nb[0,0] = a[0,0] ** 2 + a[0,1] \nb[0,1] = a[0,1] ** 3 + a[0,0]\nout = 2 * b\n#其参数要传入和out维度一样的矩阵\nout.backward(torch.FloatTensor([[1.,2.]]))\nprint('input:')\nprint(a.data)\nprint('output:')\nprint(out.data)\nprint('input gradients are:')\nprint(a.grad)\n```\n![upload successful](/images/pasted-6.png)\n嗯，这回可以理解了，我们传入的参数，是对原来模型正常求导出来的雅克比矩阵进行线性操作，可以把我们传进的参数（设为arg）看成一个列向量，那么我们得到的结果就是:\n${({M_{Jacobo}}{M_{arg}})}^T$\n在这个题目中，我们得到的实际是：\n$$\\begin{cases}\n{1\\*{\\partial out_1 \\over \\partial a_1}}+{2\\*{\\partial out_1 \\over \\partial a_2}}=12\\\\\\\\\n{1\\*{\\partial out_2 \\over \\partial a_1}}+{2\\*{\\partial out_2 \\over \\partial a_2}}=12\\\\\\\\\n\\end{cases}\n$$\n看起来一切完美的解释了，但是就在我刚刚打字的一刻，我意识到官方文档中说k.backward()传入的参数应该和k具有相同的维度，所以如果按上述去解释是解释不通的。\n哪里出问题了呢？\n\n仔细看了一下，原来是这样的：在对雅克比矩阵进行线性操作的时候，应该把我们传进的参数（设为arg）看成一个行向量（不是列向量），那么我们得到的结果就是:${({M_{arg}}{M_{Jacobo}})}^T$\n也就是：\n$$\\begin{cases}\n{1\\*{\\partial out_1 \\over \\partial a_1}}+{2\\*{\\partial out_2 \\over \\partial a_1}}=12\\\\\\\\\n{1\\*{\\partial out_1 \\over \\partial a_2}}+{2\\*{\\partial out_2 \\over \\partial a_2}}=12\\\\\\\\\n\\end{cases}\n$$\n这回我们就解释的通了。\n\n现在我们来输出一下雅克比矩阵吧，为了不引起歧义，我们让雅克比矩阵的每个数值都不一样（一开始分析错了就是因为雅克比矩阵中有相同的数据），所以模型小改动如下：\n```python\nimport torch\nfrom torch.autograd import Variable\n    \na = Variable(torch.Tensor([[2.,4.]]),requires_grad=True)\nb = torch.zeros(1,2)\nb[0,0] = a[0,0] ** 2 + a[0,1] \nb[0,1] = a[0,1] ** 3 + a[0,0] * 2\nout = 2 * b\n#其参数要传入和out维度一样的矩阵\nout.backward(torch.FloatTensor([[1,0]]),retain_graph=True)\nA_temp = copy.deepcopy(a.grad)\na.grad.zero_()\nout.backward(torch.FloatTensor([[0,1]]))\nB_temp = a.grad\nprint('jacobian matrix is:')\nprint(torch.cat( (A_temp,B_temp),0 ))\n```\n如果没问题的话咱们的雅克比矩阵应该是 [ [ 8 , 2 ] , [ 4 , 96 ] ]\n\n好了，下面是见证奇迹的时刻了,不要眨眼睛奥，千万不要眨眼睛......\n3\n2\n1\n砰............\n![upload successful](/images/pasted-7.png)\n好了，现在总结一下：因为经过了复杂的神经网络之后，out中每个数值都是由很多输入样本的属性（也就是输入数据）线性或者非线性组合而成的，那么out中的每个数值和输入数据的每个数值都有关联，也就是说【out】中的每个数都可以对【a】中每个数求导，那么我们backward（）的参数[k1,k2,k3....kn]的含义就是：\n\n\n$$\\begin{cases}\n{\\partial out \\over \\partial a_1}={k_1\\*{\\partial out_1 \\over \\partial a_1}}+{k_2\\*{\\partial out_2 \\over \\partial a_1}}+{k_3\\*{\\partial out_3 \\over \\partial a_1}}+...+{k_n\\*{\\partial out_n \\over \\partial a_1}}\\\\\\\\\n{\\partial out \\over \\partial a_2}={k_1\\*{\\partial out_1 \\over \\partial a_2}}+{k_2\\*{\\partial out_2 \\over \\partial a_2}}+{k_3\\*{\\partial out_3 \\over \\partial a_2}}+...+{k_n\\*{\\partial out_n \\over \\partial a_2}}\\\\\\\\\n...\\\\\\\\\n{\\partial out \\over \\partial a_n}={k_1\\*{\\partial out_1 \\over \\partial a_n}}+{k_2\\*{\\partial out_2 \\over \\partial a_n}}+{k_3\\*{\\partial out_3 \\over \\partial a_n}}+...+{k_n\\*{\\partial out_n \\over \\partial a_n}}\\\\\\\\\n\\end{cases}\n$$\n也可以理解成每个out分量对an求导时的权重。\n\n# 对矩阵自动求导\n现在，如果out是一个矩阵呢？\n下面的例子也可以理解为：相当于一个神经网络有两个样本，每个样本有两个属性，神经网络有两个输出。\n```python\nimport torch\nfrom torch.autograd import Variable\nfrom torch import nn\n\na = Variable(torch.FloatTensor([[2,3],[1,2]]),requires_grad=True)\nw = Variable( torch.zeros(2,1),requires_grad=True )\nout = torch.mm(a,w)\nout.backward(torch.FloatTensor([[1.],[1.]]),retain_graph=True)\nprint(\"gradients are:{}\".format(w.grad.data))\n```\n如果前面的例子理解了，那么这个也很好理解，backward输入的参数k是一个2x1的矩阵，2代表的就是样本数量，就是在前面的基础上，再对每个样本进行加权求和。\n结果是：\n\n![upload successful](/images/pasted-8.png)\n\n如果有兴趣，也可以拓展一下多个样本的多分类问题，猜一下k的维度应该是【输入样本的个数 X 分类的个数】\n\n好啦，纠结我好久的pytorch自动求导原理算是彻底搞懂啦~~~\n\n","slug":"今天我发布了新的博客","published":1,"updated":"2019-02-05T10:03:52.172Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjt8bm3um00099gojurz9ha6k","content":"<p>正常来说backward( )函数是要传入参数的，一直没弄明白backward需要传入的参数具体含义，但是没关系，生命在与折腾，咱们来折腾一下，嘿嘿。</p>\n<h1 id=\"对标量自动求导\"><a href=\"#对标量自动求导\" class=\"headerlink\" title=\"对标量自动求导\"></a>对标量自动求导</h1><p>首先，如果out.backward()中的out是一个标量的话（相当于一个神经网络有一个样本，这个样本有两个属性，神经网络有一个输出）那么此时我的backward函数是不需要输入任何参数的。<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.autograd <span class=\"keyword\">import</span> Variable</span><br><span class=\"line\">    </span><br><span class=\"line\">a = Variable(torch.Tensor([<span class=\"number\">2</span>,<span class=\"number\">3</span>]),requires_grad=<span class=\"keyword\">True</span>)</span><br><span class=\"line\">b = a + <span class=\"number\">3</span></span><br><span class=\"line\">c = b * <span class=\"number\">3</span></span><br><span class=\"line\">out = c.mean()</span><br><span class=\"line\">out.backward()</span><br><span class=\"line\">print(<span class=\"string\">'input:'</span>)</span><br><span class=\"line\">print(a.data)</span><br><span class=\"line\">print(<span class=\"string\">'output:'</span>)</span><br><span class=\"line\">print(out.data.item())</span><br><span class=\"line\">print(<span class=\"string\">'input gradients are:'</span>)</span><br><span class=\"line\">print(a.grad)</span><br></pre></td></tr></table></figure></p>\n<p>运行结果：<br><img src=\"/images/pasted-3.png\" alt=\"upload successful\"><br>不难看出，我们构建了这样的一个函数：<br>$$out=3[(a_1+3)+(a_2+3)]\\over2$$<br>所以其求导也很容易看出：<br>$${\\partial out \\over \\partial a_1}={\\partial out \\over \\partial a_2 }={3\\over2}  $$<br>这是对其进行标量自动求导的结果.</p>\n<h1 id=\"对向量自动求导\"><a href=\"#对向量自动求导\" class=\"headerlink\" title=\"对向量自动求导\"></a>对向量自动求导</h1><p>如果out.backward()中的out是一个向量（或者理解成1xN的矩阵）的话，我们对向量进行自动求导，看看会发生什么？</p>\n<p>先构建这样的一个模型（相当于一个神经网络有一个样本，这个样本有两个属性，神经网络有两个输出）：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.autograd <span class=\"keyword\">import</span> Variable</span><br><span class=\"line\">    </span><br><span class=\"line\">a = Variable(torch.Tensor([[<span class=\"number\">2.</span>,<span class=\"number\">4.</span>]]),requires_grad=<span class=\"keyword\">True</span>)</span><br><span class=\"line\">b = torch.zeros(<span class=\"number\">1</span>,<span class=\"number\">2</span>)</span><br><span class=\"line\">b[<span class=\"number\">0</span>,<span class=\"number\">0</span>] = a[<span class=\"number\">0</span>,<span class=\"number\">0</span>] ** <span class=\"number\">2</span> </span><br><span class=\"line\">b[<span class=\"number\">0</span>,<span class=\"number\">1</span>] = a[<span class=\"number\">0</span>,<span class=\"number\">1</span>] ** <span class=\"number\">3</span> </span><br><span class=\"line\">out = <span class=\"number\">2</span> * b</span><br><span class=\"line\"><span class=\"comment\">#其参数要传入和out维度一样的矩阵</span></span><br><span class=\"line\">out.backward(torch.FloatTensor([[<span class=\"number\">1.</span>,<span class=\"number\">1.</span>]]))</span><br><span class=\"line\">print(<span class=\"string\">'input:'</span>)</span><br><span class=\"line\">print(a.data)</span><br><span class=\"line\">print(<span class=\"string\">'output:'</span>)</span><br><span class=\"line\">print(out.data)</span><br><span class=\"line\">print(<span class=\"string\">'input gradients are:'</span>)</span><br><span class=\"line\">print(a.grad)</span><br></pre></td></tr></table></figure></p>\n<p>模型也很简单，不难看出out求导出来的雅克比应该是：<br>$$\\begin{pmatrix}<br>{\\partial out_1 \\over \\partial a_1}=4a_1&amp;{\\partial out_1 \\over \\partial a_2}=0\\\\<br>{\\partial out_2 \\over \\partial a_1}=0&amp;{\\partial out_2 \\over \\partial a_2}=6a_2^2\\\\<br>\\end{pmatrix}$$</p>\n<p>因为a1 = 2，a2 = 4，所以上面的矩阵应该是:$\\begin{pmatrix}<br>8&amp;0\\\\<br>0&amp;96\\\\<br>\\end{pmatrix}<br>$<br>运行的结果：<br><img src=\"/images/pasted-4.png\" alt=\"upload successful\"><br>嗯，的确是8和96，但是仔细想一想，和咱们想要的雅克比矩阵的形式也不一样啊。难道是backward自动把0给省略了？</p>\n<p>咱们继续试试，这次在上一个模型的基础上进行小修改，如下：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.autograd <span class=\"keyword\">import</span> Variable</span><br><span class=\"line\">    </span><br><span class=\"line\">a = Variable(torch.Tensor([[<span class=\"number\">2.</span>,<span class=\"number\">4.</span>]]),requires_grad=<span class=\"keyword\">True</span>)</span><br><span class=\"line\">b = torch.zeros(<span class=\"number\">1</span>,<span class=\"number\">2</span>)</span><br><span class=\"line\">b[<span class=\"number\">0</span>,<span class=\"number\">0</span>] = a[<span class=\"number\">0</span>,<span class=\"number\">0</span>] ** <span class=\"number\">2</span> + a[<span class=\"number\">0</span>,<span class=\"number\">1</span>] </span><br><span class=\"line\">b[<span class=\"number\">0</span>,<span class=\"number\">1</span>] = a[<span class=\"number\">0</span>,<span class=\"number\">1</span>] ** <span class=\"number\">3</span> + a[<span class=\"number\">0</span>,<span class=\"number\">0</span>]</span><br><span class=\"line\">out = <span class=\"number\">2</span> * b</span><br><span class=\"line\"><span class=\"comment\">#其参数要传入和out维度一样的矩阵</span></span><br><span class=\"line\">out.backward(torch.FloatTensor([[<span class=\"number\">1.</span>,<span class=\"number\">1.</span>]]))</span><br><span class=\"line\">print(<span class=\"string\">'input:'</span>)</span><br><span class=\"line\">print(a.data)</span><br><span class=\"line\">print(<span class=\"string\">'output:'</span>)</span><br><span class=\"line\">print(out.data)</span><br><span class=\"line\">print(<span class=\"string\">'input gradients are:'</span>)</span><br><span class=\"line\">print(a.grad)</span><br></pre></td></tr></table></figure></p>\n<p>可以看出这个模型的雅克比应该是：<br>$$\\begin{pmatrix}<br>{\\partial out_1 \\over \\partial a_1}=4a_1&amp;{\\partial out_1 \\over \\partial a_2}=2\\\\<br>{\\partial out_2 \\over \\partial a_1}=2&amp;{\\partial out_2 \\over \\partial a_2}=6a_2^2\\\\<br>\\end{pmatrix}$$<br>运行一下：<br><img src=\"/images/pasted-5.png\" alt=\"upload successful\"><br>等等，什么鬼？正常来说不应该是$\\begin{pmatrix}<br>8&amp;2\\\\<br>2&amp;96\\\\<br>\\end{pmatrix}<br>$么？<br>我是谁？我再哪？为什么就给我2个数，而且是  8 + 2 = 10 ，96 + 2 = 98 。难道都是加的 2 ？<br>想一想，刚才咱们backward中传的参数是 [ [ 1 , 1 ] ]，难道安装这个关系对应求和了？<br>咱们换个参数来试一试，程序中只更改传入的参数为[ [ 1 , 2 ] ]：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.autograd <span class=\"keyword\">import</span> Variable</span><br><span class=\"line\">    </span><br><span class=\"line\">a = Variable(torch.Tensor([[<span class=\"number\">2.</span>,<span class=\"number\">4.</span>]]),requires_grad=<span class=\"keyword\">True</span>)</span><br><span class=\"line\">b = torch.zeros(<span class=\"number\">1</span>,<span class=\"number\">2</span>)</span><br><span class=\"line\">b[<span class=\"number\">0</span>,<span class=\"number\">0</span>] = a[<span class=\"number\">0</span>,<span class=\"number\">0</span>] ** <span class=\"number\">2</span> + a[<span class=\"number\">0</span>,<span class=\"number\">1</span>] </span><br><span class=\"line\">b[<span class=\"number\">0</span>,<span class=\"number\">1</span>] = a[<span class=\"number\">0</span>,<span class=\"number\">1</span>] ** <span class=\"number\">3</span> + a[<span class=\"number\">0</span>,<span class=\"number\">0</span>]</span><br><span class=\"line\">out = <span class=\"number\">2</span> * b</span><br><span class=\"line\"><span class=\"comment\">#其参数要传入和out维度一样的矩阵</span></span><br><span class=\"line\">out.backward(torch.FloatTensor([[<span class=\"number\">1.</span>,<span class=\"number\">2.</span>]]))</span><br><span class=\"line\">print(<span class=\"string\">'input:'</span>)</span><br><span class=\"line\">print(a.data)</span><br><span class=\"line\">print(<span class=\"string\">'output:'</span>)</span><br><span class=\"line\">print(out.data)</span><br><span class=\"line\">print(<span class=\"string\">'input gradients are:'</span>)</span><br><span class=\"line\">print(a.grad)</span><br></pre></td></tr></table></figure></p>\n<p><img src=\"/images/pasted-6.png\" alt=\"upload successful\"><br>嗯，这回可以理解了，我们传入的参数，是对原来模型正常求导出来的雅克比矩阵进行线性操作，可以把我们传进的参数（设为arg）看成一个列向量，那么我们得到的结果就是:<br>${({M_{Jacobo}}{M_{arg}})}^T$<br>在这个题目中，我们得到的实际是：<br>$$\\begin{cases}<br>{1*{\\partial out_1 \\over \\partial a_1}}+{2*{\\partial out_1 \\over \\partial a_2}}=12\\\\<br>{1*{\\partial out_2 \\over \\partial a_1}}+{2*{\\partial out_2 \\over \\partial a_2}}=12\\\\<br>\\end{cases}<br>$$<br>看起来一切完美的解释了，但是就在我刚刚打字的一刻，我意识到官方文档中说k.backward()传入的参数应该和k具有相同的维度，所以如果按上述去解释是解释不通的。<br>哪里出问题了呢？</p>\n<p>仔细看了一下，原来是这样的：在对雅克比矩阵进行线性操作的时候，应该把我们传进的参数（设为arg）看成一个行向量（不是列向量），那么我们得到的结果就是:${({M_{arg}}{M_{Jacobo}})}^T$<br>也就是：<br>$$\\begin{cases}<br>{1*{\\partial out_1 \\over \\partial a_1}}+{2*{\\partial out_2 \\over \\partial a_1}}=12\\\\<br>{1*{\\partial out_1 \\over \\partial a_2}}+{2*{\\partial out_2 \\over \\partial a_2}}=12\\\\<br>\\end{cases}<br>$$<br>这回我们就解释的通了。</p>\n<p>现在我们来输出一下雅克比矩阵吧，为了不引起歧义，我们让雅克比矩阵的每个数值都不一样（一开始分析错了就是因为雅克比矩阵中有相同的数据），所以模型小改动如下：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.autograd <span class=\"keyword\">import</span> Variable</span><br><span class=\"line\">    </span><br><span class=\"line\">a = Variable(torch.Tensor([[<span class=\"number\">2.</span>,<span class=\"number\">4.</span>]]),requires_grad=<span class=\"keyword\">True</span>)</span><br><span class=\"line\">b = torch.zeros(<span class=\"number\">1</span>,<span class=\"number\">2</span>)</span><br><span class=\"line\">b[<span class=\"number\">0</span>,<span class=\"number\">0</span>] = a[<span class=\"number\">0</span>,<span class=\"number\">0</span>] ** <span class=\"number\">2</span> + a[<span class=\"number\">0</span>,<span class=\"number\">1</span>] </span><br><span class=\"line\">b[<span class=\"number\">0</span>,<span class=\"number\">1</span>] = a[<span class=\"number\">0</span>,<span class=\"number\">1</span>] ** <span class=\"number\">3</span> + a[<span class=\"number\">0</span>,<span class=\"number\">0</span>] * <span class=\"number\">2</span></span><br><span class=\"line\">out = <span class=\"number\">2</span> * b</span><br><span class=\"line\"><span class=\"comment\">#其参数要传入和out维度一样的矩阵</span></span><br><span class=\"line\">out.backward(torch.FloatTensor([[<span class=\"number\">1</span>,<span class=\"number\">0</span>]]),retain_graph=<span class=\"keyword\">True</span>)</span><br><span class=\"line\">A_temp = copy.deepcopy(a.grad)</span><br><span class=\"line\">a.grad.zero_()</span><br><span class=\"line\">out.backward(torch.FloatTensor([[<span class=\"number\">0</span>,<span class=\"number\">1</span>]]))</span><br><span class=\"line\">B_temp = a.grad</span><br><span class=\"line\">print(<span class=\"string\">'jacobian matrix is:'</span>)</span><br><span class=\"line\">print(torch.cat( (A_temp,B_temp),<span class=\"number\">0</span> ))</span><br></pre></td></tr></table></figure></p>\n<p>如果没问题的话咱们的雅克比矩阵应该是 [ [ 8 , 2 ] , [ 4 , 96 ] ]</p>\n<p>好了，下面是见证奇迹的时刻了,不要眨眼睛奥，千万不要眨眼睛……<br>3<br>2<br>1<br>砰…………<br><img src=\"/images/pasted-7.png\" alt=\"upload successful\"><br>好了，现在总结一下：因为经过了复杂的神经网络之后，out中每个数值都是由很多输入样本的属性（也就是输入数据）线性或者非线性组合而成的，那么out中的每个数值和输入数据的每个数值都有关联，也就是说【out】中的每个数都可以对【a】中每个数求导，那么我们backward（）的参数[k1,k2,k3….kn]的含义就是：</p>\n<p>$$\\begin{cases}<br>{\\partial out \\over \\partial a_1}={k_1*{\\partial out_1 \\over \\partial a_1}}+{k_2*{\\partial out_2 \\over \\partial a_1}}+{k_3*{\\partial out_3 \\over \\partial a_1}}+…+{k_n*{\\partial out_n \\over \\partial a_1}}\\\\<br>{\\partial out \\over \\partial a_2}={k_1*{\\partial out_1 \\over \\partial a_2}}+{k_2*{\\partial out_2 \\over \\partial a_2}}+{k_3*{\\partial out_3 \\over \\partial a_2}}+…+{k_n*{\\partial out_n \\over \\partial a_2}}\\\\<br>…\\\\<br>{\\partial out \\over \\partial a_n}={k_1*{\\partial out_1 \\over \\partial a_n}}+{k_2*{\\partial out_2 \\over \\partial a_n}}+{k_3*{\\partial out_3 \\over \\partial a_n}}+…+{k_n*{\\partial out_n \\over \\partial a_n}}\\\\<br>\\end{cases}<br>$$<br>也可以理解成每个out分量对an求导时的权重。</p>\n<h1 id=\"对矩阵自动求导\"><a href=\"#对矩阵自动求导\" class=\"headerlink\" title=\"对矩阵自动求导\"></a>对矩阵自动求导</h1><p>现在，如果out是一个矩阵呢？<br>下面的例子也可以理解为：相当于一个神经网络有两个样本，每个样本有两个属性，神经网络有两个输出。<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.autograd <span class=\"keyword\">import</span> Variable</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"></span><br><span class=\"line\">a = Variable(torch.FloatTensor([[<span class=\"number\">2</span>,<span class=\"number\">3</span>],[<span class=\"number\">1</span>,<span class=\"number\">2</span>]]),requires_grad=<span class=\"keyword\">True</span>)</span><br><span class=\"line\">w = Variable( torch.zeros(<span class=\"number\">2</span>,<span class=\"number\">1</span>),requires_grad=<span class=\"keyword\">True</span> )</span><br><span class=\"line\">out = torch.mm(a,w)</span><br><span class=\"line\">out.backward(torch.FloatTensor([[<span class=\"number\">1.</span>],[<span class=\"number\">1.</span>]]),retain_graph=<span class=\"keyword\">True</span>)</span><br><span class=\"line\">print(<span class=\"string\">\"gradients are:&#123;&#125;\"</span>.format(w.grad.data))</span><br></pre></td></tr></table></figure></p>\n<p>如果前面的例子理解了，那么这个也很好理解，backward输入的参数k是一个2x1的矩阵，2代表的就是样本数量，就是在前面的基础上，再对每个样本进行加权求和。<br>结果是：</p>\n<p><img src=\"/images/pasted-8.png\" alt=\"upload successful\"></p>\n<p>如果有兴趣，也可以拓展一下多个样本的多分类问题，猜一下k的维度应该是【输入样本的个数 X 分类的个数】</p>\n<p>好啦，纠结我好久的pytorch自动求导原理算是彻底搞懂啦~~~</p>\n","site":{"data":{}},"excerpt":"","more":"<p>正常来说backward( )函数是要传入参数的，一直没弄明白backward需要传入的参数具体含义，但是没关系，生命在与折腾，咱们来折腾一下，嘿嘿。</p>\n<h1 id=\"对标量自动求导\"><a href=\"#对标量自动求导\" class=\"headerlink\" title=\"对标量自动求导\"></a>对标量自动求导</h1><p>首先，如果out.backward()中的out是一个标量的话（相当于一个神经网络有一个样本，这个样本有两个属性，神经网络有一个输出）那么此时我的backward函数是不需要输入任何参数的。<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.autograd <span class=\"keyword\">import</span> Variable</span><br><span class=\"line\">    </span><br><span class=\"line\">a = Variable(torch.Tensor([<span class=\"number\">2</span>,<span class=\"number\">3</span>]),requires_grad=<span class=\"keyword\">True</span>)</span><br><span class=\"line\">b = a + <span class=\"number\">3</span></span><br><span class=\"line\">c = b * <span class=\"number\">3</span></span><br><span class=\"line\">out = c.mean()</span><br><span class=\"line\">out.backward()</span><br><span class=\"line\">print(<span class=\"string\">'input:'</span>)</span><br><span class=\"line\">print(a.data)</span><br><span class=\"line\">print(<span class=\"string\">'output:'</span>)</span><br><span class=\"line\">print(out.data.item())</span><br><span class=\"line\">print(<span class=\"string\">'input gradients are:'</span>)</span><br><span class=\"line\">print(a.grad)</span><br></pre></td></tr></table></figure></p>\n<p>运行结果：<br><img src=\"/images/pasted-3.png\" alt=\"upload successful\"><br>不难看出，我们构建了这样的一个函数：<br>$$out=3[(a_1+3)+(a_2+3)]\\over2$$<br>所以其求导也很容易看出：<br>$${\\partial out \\over \\partial a_1}={\\partial out \\over \\partial a_2 }={3\\over2}  $$<br>这是对其进行标量自动求导的结果.</p>\n<h1 id=\"对向量自动求导\"><a href=\"#对向量自动求导\" class=\"headerlink\" title=\"对向量自动求导\"></a>对向量自动求导</h1><p>如果out.backward()中的out是一个向量（或者理解成1xN的矩阵）的话，我们对向量进行自动求导，看看会发生什么？</p>\n<p>先构建这样的一个模型（相当于一个神经网络有一个样本，这个样本有两个属性，神经网络有两个输出）：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.autograd <span class=\"keyword\">import</span> Variable</span><br><span class=\"line\">    </span><br><span class=\"line\">a = Variable(torch.Tensor([[<span class=\"number\">2.</span>,<span class=\"number\">4.</span>]]),requires_grad=<span class=\"keyword\">True</span>)</span><br><span class=\"line\">b = torch.zeros(<span class=\"number\">1</span>,<span class=\"number\">2</span>)</span><br><span class=\"line\">b[<span class=\"number\">0</span>,<span class=\"number\">0</span>] = a[<span class=\"number\">0</span>,<span class=\"number\">0</span>] ** <span class=\"number\">2</span> </span><br><span class=\"line\">b[<span class=\"number\">0</span>,<span class=\"number\">1</span>] = a[<span class=\"number\">0</span>,<span class=\"number\">1</span>] ** <span class=\"number\">3</span> </span><br><span class=\"line\">out = <span class=\"number\">2</span> * b</span><br><span class=\"line\"><span class=\"comment\">#其参数要传入和out维度一样的矩阵</span></span><br><span class=\"line\">out.backward(torch.FloatTensor([[<span class=\"number\">1.</span>,<span class=\"number\">1.</span>]]))</span><br><span class=\"line\">print(<span class=\"string\">'input:'</span>)</span><br><span class=\"line\">print(a.data)</span><br><span class=\"line\">print(<span class=\"string\">'output:'</span>)</span><br><span class=\"line\">print(out.data)</span><br><span class=\"line\">print(<span class=\"string\">'input gradients are:'</span>)</span><br><span class=\"line\">print(a.grad)</span><br></pre></td></tr></table></figure></p>\n<p>模型也很简单，不难看出out求导出来的雅克比应该是：<br>$$\\begin{pmatrix}<br>{\\partial out_1 \\over \\partial a_1}=4a_1&amp;{\\partial out_1 \\over \\partial a_2}=0\\\\<br>{\\partial out_2 \\over \\partial a_1}=0&amp;{\\partial out_2 \\over \\partial a_2}=6a_2^2\\\\<br>\\end{pmatrix}$$</p>\n<p>因为a1 = 2，a2 = 4，所以上面的矩阵应该是:$\\begin{pmatrix}<br>8&amp;0\\\\<br>0&amp;96\\\\<br>\\end{pmatrix}<br>$<br>运行的结果：<br><img src=\"/images/pasted-4.png\" alt=\"upload successful\"><br>嗯，的确是8和96，但是仔细想一想，和咱们想要的雅克比矩阵的形式也不一样啊。难道是backward自动把0给省略了？</p>\n<p>咱们继续试试，这次在上一个模型的基础上进行小修改，如下：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.autograd <span class=\"keyword\">import</span> Variable</span><br><span class=\"line\">    </span><br><span class=\"line\">a = Variable(torch.Tensor([[<span class=\"number\">2.</span>,<span class=\"number\">4.</span>]]),requires_grad=<span class=\"keyword\">True</span>)</span><br><span class=\"line\">b = torch.zeros(<span class=\"number\">1</span>,<span class=\"number\">2</span>)</span><br><span class=\"line\">b[<span class=\"number\">0</span>,<span class=\"number\">0</span>] = a[<span class=\"number\">0</span>,<span class=\"number\">0</span>] ** <span class=\"number\">2</span> + a[<span class=\"number\">0</span>,<span class=\"number\">1</span>] </span><br><span class=\"line\">b[<span class=\"number\">0</span>,<span class=\"number\">1</span>] = a[<span class=\"number\">0</span>,<span class=\"number\">1</span>] ** <span class=\"number\">3</span> + a[<span class=\"number\">0</span>,<span class=\"number\">0</span>]</span><br><span class=\"line\">out = <span class=\"number\">2</span> * b</span><br><span class=\"line\"><span class=\"comment\">#其参数要传入和out维度一样的矩阵</span></span><br><span class=\"line\">out.backward(torch.FloatTensor([[<span class=\"number\">1.</span>,<span class=\"number\">1.</span>]]))</span><br><span class=\"line\">print(<span class=\"string\">'input:'</span>)</span><br><span class=\"line\">print(a.data)</span><br><span class=\"line\">print(<span class=\"string\">'output:'</span>)</span><br><span class=\"line\">print(out.data)</span><br><span class=\"line\">print(<span class=\"string\">'input gradients are:'</span>)</span><br><span class=\"line\">print(a.grad)</span><br></pre></td></tr></table></figure></p>\n<p>可以看出这个模型的雅克比应该是：<br>$$\\begin{pmatrix}<br>{\\partial out_1 \\over \\partial a_1}=4a_1&amp;{\\partial out_1 \\over \\partial a_2}=2\\\\<br>{\\partial out_2 \\over \\partial a_1}=2&amp;{\\partial out_2 \\over \\partial a_2}=6a_2^2\\\\<br>\\end{pmatrix}$$<br>运行一下：<br><img src=\"/images/pasted-5.png\" alt=\"upload successful\"><br>等等，什么鬼？正常来说不应该是$\\begin{pmatrix}<br>8&amp;2\\\\<br>2&amp;96\\\\<br>\\end{pmatrix}<br>$么？<br>我是谁？我再哪？为什么就给我2个数，而且是  8 + 2 = 10 ，96 + 2 = 98 。难道都是加的 2 ？<br>想一想，刚才咱们backward中传的参数是 [ [ 1 , 1 ] ]，难道安装这个关系对应求和了？<br>咱们换个参数来试一试，程序中只更改传入的参数为[ [ 1 , 2 ] ]：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.autograd <span class=\"keyword\">import</span> Variable</span><br><span class=\"line\">    </span><br><span class=\"line\">a = Variable(torch.Tensor([[<span class=\"number\">2.</span>,<span class=\"number\">4.</span>]]),requires_grad=<span class=\"keyword\">True</span>)</span><br><span class=\"line\">b = torch.zeros(<span class=\"number\">1</span>,<span class=\"number\">2</span>)</span><br><span class=\"line\">b[<span class=\"number\">0</span>,<span class=\"number\">0</span>] = a[<span class=\"number\">0</span>,<span class=\"number\">0</span>] ** <span class=\"number\">2</span> + a[<span class=\"number\">0</span>,<span class=\"number\">1</span>] </span><br><span class=\"line\">b[<span class=\"number\">0</span>,<span class=\"number\">1</span>] = a[<span class=\"number\">0</span>,<span class=\"number\">1</span>] ** <span class=\"number\">3</span> + a[<span class=\"number\">0</span>,<span class=\"number\">0</span>]</span><br><span class=\"line\">out = <span class=\"number\">2</span> * b</span><br><span class=\"line\"><span class=\"comment\">#其参数要传入和out维度一样的矩阵</span></span><br><span class=\"line\">out.backward(torch.FloatTensor([[<span class=\"number\">1.</span>,<span class=\"number\">2.</span>]]))</span><br><span class=\"line\">print(<span class=\"string\">'input:'</span>)</span><br><span class=\"line\">print(a.data)</span><br><span class=\"line\">print(<span class=\"string\">'output:'</span>)</span><br><span class=\"line\">print(out.data)</span><br><span class=\"line\">print(<span class=\"string\">'input gradients are:'</span>)</span><br><span class=\"line\">print(a.grad)</span><br></pre></td></tr></table></figure></p>\n<p><img src=\"/images/pasted-6.png\" alt=\"upload successful\"><br>嗯，这回可以理解了，我们传入的参数，是对原来模型正常求导出来的雅克比矩阵进行线性操作，可以把我们传进的参数（设为arg）看成一个列向量，那么我们得到的结果就是:<br>${({M_{Jacobo}}{M_{arg}})}^T$<br>在这个题目中，我们得到的实际是：<br>$$\\begin{cases}<br>{1*{\\partial out_1 \\over \\partial a_1}}+{2*{\\partial out_1 \\over \\partial a_2}}=12\\\\<br>{1*{\\partial out_2 \\over \\partial a_1}}+{2*{\\partial out_2 \\over \\partial a_2}}=12\\\\<br>\\end{cases}<br>$$<br>看起来一切完美的解释了，但是就在我刚刚打字的一刻，我意识到官方文档中说k.backward()传入的参数应该和k具有相同的维度，所以如果按上述去解释是解释不通的。<br>哪里出问题了呢？</p>\n<p>仔细看了一下，原来是这样的：在对雅克比矩阵进行线性操作的时候，应该把我们传进的参数（设为arg）看成一个行向量（不是列向量），那么我们得到的结果就是:${({M_{arg}}{M_{Jacobo}})}^T$<br>也就是：<br>$$\\begin{cases}<br>{1*{\\partial out_1 \\over \\partial a_1}}+{2*{\\partial out_2 \\over \\partial a_1}}=12\\\\<br>{1*{\\partial out_1 \\over \\partial a_2}}+{2*{\\partial out_2 \\over \\partial a_2}}=12\\\\<br>\\end{cases}<br>$$<br>这回我们就解释的通了。</p>\n<p>现在我们来输出一下雅克比矩阵吧，为了不引起歧义，我们让雅克比矩阵的每个数值都不一样（一开始分析错了就是因为雅克比矩阵中有相同的数据），所以模型小改动如下：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.autograd <span class=\"keyword\">import</span> Variable</span><br><span class=\"line\">    </span><br><span class=\"line\">a = Variable(torch.Tensor([[<span class=\"number\">2.</span>,<span class=\"number\">4.</span>]]),requires_grad=<span class=\"keyword\">True</span>)</span><br><span class=\"line\">b = torch.zeros(<span class=\"number\">1</span>,<span class=\"number\">2</span>)</span><br><span class=\"line\">b[<span class=\"number\">0</span>,<span class=\"number\">0</span>] = a[<span class=\"number\">0</span>,<span class=\"number\">0</span>] ** <span class=\"number\">2</span> + a[<span class=\"number\">0</span>,<span class=\"number\">1</span>] </span><br><span class=\"line\">b[<span class=\"number\">0</span>,<span class=\"number\">1</span>] = a[<span class=\"number\">0</span>,<span class=\"number\">1</span>] ** <span class=\"number\">3</span> + a[<span class=\"number\">0</span>,<span class=\"number\">0</span>] * <span class=\"number\">2</span></span><br><span class=\"line\">out = <span class=\"number\">2</span> * b</span><br><span class=\"line\"><span class=\"comment\">#其参数要传入和out维度一样的矩阵</span></span><br><span class=\"line\">out.backward(torch.FloatTensor([[<span class=\"number\">1</span>,<span class=\"number\">0</span>]]),retain_graph=<span class=\"keyword\">True</span>)</span><br><span class=\"line\">A_temp = copy.deepcopy(a.grad)</span><br><span class=\"line\">a.grad.zero_()</span><br><span class=\"line\">out.backward(torch.FloatTensor([[<span class=\"number\">0</span>,<span class=\"number\">1</span>]]))</span><br><span class=\"line\">B_temp = a.grad</span><br><span class=\"line\">print(<span class=\"string\">'jacobian matrix is:'</span>)</span><br><span class=\"line\">print(torch.cat( (A_temp,B_temp),<span class=\"number\">0</span> ))</span><br></pre></td></tr></table></figure></p>\n<p>如果没问题的话咱们的雅克比矩阵应该是 [ [ 8 , 2 ] , [ 4 , 96 ] ]</p>\n<p>好了，下面是见证奇迹的时刻了,不要眨眼睛奥，千万不要眨眼睛……<br>3<br>2<br>1<br>砰…………<br><img src=\"/images/pasted-7.png\" alt=\"upload successful\"><br>好了，现在总结一下：因为经过了复杂的神经网络之后，out中每个数值都是由很多输入样本的属性（也就是输入数据）线性或者非线性组合而成的，那么out中的每个数值和输入数据的每个数值都有关联，也就是说【out】中的每个数都可以对【a】中每个数求导，那么我们backward（）的参数[k1,k2,k3….kn]的含义就是：</p>\n<p>$$\\begin{cases}<br>{\\partial out \\over \\partial a_1}={k_1*{\\partial out_1 \\over \\partial a_1}}+{k_2*{\\partial out_2 \\over \\partial a_1}}+{k_3*{\\partial out_3 \\over \\partial a_1}}+…+{k_n*{\\partial out_n \\over \\partial a_1}}\\\\<br>{\\partial out \\over \\partial a_2}={k_1*{\\partial out_1 \\over \\partial a_2}}+{k_2*{\\partial out_2 \\over \\partial a_2}}+{k_3*{\\partial out_3 \\over \\partial a_2}}+…+{k_n*{\\partial out_n \\over \\partial a_2}}\\\\<br>…\\\\<br>{\\partial out \\over \\partial a_n}={k_1*{\\partial out_1 \\over \\partial a_n}}+{k_2*{\\partial out_2 \\over \\partial a_n}}+{k_3*{\\partial out_3 \\over \\partial a_n}}+…+{k_n*{\\partial out_n \\over \\partial a_n}}\\\\<br>\\end{cases}<br>$$<br>也可以理解成每个out分量对an求导时的权重。</p>\n<h1 id=\"对矩阵自动求导\"><a href=\"#对矩阵自动求导\" class=\"headerlink\" title=\"对矩阵自动求导\"></a>对矩阵自动求导</h1><p>现在，如果out是一个矩阵呢？<br>下面的例子也可以理解为：相当于一个神经网络有两个样本，每个样本有两个属性，神经网络有两个输出。<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.autograd <span class=\"keyword\">import</span> Variable</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"></span><br><span class=\"line\">a = Variable(torch.FloatTensor([[<span class=\"number\">2</span>,<span class=\"number\">3</span>],[<span class=\"number\">1</span>,<span class=\"number\">2</span>]]),requires_grad=<span class=\"keyword\">True</span>)</span><br><span class=\"line\">w = Variable( torch.zeros(<span class=\"number\">2</span>,<span class=\"number\">1</span>),requires_grad=<span class=\"keyword\">True</span> )</span><br><span class=\"line\">out = torch.mm(a,w)</span><br><span class=\"line\">out.backward(torch.FloatTensor([[<span class=\"number\">1.</span>],[<span class=\"number\">1.</span>]]),retain_graph=<span class=\"keyword\">True</span>)</span><br><span class=\"line\">print(<span class=\"string\">\"gradients are:&#123;&#125;\"</span>.format(w.grad.data))</span><br></pre></td></tr></table></figure></p>\n<p>如果前面的例子理解了，那么这个也很好理解，backward输入的参数k是一个2x1的矩阵，2代表的就是样本数量，就是在前面的基础上，再对每个样本进行加权求和。<br>结果是：</p>\n<p><img src=\"/images/pasted-8.png\" alt=\"upload successful\"></p>\n<p>如果有兴趣，也可以拓展一下多个样本的多分类问题，猜一下k的维度应该是【输入样本的个数 X 分类的个数】</p>\n<p>好啦，纠结我好久的pytorch自动求导原理算是彻底搞懂啦~~~</p>\n"}],"PostAsset":[],"PostCategory":[{"post_id":"cjt8bm3ue00089gojd1ox472p","category_id":"cjt8bm3u400049gojap42qmg7","_id":"cjt8bm3v2000c9gojia6k2dw1"},{"post_id":"cjt8bm3tg00009gojlykpkzru","category_id":"cjt8bm3u400049gojap42qmg7","_id":"cjt8bm3v5000f9gojrr1oydvs"},{"post_id":"cjt8bm3um00099gojurz9ha6k","category_id":"cjt8bm3u400049gojap42qmg7","_id":"cjt8bm3v6000h9goj2zx6l6i9"},{"post_id":"cjt8bm3tu00029gojw0qrgfbs","category_id":"cjt8bm3u400049gojap42qmg7","_id":"cjt8bm3va000k9goj3rmdk8ka"},{"post_id":"cjt8bm3u900069goj2hfoens1","category_id":"cjt8bm3v3000d9gojdy453yz7","_id":"cjt8bm3vc000l9gojj37idzs4"}],"PostTag":[{"post_id":"cjt8bm3tg00009gojlykpkzru","tag_id":"cjt8bm3u800059gojgqaklimz","_id":"cjt8bm3v6000g9gojdjrmy6cx"},{"post_id":"cjt8bm3tg00009gojlykpkzru","tag_id":"cjt8bm3uo000b9goj4rbw3xsh","_id":"cjt8bm3v7000i9gojiq8hrdf2"},{"post_id":"cjt8bm3tu00029gojw0qrgfbs","tag_id":"cjt8bm3uo000b9goj4rbw3xsh","_id":"cjt8bm3vd000n9gojqbwoubwq"},{"post_id":"cjt8bm3tu00029gojw0qrgfbs","tag_id":"cjt8bm3u800059gojgqaklimz","_id":"cjt8bm3vd000o9gojph5d3sqn"},{"post_id":"cjt8bm3u900069goj2hfoens1","tag_id":"cjt8bm3vc000m9goj19tipkrf","_id":"cjt8bm3vf000q9gojxyl7euqp"},{"post_id":"cjt8bm3ue00089gojd1ox472p","tag_id":"cjt8bm3u800059gojgqaklimz","_id":"cjt8bm3vh000s9goj6zd4d18q"},{"post_id":"cjt8bm3ue00089gojd1ox472p","tag_id":"cjt8bm3uo000b9goj4rbw3xsh","_id":"cjt8bm3vi000t9gojaw4dd6kw"},{"post_id":"cjt8bm3um00099gojurz9ha6k","tag_id":"cjt8bm3u800059gojgqaklimz","_id":"cjt8bm3vj000u9gojqytfyrvg"},{"post_id":"cjt8bm3um00099gojurz9ha6k","tag_id":"cjt8bm3uo000b9goj4rbw3xsh","_id":"cjt8bm3vj000v9gojm3ib6a0t"}],"Tag":[{"name":"深度学习","_id":"cjt8bm3u800059gojgqaklimz"},{"name":"Pytorch","_id":"cjt8bm3uo000b9goj4rbw3xsh"},{"name":"Ubuntu","_id":"cjt8bm3vc000m9goj19tipkrf"}]}}