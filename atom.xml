<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Jeason</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-04-17T15:53:04.601Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>王建森</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>位运算练习_01</title>
    <link href="http://yoursite.com/2019/04/12/%E4%BD%8D%E8%BF%90%E7%AE%97%E7%BB%83%E4%B9%A0%E4%BD%9C%E4%B8%9A/"/>
    <id>http://yoursite.com/2019/04/12/位运算练习作业/</id>
    <published>2019-04-12T01:44:00.000Z</published>
    <updated>2019-04-17T15:53:04.601Z</updated>
    
    <content type="html"><![CDATA[<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>进一步理解书中第二章《信息的表示和处理》部分的内容,深刻理解整数、浮点数的表示<br>和运算方法,掌握GNU GCC工具集的基本使用方法。</p><h1 id="要求"><a href="#要求" class="headerlink" title="要求"></a>要求</h1><p>请按照要求补全 bits.c 中的函数,并进行验证。包括以下6个函数 :</p><h2 id="1-int-isAsciiDigit-int-x"><a href="#1-int-isAsciiDigit-int-x" class="headerlink" title="1. int isAsciiDigit(int x)"></a>1. int isAsciiDigit(int x)</h2><p>功能:当0x30&lt;=x&lt;=0x39时(即字符0-9的ASCII码值)返回1;其他情况下返回0<br>示例:isAsciiDigit(0x35) = 1<br>isAsciiDigit(0x3a) = 0<br>isAsciiDigit(0x05) = 0<br>难度:3<br>可使用运算符数:15<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">isAsciiDigit</span><span class="params">(<span class="keyword">int</span> x)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">int</span> up_num = x + (~<span class="number">58</span> + <span class="number">1</span>);  <span class="comment">//最高位为1</span></span><br><span class="line">  <span class="keyword">int</span> low_num = x + (~<span class="number">48</span> + <span class="number">1</span>);   <span class="comment">//最高位是0</span></span><br><span class="line">  </span><br><span class="line">  <span class="keyword">int</span> Is_up_legal = (!((up_num &gt;&gt; <span class="number">31</span>) + <span class="number">1</span>));</span><br><span class="line">  <span class="keyword">int</span> Is_down_legal = ((low_num &gt;&gt; <span class="number">31</span>) + <span class="number">1</span>);</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">return</span>   (Is_up_legal &amp; Is_down_legal);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">In this function, what we should <span class="keyword">to</span> do <span class="keyword">is</span> <span class="keyword">to</span> judge weather <span class="keyword">the</span> 'x' <span class="keyword">is</span> bigger than <span class="number">48</span>(inclusive) <span class="keyword">and</span> smaller than <span class="number">57</span>(inclusive). </span><br><span class="line"><span class="keyword">first</span>:</span><br><span class="line">we mapping <span class="keyword">the</span> legal-x:</span><br><span class="line">up_num = x <span class="number">-58</span>;     </span><br><span class="line">low_num = x - <span class="number">48</span>;</span><br><span class="line">we will <span class="keyword">get</span> (up_num &lt; <span class="number">0</span>) <span class="keyword">and</span> (low_num &gt;= <span class="number">0</span>) <span class="keyword">if</span> <span class="keyword">the</span> <span class="string">"x"</span> <span class="keyword">is</span> legal.</span><br><span class="line"><span class="keyword">then</span>:</span><br><span class="line"><span class="string">"(( up_num &gt;&gt; 31) + 1)"</span> will be <span class="keyword">equal</span> <span class="keyword">to</span> <span class="number">0</span> ,<span class="keyword">if</span> (up_num &lt; <span class="number">0</span>).</span><br><span class="line"><span class="string">"((low_num &gt;&gt; 31) + 1)"</span> will be <span class="keyword">equal</span> <span class="keyword">to</span> <span class="number">1</span> ,<span class="keyword">if</span> (low_num &gt;= <span class="number">0</span>).</span><br><span class="line">so,<span class="keyword">if</span> (up_num &lt; <span class="number">0</span>), <span class="string">"Is_up_legal"</span> will be <span class="keyword">equal</span> <span class="keyword">to</span> <span class="number">1.</span><span class="keyword">if</span> (low_num &gt;= <span class="number">0</span>),<span class="string">"Is_down_legal"</span> will be <span class="keyword">equal</span> <span class="keyword">to</span> <span class="number">1.</span></span><br><span class="line"><span class="keyword">last</span>:</span><br><span class="line">only <span class="keyword">if</span> ((up_num &gt;&gt; <span class="number">31</span>) + <span class="number">1</span>) == <span class="number">0</span> <span class="keyword">and</span> ((low_num &gt;&gt; <span class="number">31</span>) + <span class="number">1</span>), we <span class="literal">return</span> <span class="number">1</span>;</span><br></pre></td></tr></table></figure><h2 id="2-int-anyEvenBit-int-x"><a href="#2-int-anyEvenBit-int-x" class="headerlink" title="2. int anyEvenBit(int x)"></a>2. int anyEvenBit(int x)</h2><p>功能:当x的任意偶数位为1时,返回1;其他情况下返回0<br>示例:anyEvenBit(0xA) = 0<br>anyEvenBit(0xE) = 1<br>难度:2<br>可使用运算符数:12<br><figure class="highlight llvm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">int anyEvenBit(int <span class="keyword">x</span>) &#123;</span><br><span class="line">  int <span class="keyword">x</span><span class="number">1</span> = <span class="keyword">x</span> &gt;&gt; <span class="number">8</span><span class="comment">;</span></span><br><span class="line"><span class="comment">  int x2 = x1 &gt;&gt; 8;</span></span><br><span class="line"><span class="comment">  int x3 = x2 &gt;&gt; 8;</span></span><br><span class="line"><span class="comment">  return !!( ( x | x1 | x2 | x3 )&amp; 0x55);</span></span><br><span class="line"><span class="comment">&#125;</span></span><br></pre></td></tr></table></figure></p><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">/*</span><br><span class="line">每次我们移动<span class="number">8</span>位，把移动的<span class="number">3</span>次结果和x一起做<span class="string">|运算，相当于把每8个信息都重叠到最低的8位，</span></span><br><span class="line">此时与<span class="number">01010101</span>做<span class="meta">&amp;运算就可以知道其偶数位是否是1.</span></span><br><span class="line">*/</span><br></pre></td></tr></table></figure><h2 id="3-int-copyLSB-int-x"><a href="#3-int-copyLSB-int-x" class="headerlink" title="3. int copyLSB(int x)"></a>3. int copyLSB(int x)</h2><p>功能:将返回值中的所有位全部置位成x中的第0位的值<br>示例:copyLSB(5) = 0xFFFFFFFF, copyLSB(6) = 0x00000000<br>难度:2<br>可使用运算符数:5<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">copyLSB</span><span class="params">(<span class="keyword">int</span> x)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">int</span> temp = x &amp; <span class="number">1</span>;</span><br><span class="line">  <span class="keyword">return</span> (~temp + <span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">/*</span><br><span class="line"><span class="keyword">first</span>, we could <span class="keyword">get</span> least significant bit <span class="keyword">by</span> used <span class="string">"x &amp; 1"</span>.</span><br><span class="line"><span class="keyword">then</span>, we could <span class="keyword">get</span> <span class="literal">result</span> </span><br><span class="line">*/</span><br></pre></td></tr></table></figure><h2 id="4-int-leastBitPos-int-x"><a href="#4-int-leastBitPos-int-x" class="headerlink" title="4. int leastBitPos(int x)"></a>4. int leastBitPos(int x)</h2><p>功能:返回一个掩码,在该掩码中标识了二进制数x的所有位中,“1”所在的位权最小的位<br>示例:leastBitPos(0x60) = 0x20<br>难度:2<br>可使用运算符数:6</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">leastBitPos</span><span class="params">(<span class="keyword">int</span> x)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> (~x+<span class="number">1</span>) &amp; x;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">/*</span><br><span class="line">we <span class="keyword">get</span> <span class="keyword">the</span> <span class="string">"-x"</span>,than <span class="string">"(-x)&amp;x"</span> <span class="keyword">is</span> what we need.</span><br><span class="line">*/</span><br></pre></td></tr></table></figure><h2 id="5-int-divpwr2-int-x-int-n"><a href="#5-int-divpwr2-int-x-int-n" class="headerlink" title="5. int divpwr2(int x, int n)"></a>5. int divpwr2(int x, int n)</h2><p>功能:计算 x / 2^n,并将结果取整<br>示例:divpwr2(15,1) = 7<br>divpwr2(-33,4) = -2<br>难度:2<br>可使用运算符数:15</p><figure class="highlight excel"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">int</span> divpwr2(<span class="built_in">int</span> x, <span class="built_in">int</span> <span class="built_in">n</span>) &#123;</span><br><span class="line">  <span class="built_in">int</span> op = (x &gt;&gt; <span class="number">31</span>) + <span class="number">1</span>;</span><br><span class="line">  <span class="built_in">int</span> temp  =   x  &gt;&gt; <span class="built_in">n</span>;</span><br><span class="line">  <span class="built_in">int</span> temp2 = temp &lt;&lt; <span class="built_in">n</span>;</span><br><span class="line">  <span class="built_in">int</span> Is_exact_division = !!(temp2 ^ x);</span><br><span class="line">  return (temp + (    (!op) &amp; (!!<span class="built_in">n</span>) &amp; (Is_exact_division)  )   );</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight excel"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">/*</span><br><span class="line">一般来说，“&gt;&gt;”符号代表 x/(<span class="number">2</span>^<span class="built_in">n</span>),并向下取整（无论正负）。</span><br><span class="line">所以有三种情况需要考虑：</span><br><span class="line">  如果负数，需要向上取整。</span><br><span class="line">  如果 <span class="built_in">n</span>=<span class="number">0</span> 应该为 x。</span><br><span class="line">  如果能够整除，就不需要取整操作。</span><br><span class="line"></span><br><span class="line">此题中，op是符号标志位，正数为<span class="number">1</span>，负数为<span class="number">0</span>.</span><br><span class="line">temp为x右移动<span class="built_in">n</span>位结果，相当直接 x/(<span class="number">2</span>^<span class="built_in">n</span>) ,向下取整。</span><br><span class="line">temp2为temp左移动<span class="built_in">n</span>位结果，利用(temp2 ^ x)，可以判断temp舍弃的几位是否都是<span class="number">0</span>（既能否整除）。</span><br><span class="line">Is_exact_division 代表如果能整除为<span class="number">0</span>，反之为<span class="number">1</span>。</span><br><span class="line">因为&gt;&gt;默认代表向下取整，所以我们需要考虑上述情况，利用 (!op) &amp; (!!<span class="built_in">n</span>) &amp; (Is_exact_division）实现，</span><br><span class="line">其含义：同时满足 (负数）(<span class="built_in">n</span>！=<span class="number">0</span>）(不能整除) 三种情况返回<span class="number">1</span>，否则返回<span class="number">0</span>。</span><br><span class="line">也意味着，满足上述三情况才需要在向下取整的前提+<span class="number">1</span>，否则向下取整结果就是正常取整结果。</span><br><span class="line">*/</span><br></pre></td></tr></table></figure><h2 id="6-int-bitCount-int-x"><a href="#6-int-bitCount-int-x" class="headerlink" title="6. int bitCount(int x)"></a>6. int bitCount(int x)</h2><p>功能:计算二进制数x中,对应位值“1”的总位数 示例:bitCount(5) = 2<br>bitCount(7) = 3<br>难度:4<br>可使用运算符数:40</p><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">int bitCount(int x) &#123;</span><br><span class="line"></span><br><span class="line">  int mask = (<span class="number">0x11</span> &lt;&lt;<span class="number">24</span>) | (<span class="number">0x11</span> &lt;&lt; <span class="number">16</span>) | (<span class="number">0x11</span> &lt;&lt; <span class="number">8</span>) |(<span class="number">0x11</span>) ;</span><br><span class="line"></span><br><span class="line">  int temp = x &amp; mask;</span><br><span class="line">  x = x &gt;&gt; <span class="number">1</span>;</span><br><span class="line">  temp = temp + (x &amp; mask); </span><br><span class="line">  x = x &gt;&gt; <span class="number">1</span>;</span><br><span class="line">  temp = temp + (x &amp; mask); </span><br><span class="line">  x = x &gt;&gt; <span class="number">1</span>;</span><br><span class="line">  temp = temp + (x &amp; mask); </span><br><span class="line">  </span><br><span class="line">  return ((temp &amp; <span class="number">0xf</span>) + </span><br><span class="line">          ( (temp &gt;&gt; <span class="number">4</span>) &amp; <span class="number">0xf</span> )+ </span><br><span class="line">          ( (temp &gt;&gt; <span class="number">8</span>) &amp; <span class="number">0xf</span> )+</span><br><span class="line">          ( (temp &gt;&gt; <span class="number">12</span>) &amp; <span class="number">0xf</span> )+</span><br><span class="line">          ( (temp &gt;&gt; <span class="number">16</span>) &amp; <span class="number">0xf</span> )+</span><br><span class="line">          ( (temp &gt;&gt; <span class="number">20</span>) &amp; <span class="number">0xf</span> )+</span><br><span class="line">          ( (temp &gt;&gt; <span class="number">24</span>) &amp; <span class="number">0xf</span> )+</span><br><span class="line">          ( (temp &gt;&gt; <span class="number">28</span>) &amp; <span class="number">0xf</span> ));</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight actionscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*首先分割成4块，每块右移动，同时比较每个块的1的个数*/</span></span><br></pre></td></tr></table></figure><h2 id="tip-程序内允许使用"><a href="#tip-程序内允许使用" class="headerlink" title="tip:程序内允许使用:"></a>tip:程序内允许使用:</h2><p>a. 运算符: ! ~ &amp; ^ | + &lt;&lt; &gt;&gt;<br>b. 范围在0 - 255之间的常数<br>c. 局部变量</p><h2 id="tip-程序内禁止以下行为"><a href="#tip-程序内禁止以下行为" class="headerlink" title="tip:程序内禁止以下行为:"></a>tip:程序内禁止以下行为:</h2><p>a. 声明和使用全局变量<br>b. 声明和使用定义宏<br>c. 声明和调用其他的函数<br>d. 类型的强制转换<br>e. 使用许可范围之外的运算符<br>f . 使用控制跳转语句:if else switch do while for<br>注意:违背以上原则均视为程序不正确!!</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h1&gt;&lt;p&gt;进一步理解书中第二章《信息的表示和处理》部分的内容,深刻理解整数、浮点数的表示&lt;br&gt;和运算方法,掌握GNU GCC工具集的基本使用方法。&lt;
      
    
    </summary>
    
      <category term="算法" scheme="http://yoursite.com/categories/%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="算法" scheme="http://yoursite.com/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>hexo博客更换电脑操作踩坑记录</title>
    <link href="http://yoursite.com/2019/03/19/hexo%E5%8D%9A%E5%AE%A2%E6%9B%B4%E6%8D%A2%E7%94%B5%E8%84%91%E6%93%8D%E4%BD%9C%E8%B8%A9%E5%9D%91%E8%AE%B0%E5%BD%95/"/>
    <id>http://yoursite.com/2019/03/19/hexo博客更换电脑操作踩坑记录/</id>
    <published>2019-03-18T16:44:00.000Z</published>
    <updated>2019-04-17T15:51:20.887Z</updated>
    
    <content type="html"><![CDATA[<p>啦啦啦啦啦啦<br>我是快乐的小画家d</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;啦啦啦啦啦啦&lt;br&gt;我是快乐的小画家d&lt;/p&gt;

      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Ubuntu18.04优化美化踩坑记录</title>
    <link href="http://yoursite.com/2019/03/06/Ubuntu18-04%E6%B7%B1%E5%BA%A6%E4%BC%98%E5%8C%96%E7%BE%8E%E5%8C%96%E8%AE%B0%E5%BD%95/"/>
    <id>http://yoursite.com/2019/03/06/Ubuntu18-04深度优化美化记录/</id>
    <published>2019-03-06T13:14:00.000Z</published>
    <updated>2019-04-12T01:15:25.591Z</updated>
    
    <content type="html"><![CDATA[<p>喜欢Ubuntu的一个好处就是相比win来说Linux可以更方便的对系统进行一些修改，而且安装很多东西很方便，相比mac可以充分利用硬件（mac没有N卡，配置深度学习的时候不能用显卡跑代码）。作为一个颜控，最重要的当然是Ubuntu界面很舒服很漂亮～～～～</p><p>自从接触了Ubuntu之后就开始了折腾之旅，装双系统、分区、熟悉操作命令、配置环境、美化界面等等，对于一个小白来说，过程还是很艰辛的。很早之前就想把折腾的过程记录下来了，但是太懒啦，哈哈哈。这两天室友刚开始用Ubuntu，激励我记录一下，文章大概可以分成：<strong>显卡配置和电源管理</strong>、<strong>美化系统主题</strong>、<strong>美化grub启动项</strong>、<strong>基础命令操作</strong>，这几个大部分。</p><hr><h1 id="显卡配置和电源管理"><a href="#显卡配置和电源管理" class="headerlink" title="显卡配置和电源管理"></a>显卡配置和电源管理</h1><p>我的电脑是小米，刚装Ubuntu的时候真的是为发烧而生，风扇声音比我散热器的声音都大，非常hot并且导致续航时间严重缩短（当初我换新电脑的时候可主要因为原来电脑续航不够哇），上网查原因主要是显卡驱动问题和Ubuntn没有win那么好的电源管理导致的。所以主要从这两个方面入手。</p><h2 id="TLP电源管理软件"><a href="#TLP电源管理软件" class="headerlink" title="TLP电源管理软件"></a>TLP电源管理软件</h2><p>首先TLP是免费的，可以减少电脑发热量和增加笔记本电池使用时间的电源管理工具。它是轻量级的工具，没有GUI，不用进行大量配置，一般的电脑使用默认配置就可以了。但是默认的配置会导致系统把显卡驱动切换到集成显卡上，所以一会我们还要管理一下显卡的驱动，首先介绍tlp的安装，随后介绍tlp的配置文件各代表了什么含义。</p><h3 id="tlp的安装"><a href="#tlp的安装" class="headerlink" title="tlp的安装"></a>tlp的安装</h3><figure class="highlight smali"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">添加PPA：</span><br><span class="line">sudo<span class="built_in"> add-apt-repository </span>ppa:linrunner/tlp</span><br><span class="line">更新软件列表：</span><br><span class="line">sudo apt-get update</span><br><span class="line">安装TLP：</span><br><span class="line">sudo apt install tlp</span><br><span class="line">启动TLP：</span><br><span class="line">sudo tlp start</span><br><span class="line">&gt;&gt;&gt; TLP started in AC mode.</span><br></pre></td></tr></table></figure><h3 id="tlp的配置文件"><a href="#tlp的配置文件" class="headerlink" title="tlp的配置文件"></a>tlp的配置文件</h3><p>现在 TLP 已经被启动起来了，而且已经设置好了节省电池所需要的默认配置。我们可以查看该配置文件。文件路径为 /etc/default/tlp。我们需要编辑该文件来修改各项配置。配置的一些示例如下：<br><figure class="highlight vala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># ------------------------------------------------------------------------------</span></span><br><span class="line"><span class="meta"># tlp - Parameters for power save</span></span><br><span class="line"><span class="meta"># See full explanation: http://linrunner.de/en/tlp/docs/tlp-configuration.html</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Hint: some features are disabled by default, remove the leading # to enable      #通过去掉“#”来开启参数</span></span><br><span class="line"><span class="meta"># them.</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Set to 0 to disable, 1 to enable TLP.    #设置为‘“1”启用TLP服务</span></span><br><span class="line">TLP_ENABLE=<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Operation mode when no power supply can be detected: AC, BAT</span></span><br><span class="line"><span class="meta"># Concerns some desktop and embedded hardware only.</span></span><br><span class="line">TLP_DEFAULT_MODE=AC</span><br><span class="line"></span><br><span class="line"><span class="meta"># Seconds laptop mode has to wait after the disk goes idle before doing a sync.</span></span><br><span class="line"><span class="meta"># Non-zero value enables, zero disables laptop mode.</span></span><br><span class="line">DISK_IDLE_SECS_ON_AC=<span class="number">0</span></span><br><span class="line">DISK_IDLE_SECS_ON_BAT=<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Dirty page values (timeouts in secs).</span></span><br><span class="line">MAX_LOST_WORK_SECS_ON_AC=<span class="number">30</span></span><br><span class="line">MAX_LOST_WORK_SECS_ON_BAT=<span class="number">90</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Hint: CPU parameters below are disabled by default, remove the leading #</span></span><br><span class="line"><span class="meta"># to enable them, otherwise kernel default values are used.</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Select a CPU frequency scaling governor.                      #CPU调度策略</span></span><br><span class="line"><span class="meta"># Intel Core i processor with intel_pstate driver:</span></span><br><span class="line"><span class="meta">#   powersave(*), performance</span></span><br><span class="line"><span class="meta"># Older hardware with acpi-cpufreq driver:</span></span><br><span class="line"><span class="meta">#   ondemand(*), powersave, performance, conservative</span></span><br><span class="line"><span class="meta"># (*) is recommended.</span></span><br><span class="line"><span class="meta"># Hint: use tlp-stat -p to show the active driver and available governors.</span></span><br><span class="line"><span class="meta"># Important:</span></span><br><span class="line"><span class="meta">#   You *must* disable your distribution's governor settings or conflicts will</span></span><br><span class="line"><span class="meta">#   occur. ondemand is sufficient for *almost all* workloads, you should know</span></span><br><span class="line"><span class="meta">#   what you're doing!</span></span><br><span class="line">CPU_SCALING_GOVERNOR_ON_AC=powersave</span><br><span class="line">CPU_SCALING_GOVERNOR_ON_BAT=powersave</span><br><span class="line"></span><br><span class="line"><span class="meta"># Set the min/max frequency available for the scaling governor.</span></span><br><span class="line"><span class="meta"># Possible values strongly depend on your CPU. For available frequencies see</span></span><br><span class="line"><span class="meta"># the output of tlp-stat -p.</span></span><br><span class="line"><span class="meta">#CPU_SCALING_MIN_FREQ_ON_AC=0</span></span><br><span class="line"><span class="meta">#CPU_SCALING_MAX_FREQ_ON_AC=0</span></span><br><span class="line"><span class="meta">#CPU_SCALING_MIN_FREQ_ON_BAT=0</span></span><br><span class="line"><span class="meta">#CPU_SCALING_MAX_FREQ_ON_BAT=0</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Set Intel P-state performance: 0..100 (%)</span></span><br><span class="line"><span class="meta"># Limit the max/min P-state to control the power dissipation of the CPU.</span></span><br><span class="line"><span class="meta"># Values are stated as a percentage of the available performance.</span></span><br><span class="line"><span class="meta"># Requires an Intel Core i processor with intel_pstate driver.</span></span><br><span class="line">CPU_MIN_PERF_ON_AC=<span class="number">0</span></span><br><span class="line">CPU_MAX_PERF_ON_AC=<span class="number">100</span></span><br><span class="line">CPU_MIN_PERF_ON_BAT=<span class="number">0</span></span><br><span class="line">CPU_MAX_PERF_ON_BAT=<span class="number">30</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Set the CPU "turbo boost" feature: 0=disable, 1=allow       #开启intel cpu 睿频</span></span><br><span class="line"><span class="meta"># Requires an Intel Core i processor.</span></span><br><span class="line"><span class="meta"># Important:</span></span><br><span class="line"><span class="meta"># - This may conflict with your distribution's governor settings</span></span><br><span class="line"><span class="meta"># - A value of 1 does *not* activate boosting, it just allows it</span></span><br><span class="line">CPU_BOOST_ON_AC=<span class="number">1</span></span><br><span class="line">CPU_BOOST_ON_BAT=<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Minimize number of used CPU cores/hyper-threads under light load conditions        #与cpu有关</span></span><br><span class="line">SCHED_POWERSAVE_ON_AC=<span class="number">0</span></span><br><span class="line">SCHED_POWERSAVE_ON_BAT=<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Kernel NMI Watchdog:</span></span><br><span class="line"><span class="meta">#   0=disable (default, saves power), 1=enable (for kernel debugging only)</span></span><br><span class="line">NMI_WATCHDOG=<span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Change CPU voltages aka "undervolting" - Kernel with PHC patch required     #调节CPU 电压以达到节能的目的 ，谨慎开启！！！</span></span><br><span class="line"><span class="meta"># Frequency voltage pairs are written to:</span></span><br><span class="line"><span class="meta">#   /sys/devices/system/cpu/cpu0/cpufreq/phc_controls</span></span><br><span class="line"><span class="meta"># CAUTION: only use this, if you thoroughly understand what you are doing!</span></span><br><span class="line"><span class="meta">#PHC_CONTROLS="F:V F:V F:V F:V"</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Set CPU performance versus energy savings policy:              #与CPU有关</span></span><br><span class="line"><span class="meta">#   performance, normal, powersave</span></span><br><span class="line"><span class="meta"># Requires kernel module msr and x86_energy_perf_policy from linux-tools</span></span><br><span class="line">ENERGY_PERF_POLICY_ON_AC=normal</span><br><span class="line">ENERGY_PERF_POLICY_ON_BAT=powersave</span><br><span class="line"></span><br><span class="line"><span class="meta"># Hard disk devices; separate multiple devices with spaces (default: sda).</span></span><br><span class="line"><span class="meta"># Devices can be specified by disk ID also (lookup with: tlp diskid).</span></span><br><span class="line">DISK_DEVICES=<span class="string">"sda sdb"</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Hard disk advanced power management level: 1..254, 255 (max saving, min, off)</span></span><br><span class="line"><span class="meta"># Levels 1..127 may spin down the disk; 255 allowable on most drives.</span></span><br><span class="line"><span class="meta"># Separate values for multiple disks with spaces. Use the special value 'keep'</span></span><br><span class="line"><span class="meta"># to keep the hardware default for the particular disk.</span></span><br><span class="line">DISK_APM_LEVEL_ON_AC=<span class="string">"254 254"</span></span><br><span class="line">DISK_APM_LEVEL_ON_BAT=<span class="string">"128 128"</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Hard disk spin down timeout:</span></span><br><span class="line"><span class="meta">#   0:        spin down disabled</span></span><br><span class="line"><span class="meta">#   1..240:   timeouts from 5s to 20min (in units of 5s)</span></span><br><span class="line"><span class="meta">#   241..251: timeouts from 30min to 5.5 hours (in units of 30min)</span></span><br><span class="line"><span class="meta"># See 'man hdparm' for details.</span></span><br><span class="line"><span class="meta"># Separate values for multiple disks with spaces. Use the special value 'keep'</span></span><br><span class="line"><span class="meta"># to keep the hardware default for the particular disk.</span></span><br><span class="line"><span class="meta">#DISK_SPINDOWN_TIMEOUT_ON_AC="0 0"</span></span><br><span class="line"><span class="meta">#DISK_SPINDOWN_TIMEOUT_ON_BAT="0 0"</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Select IO scheduler for the disk devices: cfq, deadline, noop (Default: cfq);     #选择磁盘驱动器I/O调度方式，建议deadline</span></span><br><span class="line"><span class="meta"># Separate values for multiple disks with spaces. Use the special value 'keep'</span></span><br><span class="line"><span class="meta"># to keep the kernel default scheduler for the particular disk.</span></span><br><span class="line">DISK_IOSCHED=<span class="string">"deadline cfq"</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># SATA aggressive link power management (ALPM):</span></span><br><span class="line"><span class="meta">#   min_power, medium_power, max_performance</span></span><br><span class="line">SATA_LINKPWR_ON_AC=max_performance</span><br><span class="line">SATA_LINKPWR_ON_BAT=min_power</span><br><span class="line"></span><br><span class="line"><span class="meta"># Exclude SATA host devices from link power management.</span></span><br><span class="line"><span class="meta"># Separate multiple hosts with spaces.</span></span><br><span class="line"><span class="meta">#SATA_LINKPWR_BLACKLIST="host1"</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Runtime Power Management for AHCI controllers and disks:      #请谨慎开启，有可能导致磁盘被锁或者数据丢失</span></span><br><span class="line"><span class="meta">#   on=disable, auto=enable</span></span><br><span class="line"><span class="meta"># EXPERIMENTAL ** WARNING: auto will most likely cause system lockups/data loss</span></span><br><span class="line"><span class="meta">#AHCI_RUNTIME_PM_ON_AC=on</span></span><br><span class="line"><span class="meta">#AHCI_RUNTIME_PM_ON_BAT=on</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Seconds of inactivity before disk is suspended</span></span><br><span class="line">AHCI_RUNTIME_PM_TIMEOUT=<span class="number">15</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># PCI Express Active State Power Management (PCIe ASPM):</span></span><br><span class="line"><span class="meta">#   default, performance, powersave</span></span><br><span class="line">PCIE_ASPM_ON_AC=performance</span><br><span class="line">PCIE_ASPM_ON_BAT=powersave</span><br><span class="line"></span><br><span class="line"><span class="meta"># Radeon graphics clock speed (profile method): low, mid, high, auto, default;      #NVIDIA显卡用户请无视或者禁用</span></span><br><span class="line"><span class="meta"># auto = mid on BAT, high on AC; default = use hardware defaults.</span></span><br><span class="line"><span class="meta"># (Kernel &gt;= 2.6.35 only, open-source radeon driver explicitly)</span></span><br><span class="line"><span class="meta">#RADEON_POWER_PROFILE_ON_AC=high</span></span><br><span class="line"><span class="meta">#RADEON_POWER_PROFILE_ON_BAT=low</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Radeon dynamic power management method (DPM): battery, performance        #NVIDIA显卡用户请无视或者禁用</span></span><br><span class="line"><span class="meta"># (Kernel &gt;= 3.11 only, requires boot option radeon.dpm=1)</span></span><br><span class="line"><span class="meta">#RADEON_DPM_STATE_ON_AC=performance</span></span><br><span class="line"><span class="meta">#RADEON_DPM_STATE_ON_BAT=battery</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Radeon DPM performance level: auto, low, high; auto is recommended .           #NVIDIA显卡用户请无视或者禁用</span></span><br><span class="line"><span class="meta">#RADEON_DPM_PERF_LEVEL_ON_AC=auto</span></span><br><span class="line"><span class="meta">#RADEON_DPM_PERF_LEVEL_ON_BAT=auto</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># WiFi power saving mode: on=enable, off=disable; not supported by all adapters.</span></span><br><span class="line">WIFI_PWR_ON_AC=off</span><br><span class="line">WIFI_PWR_ON_BAT=on</span><br><span class="line"></span><br><span class="line"><span class="meta"># Disable wake on LAN: Y/N                 #禁用WOL</span></span><br><span class="line">WOL_DISABLE=N</span><br><span class="line"></span><br><span class="line"><span class="meta"># Enable audio power saving for Intel HDA, AC97 devices (timeout in secs).     #与音频有关</span></span><br><span class="line"><span class="meta"># A value of 0 disables, &gt;=1 enables power save.</span></span><br><span class="line">SOUND_POWER_SAVE_ON_AC=<span class="number">0</span></span><br><span class="line">SOUND_POWER_SAVE_ON_BAT=<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Disable controller too (HDA only): Y/N            #与音频有关</span></span><br><span class="line">SOUND_POWER_SAVE_CONTROLLER=Y</span><br><span class="line"></span><br><span class="line"><span class="meta"># Set to 1 to power off optical drive in UltraBay/MediaBay when running on</span></span><br><span class="line"><span class="meta"># battery. A value of 0 disables this feature (Default).</span></span><br><span class="line"><span class="meta"># Drive can be powered on again by releasing (and reinserting) the eject lever</span></span><br><span class="line"><span class="meta"># or by pressing the disc eject button on newer models.</span></span><br><span class="line"><span class="meta"># Note: an UltraBay/MediaBay hard disk is never powered off.</span></span><br><span class="line">BAY_POWEROFF_ON_BAT=<span class="number">0</span></span><br><span class="line"><span class="meta"># Optical drive device to power off (default sr0).</span></span><br><span class="line">BAY_DEVICE=<span class="string">"sr0"</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Runtime Power Management for PCI(e) bus devices: on=disable, auto=enable</span></span><br><span class="line">RUNTIME_PM_ON_AC=on</span><br><span class="line">RUNTIME_PM_ON_BAT=auto</span><br><span class="line"></span><br><span class="line"><span class="meta"># Runtime PM for *all* PCI(e) bus devices, except blacklisted ones:</span></span><br><span class="line"><span class="meta">#   0=disable, 1=enable</span></span><br><span class="line">RUNTIME_PM_ALL=<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Exclude PCI(e) device adresses the following list from Runtime PM</span></span><br><span class="line"><span class="meta"># (separate with spaces). Use lspci to get the adresses (1st column).</span></span><br><span class="line"><span class="meta">#RUNTIME_PM_BLACKLIST="bb:dd.f 11:22.3 44:55.6"</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Exclude PCI(e) devices assigned to the listed drivers from Runtime PM</span></span><br><span class="line"><span class="meta"># (should prevent accidential power on of hybrid graphics' discrete part).</span></span><br><span class="line"><span class="meta"># Default is "radeon nouveau"; use "" to disable the feature completely.</span></span><br><span class="line"><span class="meta"># Separate multiple drivers with spaces.</span></span><br><span class="line">RUNTIME_PM_DRIVER_BLACKLIST=<span class="string">"radeon nouveau"</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Set to 0 to disable, 1 to enable USB autosuspend feature.</span></span><br><span class="line">USB_AUTOSUSPEND=<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Exclude listed devices from USB autosuspend (separate with spaces).</span></span><br><span class="line"><span class="meta"># Use lsusb to get the ids.</span></span><br><span class="line"><span class="meta"># Note: input devices (usbhid) are excluded automatically (see below)</span></span><br><span class="line"><span class="meta">#USB_BLACKLIST="1111:2222 3333:4444"</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># WWAN devices are excluded from USB autosuspend: 0=do not exclude / 1=exclude</span></span><br><span class="line">USB_BLACKLIST_WWAN=<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Include listed devices into USB autosuspend even if already excluded</span></span><br><span class="line"><span class="meta"># by the driver or WWAN blacklists above (separate with spaces).</span></span><br><span class="line"><span class="meta"># Use lsusb to get the ids.</span></span><br><span class="line"><span class="meta">#USB_WHITELIST="1111:2222 3333:4444"</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Set to 1 to disable autosuspend before shutdown, 0 to do nothing</span></span><br><span class="line"><span class="meta"># (workaround for USB devices that cause shutdown problems).</span></span><br><span class="line"><span class="meta">#USB_AUTOSUSPEND_DISABLE_ON_SHUTDOWN=1</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Restore radio device state (Bluetooth, WiFi, WWAN) from previous shutdown</span></span><br><span class="line"><span class="meta"># on system startup: 0=disable, 1=enable.</span></span><br><span class="line"><span class="meta"># Hint: the parameters DEVICES_TO_DISABLE/ENABLE_ON_STARTUP/SHUTDOWN below</span></span><br><span class="line"><span class="meta">#   are ignored when this is enabled!</span></span><br><span class="line">RESTORE_DEVICE_STATE_ON_STARTUP=<span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Radio devices to disable on startup: bluetooth, wifi, wwan.</span></span><br><span class="line"><span class="meta"># Separate multiple devices with spaces.</span></span><br><span class="line"><span class="meta">#DEVICES_TO_DISABLE_ON_STARTUP="bluetooth wifi wwan"</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Radio devices to enable on startup: bluetooth, wifi, wwan.</span></span><br><span class="line"><span class="meta"># Separate multiple devices with spaces.</span></span><br><span class="line"><span class="meta">#DEVICES_TO_ENABLE_ON_STARTUP="wifi"</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Radio devices to disable on shutdown: bluetooth, wifi, wwan</span></span><br><span class="line"><span class="meta"># (workaround for devices that are blocking shutdown).</span></span><br><span class="line"><span class="meta">#DEVICES_TO_DISABLE_ON_SHUTDOWN="bluetooth wifi wwan"</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Radio devices to enable on shutdown: bluetooth, wifi, wwan</span></span><br><span class="line"><span class="meta"># (to prevent other operating systems from missing radios).</span></span><br><span class="line"><span class="meta">#DEVICES_TO_ENABLE_ON_SHUTDOWN="wwan"</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Radio devices to enable on AC: bluetooth, wifi, wwan</span></span><br><span class="line"><span class="meta">#DEVICES_TO_ENABLE_ON_AC="bluetooth wifi wwan"</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Radio devices to disable on battery: bluetooth, wifi, wwan</span></span><br><span class="line"><span class="meta">#DEVICES_TO_DISABLE_ON_BAT="bluetooth wifi wwan"</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Radio devices to disable on battery when not in use (not connected):</span></span><br><span class="line"><span class="meta"># bluetooth, wifi, wwan</span></span><br><span class="line"><span class="meta">#DEVICES_TO_DISABLE_ON_BAT_NOT_IN_USE="bluetooth wifi wwan"</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Battery charge thresholds (ThinkPad only, tp-smapi or acpi-call kernel module             #ThinkPad笔记本使用，其它品牌谨慎开启</span></span><br><span class="line"><span class="meta"># required). Charging starts when the remaining capacity falls below the</span></span><br><span class="line"><span class="meta"># START_CHARGE_THRESH value and stops when exceeding the STOP_CHARGE_THRESH value.</span></span><br><span class="line"><span class="meta"># Main / Internal battery (values in %)</span></span><br><span class="line"><span class="meta">#START_CHARGE_THRESH_BAT0=75</span></span><br><span class="line"><span class="meta">#STOP_CHARGE_THRESH_BAT0=80</span></span><br><span class="line"><span class="meta"># Ultrabay / Slice / Replaceable battery (values in %)</span></span><br><span class="line"><span class="meta">#START_CHARGE_THRESH_BAT1=75</span></span><br><span class="line"><span class="meta">#STOP_CHARGE_THRESH_BAT1=80</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># ------------------------------------------------------------------------------</span></span><br><span class="line"><span class="meta"># tlp-rdw - Parameters for the radio device wizard</span></span><br><span class="line"><span class="meta"># Possible devices: bluetooth, wifi, wwan</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Hints:</span></span><br><span class="line"><span class="meta"># - Parameters are disabled by default, remove the leading # to enable them.</span></span><br><span class="line"><span class="meta"># - Separate multiple radio devices with spaces.</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Radio devices to disable on connect.</span></span><br><span class="line"><span class="meta">#DEVICES_TO_DISABLE_ON_LAN_CONNECT="wifi wwan"</span></span><br><span class="line"><span class="meta">#DEVICES_TO_DISABLE_ON_WIFI_CONNECT="wwan"</span></span><br><span class="line"><span class="meta">#DEVICES_TO_DISABLE_ON_WWAN_CONNECT="wifi"</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Radio devices to enable on disconnect.</span></span><br><span class="line"><span class="meta">#DEVICES_TO_ENABLE_ON_LAN_DISCONNECT="wifi wwan"</span></span><br><span class="line"><span class="meta">#DEVICES_TO_ENABLE_ON_WIFI_DISCONNECT=""</span></span><br><span class="line"><span class="meta">#DEVICES_TO_ENABLE_ON_WWAN_DISCONNECT=""</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Radio devices to enable/disable when docked.</span></span><br><span class="line"><span class="meta">#DEVICES_TO_ENABLE_ON_DOCK=""</span></span><br><span class="line"><span class="meta">#DEVICES_TO_DISABLE_ON_DOCK=""</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Radio devices to enable/disable when undocked.</span></span><br><span class="line"><span class="meta">#DEVICES_TO_ENABLE_ON_UNDOCK="wifi"</span></span><br><span class="line"><span class="meta">#DEVICES_TO_DISABLE_ON_UNDOCK=""</span></span><br></pre></td></tr></table></figure></p><p>我主要是在cpu那个部分做了一些调整，一个i7的电脑，硬生生被调成了i5的性能，我真的是后悔当时没直接买i5。更详细的信息可以参考<a href="https://linrunner.de/en/tlp/tlp.html" target="_blank" rel="noopener">官方文档</a>.</p><p>给出我的配置：<br><figure class="highlight vala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># ------------------------------------------------------------------------------</span></span><br><span class="line"><span class="meta"># tlp - Parameters for power saving</span></span><br><span class="line"><span class="meta"># See full explanation: http://linrunner.de/en/tlp/docs/tlp-configuration.html</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Hint: some features are disabled by default, remove the leading # to enable</span></span><br><span class="line"><span class="meta"># them.</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Set to 0 to disable, 1 to enable TLP.</span></span><br><span class="line">TLP_ENABLE=<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Operation mode when no power supply can be detected: AC, BAT.</span></span><br><span class="line"><span class="meta"># Concerns some desktop and embedded hardware only.</span></span><br><span class="line">TLP_DEFAULT_MODE=AC</span><br><span class="line"></span><br><span class="line"><span class="meta"># Operation mode select: 0=depend on power source, 1=always use TLP_DEFAULT_MODE</span></span><br><span class="line"><span class="meta"># Hint: use in conjunction with TLP_DEFAULT_MODE=BAT for BAT settings on AC.</span></span><br><span class="line">TLP_PERSISTENT_DEFAULT=<span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Seconds laptop mode has to wait after the disk goes idle before doing a sync.</span></span><br><span class="line"><span class="meta"># Non-zero value enables, zero disables laptop mode.</span></span><br><span class="line">DISK_IDLE_SECS_ON_AC=<span class="number">0</span></span><br><span class="line">DISK_IDLE_SECS_ON_BAT=<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Dirty page values (timeouts in secs).</span></span><br><span class="line">MAX_LOST_WORK_SECS_ON_AC=<span class="number">15</span></span><br><span class="line">MAX_LOST_WORK_SECS_ON_BAT=<span class="number">60</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Hint: CPU parameters below are disabled by default, remove the leading #</span></span><br><span class="line"><span class="meta"># to enable them, otherwise kernel default values are used.</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Select a CPU frequency scaling governor.</span></span><br><span class="line"><span class="meta"># Intel Core i processor with intel_pstate driver:</span></span><br><span class="line"><span class="meta">#   powersave(*), performance.</span></span><br><span class="line"><span class="meta"># Older hardware with acpi-cpufreq driver:</span></span><br><span class="line"><span class="meta">#   ondemand(*), powersave, performance, conservative, schedutil.</span></span><br><span class="line"><span class="meta"># (*) is recommended.</span></span><br><span class="line"><span class="meta"># Hint: use tlp-stat -p to show the active driver and available governors.</span></span><br><span class="line"><span class="meta"># Important:</span></span><br><span class="line"><span class="meta">#   powersave for intel_pstate and ondemand for acpi-cpufreq are power</span></span><br><span class="line"><span class="meta">#   efficient for *almost all* workloads and therefore kernel and most</span></span><br><span class="line"><span class="meta">#   distributions have chosen them as defaults. If you still want to change,</span></span><br><span class="line"><span class="meta">#   you should know what you're doing! You *must* disable your distribution's</span></span><br><span class="line"><span class="meta">#   governor settings or conflicts will occur.</span></span><br><span class="line">CPU_SCALING_GOVERNOR_ON_AC=powersave</span><br><span class="line">CPU_SCALING_GOVERNOR_ON_BAT=powersave</span><br><span class="line"></span><br><span class="line"><span class="meta"># Set the min/max frequency available for the scaling governor.</span></span><br><span class="line"><span class="meta"># Possible values strongly depend on your CPU. For available frequencies see</span></span><br><span class="line"><span class="meta"># the output of tlp-stat -p.</span></span><br><span class="line">CPU_SCALING_MIN_FREQ_ON_AC=<span class="number">1</span></span><br><span class="line"><span class="meta">#CPU_SCALING_MAX_FREQ_ON_AC=0</span></span><br><span class="line">CPU_SCALING_MIN_FREQ_ON_BAT=<span class="number">1</span></span><br><span class="line"><span class="meta">#CPU_SCALING_MAX_FREQ_ON_BAT=0</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Set energy performance hints (HWP) for Intel P-state governor:</span></span><br><span class="line"><span class="meta">#   performance, balance_performance, default, balance_power, power</span></span><br><span class="line"><span class="meta"># Values are given in order of increasing power saving.</span></span><br><span class="line"><span class="meta"># Note: Intel Skylake or newer CPU and Kernel &gt;= 4.10 required.</span></span><br><span class="line">CPU_HWP_ON_AC=balance_performance</span><br><span class="line">CPU_HWP_ON_BAT=balance_power</span><br><span class="line"></span><br><span class="line"><span class="meta"># Set Intel P-state performance: 0..100 (%).</span></span><br><span class="line"><span class="meta"># Limit the max/min P-state to control the power dissipation of the CPU.</span></span><br><span class="line"><span class="meta"># Values are stated as a percentage of the available performance.</span></span><br><span class="line"><span class="meta"># Requires an Intel Core i processor with intel_pstate driver.</span></span><br><span class="line"><span class="meta">#CPU_MIN_PERF_ON_AC=0</span></span><br><span class="line">CPU_MAX_PERF_ON_AC=<span class="number">40</span></span><br><span class="line"><span class="meta">#CPU_MIN_PERF_ON_BAT=0</span></span><br><span class="line">CPU_MAX_PERF_ON_BAT=<span class="number">30</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Set the CPU "turbo boost" feature: 0=disable, 1=allow</span></span><br><span class="line"><span class="meta"># Requires an Intel Core i processor.</span></span><br><span class="line"><span class="meta"># Important:</span></span><br><span class="line"><span class="meta"># - This may conflict with your distribution's governor settings</span></span><br><span class="line"><span class="meta"># - A value of 1 does *not* activate boosting, it just allows it</span></span><br><span class="line"><span class="meta">#CPU_BOOST_ON_AC=1</span></span><br><span class="line"><span class="meta">#CPU_BOOST_ON_BAT=0</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Minimize number of used CPU cores/hyper-threads under light load conditions:</span></span><br><span class="line"><span class="meta">#   0=disable, 1=enable.</span></span><br><span class="line">SCHED_POWERSAVE_ON_AC=<span class="number">1</span></span><br><span class="line">SCHED_POWERSAVE_ON_BAT=<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Kernel NMI Watchdog:</span></span><br><span class="line"><span class="meta">#   0=disable (default, saves power), 1=enable (for kernel debugging only).</span></span><br><span class="line">NMI_WATCHDOG=<span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Change CPU voltages aka "undervolting" - Kernel with PHC patch required.</span></span><br><span class="line"><span class="meta"># Frequency voltage pairs are written to:</span></span><br><span class="line"><span class="meta">#   /sys/devices/system/cpu/cpu0/cpufreq/phc_controls</span></span><br><span class="line"><span class="meta"># CAUTION: only use this, if you thoroughly understand what you are doing!</span></span><br><span class="line"><span class="meta">#PHC_CONTROLS="F:V F:V F:V F:V"</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Set CPU performance versus energy savings policy:</span></span><br><span class="line"><span class="meta">#   performance, balance-performance, default, balance-power, power.</span></span><br><span class="line"><span class="meta"># Values are given in order of increasing power saving.</span></span><br><span class="line"><span class="meta"># Requires kernel module msr and x86_energy_perf_policy from linux-tools.</span></span><br><span class="line">ENERGY_PERF_POLICY_ON_AC=performance</span><br><span class="line">ENERGY_PERF_POLICY_ON_BAT=power</span><br><span class="line"></span><br><span class="line"><span class="meta"># Disk devices; separate multiple devices with spaces (default: sda).</span></span><br><span class="line"><span class="meta"># Devices can be specified by disk ID also (lookup with: tlp diskid).</span></span><br><span class="line">DISK_DEVICES=<span class="string">"sda sdb"</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Disk advanced power management level: 1..254, 255 (max saving, min, off).</span></span><br><span class="line"><span class="meta"># Levels 1..127 may spin down the disk; 255 allowable on most drives.</span></span><br><span class="line"><span class="meta"># Separate values for multiple disks with spaces. Use the special value 'keep'</span></span><br><span class="line"><span class="meta"># to keep the hardware default for the particular disk.</span></span><br><span class="line">DISK_APM_LEVEL_ON_AC=<span class="string">"254 254"</span></span><br><span class="line">DISK_APM_LEVEL_ON_BAT=<span class="string">"128 128"</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Hard disk spin down timeout:</span></span><br><span class="line"><span class="meta">#   0:        spin down disabled</span></span><br><span class="line"><span class="meta">#   1..240:   timeouts from 5s to 20min (in units of 5s)</span></span><br><span class="line"><span class="meta">#   241..251: timeouts from 30min to 5.5 hours (in units of 30min)</span></span><br><span class="line"><span class="meta"># See 'man hdparm' for details.</span></span><br><span class="line"><span class="meta"># Separate values for multiple disks with spaces. Use the special value 'keep'</span></span><br><span class="line"><span class="meta"># to keep the hardware default for the particular disk.</span></span><br><span class="line"><span class="meta">#DISK_SPINDOWN_TIMEOUT_ON_AC="0 0"</span></span><br><span class="line"><span class="meta">#DISK_SPINDOWN_TIMEOUT_ON_BAT="0 0"</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Select IO scheduler for the disk devices: cfq, deadline, noop (Default: cfq).</span></span><br><span class="line"><span class="meta"># Separate values for multiple disks with spaces. Use the special value 'keep'</span></span><br><span class="line"><span class="meta"># to keep the kernel default scheduler for the particular disk.</span></span><br><span class="line"><span class="meta">#DISK_IOSCHED="cfq cfq"</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># AHCI link power management (ALPM) for disk devices:</span></span><br><span class="line"><span class="meta">#   min_power, med_power_with_dipm(*), medium_power, max_performance.</span></span><br><span class="line"><span class="meta"># (*) Kernel &gt;= 4.15 required, then recommended.</span></span><br><span class="line"><span class="meta"># Multiple values separated with spaces are tried sequentially until success.</span></span><br><span class="line">SATA_LINKPWR_ON_AC=<span class="string">"med_power_with_dipm max_performance"</span></span><br><span class="line">SATA_LINKPWR_ON_BAT=<span class="string">"med_power_with_dipm min_power"</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Exclude host devices from AHCI link power management.</span></span><br><span class="line"><span class="meta"># Separate multiple hosts with spaces.</span></span><br><span class="line"><span class="meta">#SATA_LINKPWR_BLACKLIST="host1"</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Runtime Power Management for AHCI host and disks devices:</span></span><br><span class="line"><span class="meta">#   on=disable, auto=enable.</span></span><br><span class="line"><span class="meta"># EXPERIMENTAL ** WARNING: auto will most likely cause system lockups/data loss.</span></span><br><span class="line"><span class="meta">#AHCI_RUNTIME_PM_ON_AC=on</span></span><br><span class="line"><span class="meta">#AHCI_RUNTIME_PM_ON_BAT=on</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Seconds of inactivity before disk is suspended.</span></span><br><span class="line">AHCI_RUNTIME_PM_TIMEOUT=<span class="number">15</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># PCI Express Active State Power Management (PCIe ASPM):</span></span><br><span class="line"><span class="meta">#   default, performance, powersave.</span></span><br><span class="line">PCIE_ASPM_ON_AC=performance</span><br><span class="line">PCIE_ASPM_ON_BAT=powersave</span><br><span class="line"></span><br><span class="line"><span class="meta"># Radeon graphics clock speed (profile method): low, mid, high, auto, default;</span></span><br><span class="line"><span class="meta"># auto = mid on BAT, high on AC; default = use hardware defaults.</span></span><br><span class="line">RADEON_POWER_PROFILE_ON_AC=low</span><br><span class="line">RADEON_POWER_PROFILE_ON_BAT=low</span><br><span class="line"></span><br><span class="line"><span class="meta"># Radeon dynamic power management method (DPM): battery, performance.</span></span><br><span class="line">RADEON_DPM_STATE_ON_AC=performance</span><br><span class="line">RADEON_DPM_STATE_ON_BAT=battery</span><br><span class="line"></span><br><span class="line"><span class="meta"># Radeon DPM performance level: auto, low, high; auto is recommended.</span></span><br><span class="line">RADEON_DPM_PERF_LEVEL_ON_AC=auto</span><br><span class="line">RADEON_DPM_PERF_LEVEL_ON_BAT=auto</span><br><span class="line"></span><br><span class="line"><span class="meta"># WiFi power saving mode: on=enable, off=disable; not supported by all adapters.</span></span><br><span class="line">WIFI_PWR_ON_AC=off</span><br><span class="line">WIFI_PWR_ON_BAT=on</span><br><span class="line"></span><br><span class="line"><span class="meta"># Disable wake on LAN: Y/N.</span></span><br><span class="line">WOL_DISABLE=Y</span><br><span class="line"></span><br><span class="line"><span class="meta"># Enable audio power saving for Intel HDA, AC97 devices (timeout in secs).</span></span><br><span class="line"><span class="meta"># A value of 0 disables, &gt;=1 enables power saving (recommended: 1).</span></span><br><span class="line">SOUND_POWER_SAVE_ON_AC=<span class="number">0</span></span><br><span class="line">SOUND_POWER_SAVE_ON_BAT=<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Disable controller too (HDA only): Y/N.</span></span><br><span class="line">SOUND_POWER_SAVE_CONTROLLER=Y</span><br><span class="line"></span><br><span class="line"><span class="meta"># Power off optical drive in UltraBay/MediaBay: 0=disable, 1=enable.</span></span><br><span class="line"><span class="meta"># Drive can be powered on again by releasing (and reinserting) the eject lever</span></span><br><span class="line"><span class="meta"># or by pressing the disc eject button on newer models.</span></span><br><span class="line"><span class="meta"># Note: an UltraBay/MediaBay hard disk is never powered off.</span></span><br><span class="line">BAY_POWEROFF_ON_AC=<span class="number">0</span></span><br><span class="line">BAY_POWEROFF_ON_BAT=<span class="number">0</span></span><br><span class="line"><span class="meta"># Optical drive device to power off (default sr0).</span></span><br><span class="line">BAY_DEVICE=<span class="string">"sr0"</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Runtime Power Management for PCI(e) bus devices: on=disable, auto=enable.</span></span><br><span class="line">RUNTIME_PM_ON_AC=on</span><br><span class="line">RUNTIME_PM_ON_BAT=auto</span><br><span class="line"></span><br><span class="line"><span class="meta"># Exclude PCI(e) device adresses the following list from Runtime PM</span></span><br><span class="line"><span class="meta"># (separate with spaces). Use lspci to get the adresses (1st column).</span></span><br><span class="line"><span class="meta">#RUNTIME_PM_BLACKLIST="bb:dd.f 11:22.3 44:55.6"</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Exclude PCI(e) devices assigned to the listed drivers from Runtime PM.</span></span><br><span class="line"><span class="meta"># Default when unconfigured is "amdgpu nouveau nvidia radeon" which</span></span><br><span class="line"><span class="meta"># prevents accidential power-on of dGPU in hybrid graphics setups.</span></span><br><span class="line"><span class="meta"># Use "" to disable the feature completely.</span></span><br><span class="line"><span class="meta"># Separate multiple drivers with spaces.</span></span><br><span class="line"><span class="meta">#RUNTIME_PM_DRIVER_BLACKLIST="amdgpu nouveau nvidia radeon"</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Set to 0 to disable, 1 to enable USB autosuspend feature.</span></span><br><span class="line">USB_AUTOSUSPEND=<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Exclude listed devices from USB autosuspend (separate with spaces).</span></span><br><span class="line"><span class="meta"># Use lsusb to get the ids.</span></span><br><span class="line"><span class="meta"># Note: input devices (usbhid) are excluded automatically</span></span><br><span class="line"><span class="meta">#USB_BLACKLIST="1111:2222 3333:4444"</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Bluetooth devices are excluded from USB autosuspend:</span></span><br><span class="line"><span class="meta">#   0=do not exclude, 1=exclude.</span></span><br><span class="line">USB_BLACKLIST_BTUSB=<span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Phone devices are excluded from USB autosuspend:</span></span><br><span class="line"><span class="meta">#   0=do not exclude, 1=exclude (enable charging).</span></span><br><span class="line">USB_BLACKLIST_PHONE=<span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Printers are excluded from USB autosuspend:</span></span><br><span class="line"><span class="meta">#   0=do not exclude, 1=exclude.</span></span><br><span class="line">USB_BLACKLIST_PRINTER=<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># WWAN devices are excluded from USB autosuspend:</span></span><br><span class="line"><span class="meta">#   0=do not exclude, 1=exclude.</span></span><br><span class="line">USB_BLACKLIST_WWAN=<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Include listed devices into USB autosuspend even if already excluded</span></span><br><span class="line"><span class="meta"># by the blacklists above (separate with spaces).</span></span><br><span class="line"><span class="meta"># Use lsusb to get the ids.</span></span><br><span class="line"><span class="meta">#USB_WHITELIST="1111:2222 3333:4444"</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Set to 1 to disable autosuspend before shutdown, 0 to do nothing</span></span><br><span class="line"><span class="meta"># (workaround for USB devices that cause shutdown problems).</span></span><br><span class="line"><span class="meta">#USB_AUTOSUSPEND_DISABLE_ON_SHUTDOWN=1</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Restore radio device state (Bluetooth, WiFi, WWAN) from previous shutdown</span></span><br><span class="line"><span class="meta"># on system startup: 0=disable, 1=enable.</span></span><br><span class="line"><span class="meta"># Hint: the parameters DEVICES_TO_DISABLE/ENABLE_ON_STARTUP/SHUTDOWN below</span></span><br><span class="line"><span class="meta">#   are ignored when this is enabled!</span></span><br><span class="line">RESTORE_DEVICE_STATE_ON_STARTUP=<span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Radio devices to disable on startup: bluetooth, wifi, wwan.</span></span><br><span class="line"><span class="meta"># Separate multiple devices with spaces.</span></span><br><span class="line"><span class="meta">#DEVICES_TO_DISABLE_ON_STARTUP="bluetooth wifi wwan"</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Radio devices to enable on startup: bluetooth, wifi, wwan.</span></span><br><span class="line"><span class="meta"># Separate multiple devices with spaces.</span></span><br><span class="line"><span class="meta">#DEVICES_TO_ENABLE_ON_STARTUP="wifi"</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Radio devices to disable on shutdown: bluetooth, wifi, wwan.</span></span><br><span class="line"><span class="meta"># (workaround for devices that are blocking shutdown).</span></span><br><span class="line"><span class="meta">#DEVICES_TO_DISABLE_ON_SHUTDOWN="bluetooth wifi wwan"</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Radio devices to enable on shutdown: bluetooth, wifi, wwan.</span></span><br><span class="line"><span class="meta"># (to prevent other operating systems from missing radios).</span></span><br><span class="line"><span class="meta">#DEVICES_TO_ENABLE_ON_SHUTDOWN="wwan"</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Radio devices to enable on AC: bluetooth, wifi, wwan.</span></span><br><span class="line"><span class="meta">#DEVICES_TO_ENABLE_ON_AC="bluetooth wifi wwan"</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Radio devices to disable on battery: bluetooth, wifi, wwan.</span></span><br><span class="line"><span class="meta">#DEVICES_TO_DISABLE_ON_BAT="bluetooth wifi wwan"</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Radio devices to disable on battery when not in use (not connected):</span></span><br><span class="line"><span class="meta">#   bluetooth, wifi, wwan.</span></span><br><span class="line"><span class="meta">#DEVICES_TO_DISABLE_ON_BAT_NOT_IN_USE="bluetooth wifi wwan"</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Battery charge thresholds (ThinkPad only, tp-smapi or acpi-call kernel module</span></span><br><span class="line"><span class="meta"># required). Charging starts when the remaining capacity falls below the</span></span><br><span class="line"><span class="meta"># START_CHARGE_THRESH value and stops when exceeding the STOP_CHARGE_THRESH value.</span></span><br><span class="line"><span class="meta"># Main / Internal battery (values in %)</span></span><br><span class="line"><span class="meta">#START_CHARGE_THRESH_BAT0=75</span></span><br><span class="line"><span class="meta">#STOP_CHARGE_THRESH_BAT0=80</span></span><br><span class="line"><span class="meta"># Ultrabay / Slice / Replaceable battery (values in %)</span></span><br><span class="line"><span class="meta">#START_CHARGE_THRESH_BAT1=75</span></span><br><span class="line"><span class="meta">#STOP_CHARGE_THRESH_BAT1=80</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Restore charge thresholds when AC is unplugged: 0=disable, 1=enable.</span></span><br><span class="line"><span class="meta">#RESTORE_THRESHOLDS_ON_BAT=1</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># ------------------------------------------------------------------------------</span></span><br><span class="line"><span class="meta"># tlp-rdw - Parameters for the radio device wizard</span></span><br><span class="line"><span class="meta"># Possible devices: bluetooth, wifi, wwan.</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Hints:</span></span><br><span class="line"><span class="meta"># - Parameters are disabled by default, remove the leading # to enable them</span></span><br><span class="line"><span class="meta"># - Separate multiple radio devices with spaces</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Radio devices to disable on connect.</span></span><br><span class="line"><span class="meta">#DEVICES_TO_DISABLE_ON_LAN_CONNECT="wifi wwan"</span></span><br><span class="line"><span class="meta">#DEVICES_TO_DISABLE_ON_WIFI_CONNECT="wwan"</span></span><br><span class="line"><span class="meta">#DEVICES_TO_DISABLE_ON_WWAN_CONNECT="wifi"</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Radio devices to enable on disconnect.</span></span><br><span class="line"><span class="meta">#DEVICES_TO_ENABLE_ON_LAN_DISCONNECT="wifi wwan"</span></span><br><span class="line"><span class="meta">#DEVICES_TO_ENABLE_ON_WIFI_DISCONNECT=""</span></span><br><span class="line"><span class="meta">#DEVICES_TO_ENABLE_ON_WWAN_DISCONNECT=""</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Radio devices to enable/disable when docked.</span></span><br><span class="line"><span class="meta">#DEVICES_TO_ENABLE_ON_DOCK=""</span></span><br><span class="line"><span class="meta">#DEVICES_TO_DISABLE_ON_DOCK=""</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Radio devices to enable/disable when undocked.</span></span><br><span class="line"><span class="meta">#DEVICES_TO_ENABLE_ON_UNDOCK="wifi"</span></span><br><span class="line"><span class="meta">#DEVICES_TO_DISABLE_ON_UNDOCK=""</span></span><br></pre></td></tr></table></figure></p><h2 id="显卡配置"><a href="#显卡配置" class="headerlink" title="显卡配置"></a>显卡配置</h2><p>因为跑深度学习的代码需要用很多N卡配置，而默认不使用tlp的话电脑会很热，用了tlp系统会自动的把显卡切换到intel的集成显卡，这个问题一开始苦恼了我好久。其实ubuntu里面可以手动切换显卡的。前提是需要正确安装电脑对应的显卡，我们需要在N卡的驱动附加软件里面实现这个显卡切换功能。ps.Linux上显卡驱动真的是个大坑~<br><img src="/images/pasted-14.png" alt="upload successful">可以看到我现在用的是intel的集成显卡，而实际上我是有N卡的。</p><h3 id="正确安装对应驱动"><a href="#正确安装对应驱动" class="headerlink" title="正确安装对应驱动"></a>正确安装对应驱动</h3><p>如果我们刚安装完ubuntu新系统的话，我们正常在上图看到的显卡类型应该是<strong>nouveau</strong>的驱动，nouveau是一个自由及开放源代码显卡驱动程序，是为Nvidia的显示卡所编写，也可用于属于系统芯片的NVIDIA Tegra系列，该项目的目标为利用逆向工程Nvidia的专有Linux驱动程序来创造一个开放源代码的驱动程序，所以nouveau开源驱动基本上是不能正常使用的。<br>首先查看电脑目前有什么显卡，使用如下命令：<br><figure class="highlight perl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">lspci | <span class="keyword">grep</span> VGA     <span class="comment"># 查看集成显卡</span></span><br><span class="line">lspci | <span class="keyword">grep</span> NVIDIA  <span class="comment"># 查看NVIDIA显卡</span></span><br></pre></td></tr></table></figure></p><p>我的电脑是小米pro，可以看出显卡是GeForce MX150的。<br><img src="/images/pasted-16.png" alt="upload successful"></p><p>下面查看应该安装什么版本的驱动，如下命令：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ubuntu-drivers devices   <span class="comment"># 查询所有ubuntu推荐的驱动</span></span><br></pre></td></tr></table></figure><p>可以看出ubutnu提示我应该安装390版本的驱动。<br><img src="/images/pasted-17.png" alt="upload successful"></p><p>然后就可以使用下面条命令安装所有推荐的驱动程序：</p><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">sudo ubuntu-drivers autoinstall</span></span><br></pre></td></tr></table></figure><p>或者<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-<span class="builtin-name">get</span> install nvidia-390</span><br></pre></td></tr></table></figure></p><p>安装完成后重启，在<strong>软件和更新</strong>中我们可以看到已经安装了哪些驱动：<br><img src="/images/pasted-18.png" alt="upload successful"><br>可以看到我已经安装了390和nouveau这两个驱动，勾选390那个就行。</p><h3 id="切换显卡"><a href="#切换显卡" class="headerlink" title="切换显卡"></a>切换显卡</h3><p>随后我们就可以使用使用nvidia-settings命令了：<br><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">nvidia-settings</span></span><br></pre></td></tr></table></figure></p><p>我们可以在这里选择我们要使用哪个显卡，切换之后reboot就好了。<br><img src="/images/pasted-19.png" alt="upload successful"></p><p>还有最后一点注意的！！！！！！！！！！！！！</p><p><strong>如果安装cuda的话，一定不再选择安装驱动，要不然就前功尽弃啦！！！</strong></p><h1 id="美化系统主题"><a href="#美化系统主题" class="headerlink" title="美化系统主题"></a>美化系统主题</h1><p>作为一个颜控党，主题这种东西如果不和为空是万万不可以的。<br>从ubuntu17.10开始，官方又开始使用gnome作为默认的桌面环境，意味着我们可以轻松的使用GNOME Shell扩展了，美滋滋。先放几张图：<br><img src="/images/pasted-20.png" alt="upload successful"></p><p><img src="/images/pasted-21.png" alt="upload successful"></p><p><img src="/images/pasted-22.png" alt="upload successful">搞了个北欧风，十分舒服～～</p><p>首先我们要安装GNOME Tweak Tool。</p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt <span class="keyword">install</span> gnome-tweak-tool</span><br></pre></td></tr></table></figure><p>安装之后你就能找到这个东西：</p><p><img src="/images/pasted-23.png" alt="upload successful"><br>打开：<br><img src="/images/pasted-24.png" alt="upload successful"></p><p>在这里可以配置不同的外观，也可以安装不同的扩展小插件，在这里我已经安装好配置主题需要的扩展了，正常来说我们需要先安装User themes这个扩展。<br><img src="/images/pasted-25.png" alt="upload successful"><br>我们可通过chrome来方便的安装任何扩展，首先我们需要在chrome里面安装GNOME的小程序：!<br><img src="/images/pasted-26.png" alt="upload successful"></p><p>随后浏览器打开<a href="https://extensions.gnome.org/" target="_blank" rel="noopener">https://extensions.gnome.org/</a> 安装这个扩展：<br><img src="/images/pasted-27.png" alt="upload successful"><br>点进去，把开关调成on状态就好了：<br><img src="/images/pasted-28.png" alt="upload successful"><br>随后我们就能使用主题包来装饰我们的系统了。</p><p>对于Gnome桌面，你最需要连接的就是这个<a href="https://www.gnome-look.org/" target="_blank" rel="noopener">网站</a>，它提供了包括主题、图标、字体等在内的很多包。<br>因为在上述中安装了User Themes 扩展，所以我们可以把下载好的主题放置在自己的家目录下，为此，在家目录下的.local/share中新建themes、fonts、icons 三个文件夹，分别存放主题、字体和图标 。下载我们喜欢的主题和图标之后（一般是一个压缩包），我们需要把压缩包放到对应的文件夹里，并且进行解压缩，这里提供几个常用的命令：<br><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">//解压</span><br><span class="line">$ xz -d <span class="keyword">node</span><span class="title">-v6</span>.<span class="number">10.1</span>-linux-x64.tar.xz</span><br><span class="line">$ tar -xvf <span class="keyword">node</span><span class="title">-v6</span>.<span class="number">10.1</span>-linux-x64.tar</span><br><span class="line"></span><br><span class="line">//移动文件</span><br><span class="line">cp -r 文件 ~/.local/share/icons</span><br><span class="line"></span><br><span class="line">//本机gnome主题、字体、图标地址(https://www.gnome-look.org/)</span><br><span class="line">~/.local/share/themes</span><br><span class="line">~/.local/share/fonts</span><br><span class="line">~/.local/share/icons</span><br></pre></td></tr></table></figure></p><p>上面的步骤完成后，我们就可以在这里进行修改了：<br><img src="/images/pasted-29.png" alt="upload successful"></p><h1 id="美化grub启动项"><a href="#美化grub启动项" class="headerlink" title="美化grub启动项"></a>美化grub启动项</h1><p>个人是不太喜欢紫色的，尤其开机grub引导项的那个基佬紫，这的受不了啊，要想个办法改一下。<br>这个部分我主要参考的是这个<a href="https://blog.csdn.net/w84963568/article/details/78884003" target="_blank" rel="noopener">博主</a>的文章，所以不过多的介绍了。</p><h1 id="基础命令操作"><a href="#基础命令操作" class="headerlink" title="基础命令操作"></a>基础命令操作</h1>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;喜欢Ubuntu的一个好处就是相比win来说Linux可以更方便的对系统进行一些修改，而且安装很多东西很方便，相比mac可以充分利用硬件（mac没有N卡，配置深度学习的时候不能用显卡跑代码）。作为一个颜控，最重要的当然是Ubuntu界面很舒服很漂亮～～～～&lt;/p&gt;
&lt;p&gt;自
      
    
    </summary>
    
      <category term="Ubuntu" scheme="http://yoursite.com/categories/Ubuntu/"/>
    
    
      <category term="Ubuntu" scheme="http://yoursite.com/tags/Ubuntu/"/>
    
  </entry>
  
  <entry>
    <title>交叉熵的数学原理及应用——Pytorch中的CrossEntropyLoss()函数</title>
    <link href="http://yoursite.com/2019/02/15/%E4%BA%A4%E5%8F%89%E7%86%B5%E7%9A%84%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86%E5%8F%8A%E5%BA%94%E7%94%A8%E2%80%94%E2%80%94Pytorch%E4%B8%AD%E7%9A%84CrossEntropyLoss()%E5%87%BD%E6%95%B0/"/>
    <id>http://yoursite.com/2019/02/15/交叉熵的数学原理及应用——Pytorch中的CrossEntropyLoss()函数/</id>
    <published>2019-02-15T10:47:00.000Z</published>
    <updated>2019-03-18T16:31:04.383Z</updated>
    
    <content type="html"><![CDATA[<p>分类问题中，交叉熵函数是比较常用也是比较基础的损失函数，原来就是了解，但一直搞不懂他是怎么来的？为什么交叉熵能够表征真实样本标签和预测概率之间的差值？趁着这次学习把这些概念系统学习了一下。</p><h1 id="交叉熵的数学原理"><a href="#交叉熵的数学原理" class="headerlink" title="交叉熵的数学原理"></a>交叉熵的数学原理</h1><p>首先说起交叉熵，脑子里就会出现这个东西：<br>$$L=-[y\log{\hat{y}}+(1-y)\log{(1-\hat{y})}]<br>$$<br>随后我们脑子里可能还会出现Sigmoid()这个函数:<br>$$<br>{g(s)}=\frac{1}{1+e^{-s}}<br>$$<br>pytorch中的CrossEntropyLoss()函数实际就是先把输出结果进行sigmoid，随后再放到传统的交叉熵函数中，就会得到结果。<br>那我们就先从sigmoid开始说起，我们知道sigmoid的作用其实是把前一层的输入映射到0~1这个区间上，可以认为上一层某个样本的输入数据越大，就代表这个样本标签属于1的概率就越大，反之，上一层某样本的输入数据越小，这个样本标签属于0的概率就越大，而且通过sigmoid函数的图像我们可以看出来，随着输入数值的增大，其对概率增大的作用效果是逐渐减弱的，反之同理，这就是非线性映射的一个好处，让模型对处于中间范围的输入数据更敏感。下面是sigmoid函数图：</p><p><img src="/images/pasted-10.png" width="40%" height="50%"></p><p>既然经过sigmoid之后的数据能表示样本所属某个标签的概率，那么举个例子，我们模型预测某个样本标签为1的概率是：<br>$$<br>\hat{y}=P(y=1|x)<br>$$<br>那么自然的，这个样本标签不为1的概率是：<br>$$<br>1-\hat{y}=P(y=0|x)<br>$$<br>从极大似然的角度来说就是：<br>$$<br>P(y|x)=\hat{y}^{y}(1-\hat{y})^{1-y}<br>$$</p><p>上式可以理解为，某一个样本x，我们通过模型预测出其属于样本标签为y的概率，因为y是我们给的正确结果，所以我们当然希望上式越大越好。</p><p>下一步我们要在<strong>P(y|x)</strong>  的外面套上一层log函数，相当于进行了一次非线性的映射。log函数是不会改变单调性的，所以我们也希望<strong>log(P(y|x))</strong> 越大越好。<br>$$<br>\log{(P(y|x))}=\log{(\hat{y}^{y}(1-\hat{y})^{1-y})}=y\log{\hat{y}}+(1-y)\log{(1-\hat{y})}<br>$$<br>这样，就得到了我们一开始说的交叉熵的形式了，但是等一等，好像还差一个符号。</p><p>因为一般来说我们相用上述公式做loss函数来使用，所以我们想要loss越小越好，这样符合我们的直观理解，所以我们只要<strong>-log(P(y|x))</strong> 就达到了我们的目的。<br>$$<br>L=-[y\log{\hat{y}}+(1-y)\log{(1-\hat{y})}]<br>$$<br>上面是二分类问题的交叉熵，如果是有多分类，就对每个标签类别下的可能概率分别求相应的负log对数然后求和就好了：<br>$$<br>L=-\sum_{i=1}^n y^{(i)}\log{\hat{y}^{(i)}}<br>$$<br>是不是突然也感觉有些理解了，(<em>^__^</em>) ……</p><h1 id="Pytorch中的函数-CrossEntropyLoss"><a href="#Pytorch中的函数-CrossEntropyLoss" class="headerlink" title="Pytorch中的函数 CrossEntropyLoss()"></a>Pytorch中的函数 CrossEntropyLoss()</h1><p>上面是对交叉熵进行了推导，下面要结合pytorch中的函数 CrossEntropyLoss()  来说一说具体怎么使用了。</p><p>举个小例子，假设我们有个一样本，他经过我们的神经网络后会输出一个5维的向量，分别代表这个样本分别属于这5种标签的数值（注意此时我们的5个数求和还并不等于1，需要先经过softmax处理，下面会说），我们还会从数据集中得到该样本的正确分类结果，下面我们要把经过神经网络的5维向量和正确的分类结果放到CrossEntropyLoss() 中，看看会发生什么：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line">loss = nn.CrossEntropyLoss()</span><br><span class="line">input = torch.randn(<span class="number">1</span>,<span class="number">5</span>,requires_grad=<span class="keyword">True</span>)</span><br><span class="line">target = torch.empty(<span class="number">1</span>,dtype=torch.long).random_(<span class="number">5</span>)</span><br><span class="line">output = loss(input,target)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"输入为5类："</span>)</span><br><span class="line">print(input)</span><br><span class="line">print(<span class="string">"要计算的loss的类别："</span>)</span><br><span class="line">print(target)</span><br><span class="line">print(<span class="string">"要计算的loss的结果："</span>)</span><br><span class="line">print(output)</span><br><span class="line"></span><br><span class="line">first = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>):</span><br><span class="line">first -= input[i][target[i]]</span><br><span class="line">second = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>):</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">second += math.exp(input[i][j])</span><br><span class="line">res = <span class="number">0</span></span><br><span class="line">res += first + printmath.log(second)</span><br><span class="line">print(<span class="string">"手动的计算结果"</span>)</span><br><span class="line">print(res)</span><br></pre></td></tr></table></figure></p><p>看一看我们的input和target：</p><p><img src="/images/pasted-11.png" width="90%" height="50%"></p><p>可以看到我们的target就是一个只有一个数的数组形式（不是向量，不是矩阵，只是一个简单的数组，而且里面就一个数），input是一个5维的向量，但这，在计算交叉熵之前，我们需要先获得下面交叉熵公式的$\hat{y}^{(i)}$.<br>$$<br>L=-\sum_{i=1}^n y^{(i)}\log{\hat{y}^{(i)}}<br>$$<br>此处的$\hat{y}^{(i)}$需要我们将输入的input向量进行softmax处理，使得input变成对应属于每个标签的概率值，对每个<strong>input[i]</strong> 进行如下处理：<br>$$<br>\hat{y}=P(\hat{y}=i|x)=\frac{e^{input[i]}}{ \sum_{j=0}^ne^{input[j]} }<br>$$</p><p>这样我们就得到了交叉熵公式中$\hat{y}^{(i)}$</p><p>随后我们就可以把$\hat{y}^{(i)}$带入公式了，下面我们还缺$y^{(i)}$就可以了，而奇怪的是我们输入的target是一个只有一个数的数组啊，而$\hat{y}^{(i)}$是一个5维的向量，这什么情况？</p><p>原来CrossEntropyLoss() 会把target变成ont-hot形式（网上别人说的，想等有时间去看看函数的源代码随后补充一下这里），我们现在例子的样本标签是【4】（从0开始计算）。那么转换成one-hot编码就是【0，0，0，0，1】，所以我们的$y^{(i)}$最后也会变成一个5维的向量的向量，并且不是该样本标签的数值为0，这样我们在计算交叉熵的时候只计算$y^{(i)}$给定的那一项的sorce就好了，所以我们的公式最后变成了：<br>$$<br>L(input,target)=-\log{\frac{e^{input[target]}}{ \sum_{j=0}^ne^{input[j]} }}=-input[target]+\log{(\sum_{j=0}^ne^{input[j]})}<br>$$</p><p>好，安装上面我们的推导来运行一下程序：</p><p><img src="/images/pasted-12.png" alt="upload successful"></p><p>破发科特<del>~</del>~<br>开学快乐(<em>^__^</em>) ……</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;分类问题中，交叉熵函数是比较常用也是比较基础的损失函数，原来就是了解，但一直搞不懂他是怎么来的？为什么交叉熵能够表征真实样本标签和预测概率之间的差值？趁着这次学习把这些概念系统学习了一下。&lt;/p&gt;
&lt;h1 id=&quot;交叉熵的数学原理&quot;&gt;&lt;a href=&quot;#交叉熵的数学原理&quot; 
      
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Pytorch" scheme="http://yoursite.com/tags/Pytorch/"/>
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Pytorch中的squeeze()和unsqueeze()函数</title>
    <link href="http://yoursite.com/2019/02/14/Pytorch%E4%B8%AD%E7%9A%84squeeze-%E5%92%8Cunsqueeze-%E5%87%BD%E6%95%B0/"/>
    <id>http://yoursite.com/2019/02/14/Pytorch中的squeeze-和unsqueeze-函数/</id>
    <published>2019-02-14T14:37:00.000Z</published>
    <updated>2019-03-18T16:31:04.383Z</updated>
    
    <content type="html"><![CDATA[<p>在numpy库中，经常会出现“秩为1的一维数组”（come from 吴恩达的深度学习，目前还没有搞清楚numpy中如此设计的意图）。比如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.rand(<span class="number">3</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.shape</span><br><span class="line">torch.Size([<span class="number">3</span>])</span><br></pre></td></tr></table></figure><p>注意这里的a的shape是[3] ，既不是 [1,3] 也不是 [3,1]。这就说明它既不是行向量也不是列向量，只是一个数组。</p><p>但是我们可以用squeeze（）和unsqueeze（）对其进行操作，比如：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.rand(<span class="number">3</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.shape</span><br><span class="line">torch.Size([<span class="number">3</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = a.unsqueeze(<span class="number">1</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b.shape</span><br><span class="line">torch.Size([<span class="number">3</span>,<span class="number">1</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>c = a.unsqueeze(<span class="number">0</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>c.shape</span><br><span class="line">torch.Size([<span class="number">1</span>,<span class="number">3</span>])</span><br></pre></td></tr></table></figure></p><p>在对这两个函数讲解之前，我们先统一一下定义:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x.shape</span><br><span class="line">torch.Size([a,b,c])</span><br></pre></td></tr></table></figure></p><p>这里我们说a是第一个维度（表示第一个维度的数量是a），b是第二个维度（表示第一个维度的数量是b），c是第三个维度（表示第三个维度的数量是c）。</p><p>python中很多函数都是这样表示<strong>“维度”</strong>这个概念的,比如numpy.stack()。</p><p>简而言之，unsqueeze（arg）是增添第arg个维度为1，以插入的形式填充。比如：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.rand(<span class="number">1</span>,<span class="number">3</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.shape</span><br><span class="line">torch.Size([<span class="number">1</span>,<span class="number">3</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = a.unsqueeze(<span class="number">1</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b.shape</span><br><span class="line">torch.Size([<span class="number">1</span>,<span class="number">1</span>,<span class="number">3</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b</span><br><span class="line">tensor([[[<span class="number">0.6289</span>,<span class="number">0.0814</span>,<span class="number">0.9177</span>]]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">tensor([[<span class="number">0.6289</span>,<span class="number">0.0814</span>,<span class="number">0.9177</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>c = a.unsqueeze(<span class="number">0</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>c.shape</span><br><span class="line">torch.Size([<span class="number">1</span>,<span class="number">1</span>,<span class="number">3</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>d = a.unsqueeze(<span class="number">2</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>d.shape</span><br><span class="line">torch.Size([<span class="number">1</span>,<span class="number">3</span>,<span class="number">1</span>])</span><br></pre></td></tr></table></figure></p><p>相反，squeeze（arg）是删除第arg个维度(如果当前维度不为1，则不会进行删除)，比如：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>d.shape</span><br><span class="line">torch.Size([<span class="number">1</span>,<span class="number">3</span>,<span class="number">1</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>e = d.squeeze(<span class="number">0</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>e.shape</span><br><span class="line">torch.Size([<span class="number">3</span>,<span class="number">1</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>e</span><br><span class="line">tensor([[<span class="number">0</span>,<span class="number">6289</span>],</span><br><span class="line">[<span class="number">0.0814</span>],</span><br><span class="line">        [<span class="number">0.9177</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>f = d.squeeze(<span class="number">1</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>f.shape</span><br><span class="line">torch.Size([<span class="number">1</span>,<span class="number">3</span>,<span class="number">1</span>])</span><br></pre></td></tr></table></figure></p><p>最后，奥哈哈，情人节快乐～～！！！<strong>(*^__^*) ……</strong></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在numpy库中，经常会出现“秩为1的一维数组”（come from 吴恩达的深度学习，目前还没有搞清楚numpy中如此设计的意图）。比如：&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;
      
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Pytorch" scheme="http://yoursite.com/tags/Pytorch/"/>
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Pytorch中的torch.cat()函数</title>
    <link href="http://yoursite.com/2019/02/09/Pytorch%E4%B8%AD%E7%9A%84torch-cat-%E5%87%BD%E6%95%B0/"/>
    <id>http://yoursite.com/2019/02/09/Pytorch中的torch-cat-函数/</id>
    <published>2019-02-09T03:53:00.000Z</published>
    <updated>2019-03-18T16:31:04.383Z</updated>
    
    <content type="html"><![CDATA[<p>cat是concatnate的意思：拼接，联系在一起。</p><hr><p>先说cat( )的普通用法<br>如果我们有两个tensor是A和B，想把他们拼接在一起，需要如下操作：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">C = torch.cat( (A,B),<span class="number">0</span> )  <span class="comment">#按维数0拼接（竖着拼）</span></span><br><span class="line">C = torch.cat( (A,B),<span class="number">1</span> )  <span class="comment">#按维数1拼接（横着拼）</span></span><br></pre></td></tr></table></figure></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> torch</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>A=torch.ones(<span class="number">2</span>,<span class="number">3</span>)    <span class="comment">#2x3的张量（矩阵）                                     </span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>A</span><br><span class="line">tensor([[ <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>],</span><br><span class="line">        [ <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>B=<span class="number">2</span>*torch.ones(<span class="number">4</span>,<span class="number">3</span>)  <span class="comment">#4x3的张量（矩阵）                                    </span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>B</span><br><span class="line">tensor([[ <span class="number">2.</span>,  <span class="number">2.</span>,  <span class="number">2.</span>],</span><br><span class="line">        [ <span class="number">2.</span>,  <span class="number">2.</span>,  <span class="number">2.</span>],</span><br><span class="line">        [ <span class="number">2.</span>,  <span class="number">2.</span>,  <span class="number">2.</span>],</span><br><span class="line">        [ <span class="number">2.</span>,  <span class="number">2.</span>,  <span class="number">2.</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>C=torch.cat((A,B),<span class="number">0</span>)  <span class="comment">#按维数0（行）拼接</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>C</span><br><span class="line">tensor([[ <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>],</span><br><span class="line">         [ <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>],</span><br><span class="line">         [ <span class="number">2.</span>,  <span class="number">2.</span>,  <span class="number">2.</span>],</span><br><span class="line">         [ <span class="number">2.</span>,  <span class="number">2.</span>,  <span class="number">2.</span>],</span><br><span class="line">         [ <span class="number">2.</span>,  <span class="number">2.</span>,  <span class="number">2.</span>],</span><br><span class="line">         [ <span class="number">2.</span>,  <span class="number">2.</span>,  <span class="number">2.</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>C.size()</span><br><span class="line">torch.Size([<span class="number">6</span>, <span class="number">3</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>D=<span class="number">2</span>*torch.ones(<span class="number">2</span>,<span class="number">4</span>) <span class="comment">#2x4的张量（矩阵）</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>C=torch.cat((A,D),<span class="number">1</span>)<span class="comment">#按维数1（列）拼接</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>C</span><br><span class="line">tensor([[ <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">2.</span>,  <span class="number">2.</span>,  <span class="number">2.</span>,  <span class="number">2.</span>],</span><br><span class="line">        [ <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">2.</span>,  <span class="number">2.</span>,  <span class="number">2.</span>,  <span class="number">2.</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>C.size()</span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">7</span>])</span><br></pre></td></tr></table></figure><p>其次，cat还可以把list中的tensor拼接起来。<br>比如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.Tensor([ [<span class="number">1</span>],[<span class="number">2</span>],[<span class="number">3</span>] ])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x1 = [ x*<span class="number">2</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">4</span>) ]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x.shape</span><br><span class="line">torch.Size([<span class="number">3</span>,<span class="number">1</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>len(x1)</span><br><span class="line"><span class="number">3</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x1</span><br><span class="line">[tensor( [[<span class="number">2.</span>],</span><br><span class="line">  [<span class="number">4.</span>],</span><br><span class="line">          [<span class="number">6.</span>]] ),tensor( [[<span class="number">2.</span>],</span><br><span class="line">          [<span class="number">4.</span>],</span><br><span class="line">          [<span class="number">6.</span>]] ),tensor( [[<span class="number">2.</span>],</span><br><span class="line">          [<span class="number">4.</span>],</span><br><span class="line">          [<span class="number">6.</span>]] )]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x2 = torch.cat( (x1),<span class="number">1</span> )</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x2</span><br><span class="line">tensor([[<span class="number">2.</span>,<span class="number">2.</span>,<span class="number">2.</span>],</span><br><span class="line">[<span class="number">4.</span>,<span class="number">4.</span>,<span class="number">4.</span>],</span><br><span class="line">        [<span class="number">6.</span>,<span class="number">6.</span>,<span class="number">6.</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>type(x1)</span><br><span class="line">list</span><br></pre></td></tr></table></figure><p>上面的代码可以合成一行来写：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x2 = torch.cat( [x*<span class="number">2</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">4</span>)],<span class="number">1</span> )</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x2</span><br><span class="line">tensor([[<span class="number">2.</span>,<span class="number">2.</span>,<span class="number">2.</span>],</span><br><span class="line">[<span class="number">4.</span>,<span class="number">4.</span>,<span class="number">4.</span>],</span><br><span class="line">        [<span class="number">6.</span>,<span class="number">6.</span>,<span class="number">6.</span>]])</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;cat是concatnate的意思：拼接，联系在一起。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;先说cat( )的普通用法&lt;br&gt;如果我们有两个tensor是A和B，想把他们拼接在一起，需要如下操作：&lt;br&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;
      
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Pytorch" scheme="http://yoursite.com/tags/Pytorch/"/>
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Pytorch中的自动求导函数backward()所需参数的含义</title>
    <link href="http://yoursite.com/2019/02/01/%E4%BB%8A%E5%A4%A9%E6%88%91%E5%8F%91%E5%B8%83%E4%BA%86%E6%96%B0%E7%9A%84%E5%8D%9A%E5%AE%A2/"/>
    <id>http://yoursite.com/2019/02/01/今天我发布了新的博客/</id>
    <published>2019-02-01T11:53:00.000Z</published>
    <updated>2019-03-18T16:31:04.383Z</updated>
    
    <content type="html"><![CDATA[<p>正常来说backward( )函数是要传入参数的，一直没弄明白backward需要传入的参数具体含义，但是没关系，生命在与折腾，咱们来折腾一下，嘿嘿。</p><h1 id="对标量自动求导"><a href="#对标量自动求导" class="headerlink" title="对标量自动求导"></a>对标量自动求导</h1><p>首先，如果out.backward()中的out是一个标量的话（相当于一个神经网络有一个样本，这个样本有两个属性，神经网络有一个输出）那么此时我的backward函数是不需要输入任何参数的。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line">    </span><br><span class="line">a = Variable(torch.Tensor([<span class="number">2</span>,<span class="number">3</span>]),requires_grad=<span class="keyword">True</span>)</span><br><span class="line">b = a + <span class="number">3</span></span><br><span class="line">c = b * <span class="number">3</span></span><br><span class="line">out = c.mean()</span><br><span class="line">out.backward()</span><br><span class="line">print(<span class="string">'input:'</span>)</span><br><span class="line">print(a.data)</span><br><span class="line">print(<span class="string">'output:'</span>)</span><br><span class="line">print(out.data.item())</span><br><span class="line">print(<span class="string">'input gradients are:'</span>)</span><br><span class="line">print(a.grad)</span><br></pre></td></tr></table></figure></p><p>运行结果：<br><img src="/images/pasted-3.png" alt="upload successful"><br>不难看出，我们构建了这样的一个函数：<br>$$out=3[(a_1+3)+(a_2+3)]\over2$$<br>所以其求导也很容易看出：<br>$${\partial out \over \partial a_1}={\partial out \over \partial a_2 }={3\over2}  $$<br>这是对其进行标量自动求导的结果.</p><h1 id="对向量自动求导"><a href="#对向量自动求导" class="headerlink" title="对向量自动求导"></a>对向量自动求导</h1><p>如果out.backward()中的out是一个向量（或者理解成1xN的矩阵）的话，我们对向量进行自动求导，看看会发生什么？</p><p>先构建这样的一个模型（相当于一个神经网络有一个样本，这个样本有两个属性，神经网络有两个输出）：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line">    </span><br><span class="line">a = Variable(torch.Tensor([[<span class="number">2.</span>,<span class="number">4.</span>]]),requires_grad=<span class="keyword">True</span>)</span><br><span class="line">b = torch.zeros(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">b[<span class="number">0</span>,<span class="number">0</span>] = a[<span class="number">0</span>,<span class="number">0</span>] ** <span class="number">2</span> </span><br><span class="line">b[<span class="number">0</span>,<span class="number">1</span>] = a[<span class="number">0</span>,<span class="number">1</span>] ** <span class="number">3</span> </span><br><span class="line">out = <span class="number">2</span> * b</span><br><span class="line"><span class="comment">#其参数要传入和out维度一样的矩阵</span></span><br><span class="line">out.backward(torch.FloatTensor([[<span class="number">1.</span>,<span class="number">1.</span>]]))</span><br><span class="line">print(<span class="string">'input:'</span>)</span><br><span class="line">print(a.data)</span><br><span class="line">print(<span class="string">'output:'</span>)</span><br><span class="line">print(out.data)</span><br><span class="line">print(<span class="string">'input gradients are:'</span>)</span><br><span class="line">print(a.grad)</span><br></pre></td></tr></table></figure></p><p>模型也很简单，不难看出out求导出来的雅克比应该是：<br>$$\begin{pmatrix}<br>{\partial out_1 \over \partial a_1}=4a_1&amp;{\partial out_1 \over \partial a_2}=0\\<br>{\partial out_2 \over \partial a_1}=0&amp;{\partial out_2 \over \partial a_2}=6a_2^2\\<br>\end{pmatrix}$$</p><p>因为a1 = 2，a2 = 4，所以上面的矩阵应该是:$\begin{pmatrix}<br>8&amp;0\\<br>0&amp;96\\<br>\end{pmatrix}<br>$<br>运行的结果：<br><img src="/images/pasted-4.png" alt="upload successful"><br>嗯，的确是8和96，但是仔细想一想，和咱们想要的雅克比矩阵的形式也不一样啊。难道是backward自动把0给省略了？</p><p>咱们继续试试，这次在上一个模型的基础上进行小修改，如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line">    </span><br><span class="line">a = Variable(torch.Tensor([[<span class="number">2.</span>,<span class="number">4.</span>]]),requires_grad=<span class="keyword">True</span>)</span><br><span class="line">b = torch.zeros(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">b[<span class="number">0</span>,<span class="number">0</span>] = a[<span class="number">0</span>,<span class="number">0</span>] ** <span class="number">2</span> + a[<span class="number">0</span>,<span class="number">1</span>] </span><br><span class="line">b[<span class="number">0</span>,<span class="number">1</span>] = a[<span class="number">0</span>,<span class="number">1</span>] ** <span class="number">3</span> + a[<span class="number">0</span>,<span class="number">0</span>]</span><br><span class="line">out = <span class="number">2</span> * b</span><br><span class="line"><span class="comment">#其参数要传入和out维度一样的矩阵</span></span><br><span class="line">out.backward(torch.FloatTensor([[<span class="number">1.</span>,<span class="number">1.</span>]]))</span><br><span class="line">print(<span class="string">'input:'</span>)</span><br><span class="line">print(a.data)</span><br><span class="line">print(<span class="string">'output:'</span>)</span><br><span class="line">print(out.data)</span><br><span class="line">print(<span class="string">'input gradients are:'</span>)</span><br><span class="line">print(a.grad)</span><br></pre></td></tr></table></figure></p><p>可以看出这个模型的雅克比应该是：<br>$$\begin{pmatrix}<br>{\partial out_1 \over \partial a_1}=4a_1&amp;{\partial out_1 \over \partial a_2}=2\\<br>{\partial out_2 \over \partial a_1}=2&amp;{\partial out_2 \over \partial a_2}=6a_2^2\\<br>\end{pmatrix}$$<br>运行一下：<br><img src="/images/pasted-5.png" alt="upload successful"><br>等等，什么鬼？正常来说不应该是$\begin{pmatrix}<br>8&amp;2\\<br>2&amp;96\\<br>\end{pmatrix}<br>$么？<br>我是谁？我再哪？为什么就给我2个数，而且是  8 + 2 = 10 ，96 + 2 = 98 。难道都是加的 2 ？<br>想一想，刚才咱们backward中传的参数是 [ [ 1 , 1 ] ]，难道安装这个关系对应求和了？<br>咱们换个参数来试一试，程序中只更改传入的参数为[ [ 1 , 2 ] ]：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line">    </span><br><span class="line">a = Variable(torch.Tensor([[<span class="number">2.</span>,<span class="number">4.</span>]]),requires_grad=<span class="keyword">True</span>)</span><br><span class="line">b = torch.zeros(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">b[<span class="number">0</span>,<span class="number">0</span>] = a[<span class="number">0</span>,<span class="number">0</span>] ** <span class="number">2</span> + a[<span class="number">0</span>,<span class="number">1</span>] </span><br><span class="line">b[<span class="number">0</span>,<span class="number">1</span>] = a[<span class="number">0</span>,<span class="number">1</span>] ** <span class="number">3</span> + a[<span class="number">0</span>,<span class="number">0</span>]</span><br><span class="line">out = <span class="number">2</span> * b</span><br><span class="line"><span class="comment">#其参数要传入和out维度一样的矩阵</span></span><br><span class="line">out.backward(torch.FloatTensor([[<span class="number">1.</span>,<span class="number">2.</span>]]))</span><br><span class="line">print(<span class="string">'input:'</span>)</span><br><span class="line">print(a.data)</span><br><span class="line">print(<span class="string">'output:'</span>)</span><br><span class="line">print(out.data)</span><br><span class="line">print(<span class="string">'input gradients are:'</span>)</span><br><span class="line">print(a.grad)</span><br></pre></td></tr></table></figure></p><p><img src="/images/pasted-6.png" alt="upload successful"><br>嗯，这回可以理解了，我们传入的参数，是对原来模型正常求导出来的雅克比矩阵进行线性操作，可以把我们传进的参数（设为arg）看成一个列向量，那么我们得到的结果就是:<br>${({M_{Jacobo}}{M_{arg}})}^T$<br>在这个题目中，我们得到的实际是：<br>$$\begin{cases}<br>{1*{\partial out_1 \over \partial a_1}}+{2*{\partial out_1 \over \partial a_2}}=12\\<br>{1*{\partial out_2 \over \partial a_1}}+{2*{\partial out_2 \over \partial a_2}}=12\\<br>\end{cases}<br>$$<br>看起来一切完美的解释了，但是就在我刚刚打字的一刻，我意识到官方文档中说k.backward()传入的参数应该和k具有相同的维度，所以如果按上述去解释是解释不通的。<br>哪里出问题了呢？</p><p>仔细看了一下，原来是这样的：在对雅克比矩阵进行线性操作的时候，应该把我们传进的参数（设为arg）看成一个行向量（不是列向量），那么我们得到的结果就是:${({M_{arg}}{M_{Jacobo}})}^T$<br>也就是：<br>$$\begin{cases}<br>{1*{\partial out_1 \over \partial a_1}}+{2*{\partial out_2 \over \partial a_1}}=12\\<br>{1*{\partial out_1 \over \partial a_2}}+{2*{\partial out_2 \over \partial a_2}}=12\\<br>\end{cases}<br>$$<br>这回我们就解释的通了。</p><p>现在我们来输出一下雅克比矩阵吧，为了不引起歧义，我们让雅克比矩阵的每个数值都不一样（一开始分析错了就是因为雅克比矩阵中有相同的数据），所以模型小改动如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line">    </span><br><span class="line">a = Variable(torch.Tensor([[<span class="number">2.</span>,<span class="number">4.</span>]]),requires_grad=<span class="keyword">True</span>)</span><br><span class="line">b = torch.zeros(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">b[<span class="number">0</span>,<span class="number">0</span>] = a[<span class="number">0</span>,<span class="number">0</span>] ** <span class="number">2</span> + a[<span class="number">0</span>,<span class="number">1</span>] </span><br><span class="line">b[<span class="number">0</span>,<span class="number">1</span>] = a[<span class="number">0</span>,<span class="number">1</span>] ** <span class="number">3</span> + a[<span class="number">0</span>,<span class="number">0</span>] * <span class="number">2</span></span><br><span class="line">out = <span class="number">2</span> * b</span><br><span class="line"><span class="comment">#其参数要传入和out维度一样的矩阵</span></span><br><span class="line">out.backward(torch.FloatTensor([[<span class="number">1</span>,<span class="number">0</span>]]),retain_graph=<span class="keyword">True</span>)</span><br><span class="line">A_temp = copy.deepcopy(a.grad)</span><br><span class="line">a.grad.zero_()</span><br><span class="line">out.backward(torch.FloatTensor([[<span class="number">0</span>,<span class="number">1</span>]]))</span><br><span class="line">B_temp = a.grad</span><br><span class="line">print(<span class="string">'jacobian matrix is:'</span>)</span><br><span class="line">print(torch.cat( (A_temp,B_temp),<span class="number">0</span> ))</span><br></pre></td></tr></table></figure></p><p>如果没问题的话咱们的雅克比矩阵应该是 [ [ 8 , 2 ] , [ 4 , 96 ] ]</p><p>好了，下面是见证奇迹的时刻了,不要眨眼睛奥，千万不要眨眼睛……<br>3<br>2<br>1<br>砰…………<br><img src="/images/pasted-7.png" alt="upload successful"><br>好了，现在总结一下：因为经过了复杂的神经网络之后，out中每个数值都是由很多输入样本的属性（也就是输入数据）线性或者非线性组合而成的，那么out中的每个数值和输入数据的每个数值都有关联，也就是说【out】中的每个数都可以对【a】中每个数求导，那么我们backward（）的参数[k1,k2,k3….kn]的含义就是：</p><p>$$\begin{cases}<br>{\partial out \over \partial a_1}={k_1*{\partial out_1 \over \partial a_1}}+{k_2*{\partial out_2 \over \partial a_1}}+{k_3*{\partial out_3 \over \partial a_1}}+…+{k_n*{\partial out_n \over \partial a_1}}\\<br>{\partial out \over \partial a_2}={k_1*{\partial out_1 \over \partial a_2}}+{k_2*{\partial out_2 \over \partial a_2}}+{k_3*{\partial out_3 \over \partial a_2}}+…+{k_n*{\partial out_n \over \partial a_2}}\\<br>…\\<br>{\partial out \over \partial a_n}={k_1*{\partial out_1 \over \partial a_n}}+{k_2*{\partial out_2 \over \partial a_n}}+{k_3*{\partial out_3 \over \partial a_n}}+…+{k_n*{\partial out_n \over \partial a_n}}\\<br>\end{cases}<br>$$<br>也可以理解成每个out分量对an求导时的权重。</p><h1 id="对矩阵自动求导"><a href="#对矩阵自动求导" class="headerlink" title="对矩阵自动求导"></a>对矩阵自动求导</h1><p>现在，如果out是一个矩阵呢？<br>下面的例子也可以理解为：相当于一个神经网络有两个样本，每个样本有两个属性，神经网络有两个输出。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line">a = Variable(torch.FloatTensor([[<span class="number">2</span>,<span class="number">3</span>],[<span class="number">1</span>,<span class="number">2</span>]]),requires_grad=<span class="keyword">True</span>)</span><br><span class="line">w = Variable( torch.zeros(<span class="number">2</span>,<span class="number">1</span>),requires_grad=<span class="keyword">True</span> )</span><br><span class="line">out = torch.mm(a,w)</span><br><span class="line">out.backward(torch.FloatTensor([[<span class="number">1.</span>],[<span class="number">1.</span>]]),retain_graph=<span class="keyword">True</span>)</span><br><span class="line">print(<span class="string">"gradients are:&#123;&#125;"</span>.format(w.grad.data))</span><br></pre></td></tr></table></figure></p><p>如果前面的例子理解了，那么这个也很好理解，backward输入的参数k是一个2x1的矩阵，2代表的就是样本数量，就是在前面的基础上，再对每个样本进行加权求和。<br>结果是：</p><p><img src="/images/pasted-8.png" alt="upload successful"></p><p>如果有兴趣，也可以拓展一下多个样本的多分类问题，猜一下k的维度应该是【输入样本的个数 X 分类的个数】</p><p>好啦，纠结我好久的pytorch自动求导原理算是彻底搞懂啦~~~</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;正常来说backward( )函数是要传入参数的，一直没弄明白backward需要传入的参数具体含义，但是没关系，生命在与折腾，咱们来折腾一下，嘿嘿。&lt;/p&gt;
&lt;h1 id=&quot;对标量自动求导&quot;&gt;&lt;a href=&quot;#对标量自动求导&quot; class=&quot;headerlink&quot; ti
      
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Pytorch" scheme="http://yoursite.com/tags/Pytorch/"/>
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
</feed>
